---
layout: post
comments: false
title: "Keypoint"
date: 2020-01-01 01:09:00
tags: paper-reading
---

> This post is a summary of keypoint related papers.


<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## Unsupervised 3D Keypoint Learning

### 1. [KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control](https://openaccess.thecvf.com/content/CVPR2021/papers/Jakab_KeypointDeformer_Unsupervised_3D_Keypoint_Discovery_for_Shape_Control_CVPR_2021_paper.pdf)

*Tomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu, Noah Snavely, Angjoo Kanazawa*

*CVPR 2021*

[page](http://tomasjakab.github.io/KeypointDeformer)

现在在Internet上有非常多的3D shapes，给用户提供简单的interface，让他们可以在保留关键shape性质的情况下对3D object做semantically manipulating。文章里为interative editing提出自动检测intuitive和semantically有意义的control points，从而通过控制这些control points来对每个object类别的3D模型进行deformation，并且还保留了他们的shape的细节。

更准确的说，文章将3D keypoints作为shape editing的intuitive和simple interface。Keypoints是那些在一个object类别所有的3D shape间都semantically consistent的3D points。文章提出一个学习框架使用非监督学习的方式来找到这样的keypoints，并且设计一个deformation model来利用这些keypoints在保留局部形状特征的前提下改变物体的shape。这个模型叫KeypointDeformer。

Fig 1描述了KeypointDeformer在inference时候的过程。给一个新的3D shape，KeypointDeformer在它的surface上预测3D keypoints。如果一个用户将chair leg上的keypoint向上移动，整个chair leg都会超相同的方向形变（fig 1下面一行）。KeypointDeformer在这些可操纵的keypoints上提供了可选择的categorical deformation prior，比如说如果一个用户将一个airplane一侧wing上的keypoints向后移动，这一侧的wing会整体向后移动，而另一侧的wing也会随之移动同样的程度（fig 1上面一行）。当用户仅仅希望移动一侧wing的时候，我们的方法同样也允许这种操作。KeypointDeformer可以仅仅对于shape进行editing，也可以对两个shapes做shape alignment，还可以生成新的shapes来扩充datasets。

![overs]({{ '/assets/images/DEFORM-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1 用非监督学习到的3D keypoints来进行shape deformation。我们用非监督学习的方式学习到的3D keypoints可以对object的shape进行intuitive控制。这个figure显示了交互式控制的独立的步骤。红色的箭头标注了keypoints被操作要移动的方向。注意到移动keypoints造成的shape形变是局部的并且object shape是按照intuitive的方式形变的：比如说，将airplane的wing上的keypoint向后拉，则整个wing也会向后倾斜，而保持原本shape的细节不变化。*

尽管3D keypoints对于shape editing来说很有效果，但是获取3D keypoints和deformation models的明确的监督信息不仅很贵，而且是ill-defined的。文章提出了一个unsupervised框架来将寻找3D keypoints和构建deformation model这两个任务同时完成。模型有两个在一起作用的模块：1）一个detecting keypoints的部分；2）一个将keypoints的位移信息传递到shape的其它部分从而进行deformation的deformation model。模型利用将一个source shape align到一个target shape上这样一个任务来训练网络，而且这两个shapes可以是同一个object category里差别很大的两个instances。模型还利用了一个keypoint regularizer，来促进学习到semantically consistent的keypoints。这些keypoints分布的很好，靠近object的surface并且隐式的保留着shape symmetries。KeypointDeformer训练之后所得到的就是一个deformable model，可以基于自动监测到的3D control keypoints来deform一个shape。因为keypoints是低维的，我们还可以在这些keypoints上学习一个category prior，这样就可以进行semantic shape editing了。

文章有以下几个关键的优势：

* 其给了用户一个intuitive并且简单的方法来交互式的控制object shapes
* keypoint prediction和deformable的模型都是unsupervised的
* 由文章的方法所找到的3D keypoints对于shape control来说比其他的keypoints都要好，包括人为标注的
* 文章的unsupervised 3D keypoints对于同一类别的object的不同的instances来说是semantically consistent的，从而给了我们两个point cloud的sparse correspondences。


**Related Work**

*Shape deformation*

文章的方法和geometric modeling里的detail-preserving deformations十分相关，包括[Differential Representations for Mesh Processing](http://mesh.brown.edu/dgp/pdfs/sorkine-cgf2006.pdf)，[As Rigid As Possible Surface Modeling](https://diglib.eg.org/bitstream/handle/10.2312/SGP.SGP07.109-116/109-116.pdf?sequence=1&isAllowed=n)和[Mean Value Coordinates for Closed Triangular Meshes](https://www.cse.wustl.edu/~taoju/research/meanvalue.pdf)。这些方法通过各种类型的限制（比如说points在一个optimization框架里）来允许进行shape editing，但它们一个最主要的问题就是它们仅仅依赖于geometric properties而并没有考虑到semantic attributes或者category-specific shape priors。这样的priors可以通过利用stiffness性质给object surface涂色获得，或者从一系列已经知道correspondence的meshes上学习得到。然而，这种监督信息十分昂贵，而且对于新的shapes来说就不管用了（training set没有见过的shapes，或者有新的priors的shapes）。[Semantic Shape Editing Using Deformation Handles](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.723.3455&rep=rep1&type=pdf)用一个提供了多个控制shape的sliders的data-driven的框架来解决了这个问题。然而这个方法需要一系列从专家标注的信息中提取的predefined attributes。

另一个相关的问题是[deformation transfer](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.6553&rep=rep1&type=pdf)，也就是利用两个shapes之间已知的correspondences将source mesh上的deformation转移到target mesh上。近期有些工作利用deep learning来隐式的学习shape correspondences来align两个shapes，比如[1](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yifan_Neural_Cages_for_Detail-Preserving_3D_Deformations_CVPR_2020_paper.pdf)，[2](https://arxiv.org/pdf/1804.08497.pdf)和[3](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_3DN_3D_Deformation_Network_CVPR_2019_paper.pdf)。

*User-guided shape editing*

文章的方法和最近的利用deep learning来学习可以提供对shape做interactive editing的generative模型。[Tulsiani的文章](https://openaccess.thecvf.com/content_cvpr_2017/papers/Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper.pdf)用primitives来抽象代表shapes，然后通过surface的primitives的deformation来edit shape。但是，shape editing并不是它们主要的目标，而且也不清楚直接进行primitive transformation能多大程度的保留local shape details。近似的工作进一步改进了这个方法，他们通过学习一个[point-based](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hao_DualSDF_Semantic_Shape_Manipulation_Using_a_Two-Level_Representation_CVPR_2020_paper.pdf)、[shape handles](https://openaccess.thecvf.com/content_CVPR_2020/papers/Gadelha_Learning_Generative_Models_of_Shape_Handles_CVPR_2020_paper.pdf)或者[disconnected shape manifolds](https://openaccess.thecvf.com/content_ICCV_2019/papers/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.pdf)的primitives的generative model来改进原先的基于primitives的model的缺点。这些方法通过找到最佳匹配用户editing的latent primitive representations来做到interactive editing。但是他们的方法所用到的用户interface比较复杂，需要素描或者直接操控primitives。而且最关键的，因为这些editing是基于generative models的，这些方法可能会改变original shape的local details。而相对而言，我们直接对原shape进行deform，会有更好的shape detail的保留。我们将提出的方法和[DualSDF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hao_DualSDF_Semantic_Shape_Manipulation_Using_a_Two-Level_Representation_CVPR_2020_paper.pdf)的结果进行对比来阐述上述的优势。


*Unsupervised keypoints*

在2D keypoint discovery领域，unsupervised方法有很多论文都已经有了不错的结果，[Unsupervised learning of object landmarks through conditional image generation](https://proceedings.neurips.cc/paper/2018/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf)，[Self-supervised learning of interpretable keypoints from unlabelled videos](https://openaccess.thecvf.com/content_CVPR_2020/papers/Jakab_Self-Supervised_Learning_of_Interpretable_Keypoints_From_Unlabelled_Videos_CVPR_2020_paper.pdf)，[Self-supervised learning of a facial attribute embedding from video](https://arxiv.org/pdf/1808.06882.pdf)，[Unsupervised learning of landmarks by descriptor vector exchange](https://openaccess.thecvf.com/content_ICCV_2019/papers/Thewlis_Unsupervised_Learning_of_Landmarks_by_Descriptor_Vector_Exchange_ICCV_2019_paper.pdf)[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)[Unsupervised discovery of object landmarks as structural representations](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)，但在3D keypoint discovery领域，unsupervised的方法却还没有被研究完全。[Discovery of latent 3d keypoints via end-to-end geometric reasoning](https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf)利用3D pose information作为supervision来从两张关于同一个object的不同角度的图片中检测3D keypoints。我们这篇文章聚焦于在从3D shapes上学习3D keypoints。[Unsupervised learning of intrinsic structural representation points](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Unsupervised_Learning_of_Intrinsic_Structural_Representation_Points_CVPR_2020_paper.pdf)输出一个结构化的3D representation来获取sparse或者dense的shape correspondences。和我们这篇文章里进行3D keypoints discovery方法最像的文章是[Unsupervised learning of category-specific symmetric 3d keypoints from point sets](https://arxiv.org/pdf/2003.07619.pdf)，他们采用了显式的对称性限制条件。在这篇文章里，我们是为了shape control这个任务来使用非监督的方式寻找keypoints。尽管我们重点在于shape editing，但是我们的模型构造使得我们学习到了semantic consistent的3D keypoints。这样的以非监督方式学到的3D keypoints对于机器人来说没准是有用的，那些机器人相关的任务可以将3D keypoints作为latent representation用来控制机器人，而他们现在还需要手动定义3D keypoints来作为控制机器人的信号。


**Method**

目标是学习一个keypoint detector，$$\Phi: x \longrightarrow p$$，来将一个3D object shape $$x$$映射到一个semantically consistent的3D keypoints的集合$$p$$。我们同时也想学习一个输入为keypoints的conditional deformation model，$$\Psi: (x, p, p^{'}) \longrightarrow x^{'}$$，将shape $$x$$利用deformed control keypoints映射到shape $$x^{'}$$，其中$$p$$描述的是initial（source）keypoint locations，$$p^{'}$$描述的是target keypoint locations。为keypoints和deformation model获取显式的监督信息十分expensive而且ill-defined。因此，我们提出了一个unsupervised的learning框架来训练上述提到的两个functions。我们通过设计了一个pair-wise shape alignment的辅助任务来实现，这个辅助任务的核心想法就是将keypoints learning和deformation model联合起来学习，从而可以对两个任意的shapes做alignment。更仔细地说，模型首先利用一个Siamense network在source和target shapes上预测3D keypoint locations。之后我们利用检测到的keypoints的对应关系来deform source shape（检测到的keypoints是默认有序的，从而有着对应关系）。为了保持local shape detail，我们使用了一个基于keypoints的cage-based deformation方法。我们使用了一个新颖的但十分简单高效的keypoint regularization term，使得keypoints是well-distributed的，并且距离object surface很近。Fig 2显示了我们的模型的整体框架。

![framework]({{ '/assets/images/DEFORM-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. Model。我们的模型使用预测到的unsupervised keypoints $$p$$和$$p^{'}$$来将source shape $$x$$ aligns到target shape $$x^{'}$$。unsupervised keypoints描述了object的pose并用作deformation的control points。整个模型使用在deformed source shape $$x^{\*}$$和target shape $$x^{'}$$之间的similarity loss和keypoint regularization loss来进行end-to-end的训练。在interactive shape manipulation的test time，用户可以选择只输入一张source shape $$x$$，keypoint predictor $$\Phi$$就会预测一些unsupervised 3D keypoints $$p$$出来。然后用户可以手动控制keypoints $$p$$使其变成target keypoints$$p^{'}$$，然后再用deformation model $$\Psi$$来生成deformed source shape $$x^{\ast}$$，如fig 1，fig 9或者project page上的补充材料里的视频所示。*

**1. Shape Deformation with Keypoints**

我们将每个object表示为一个point cloud $$x \in R^{3 \times N}$$，是从object mesh里均匀采样得来的。我们先从source和target里预测keypoints。keypoint predictor $$\Phi$$使用$$x$$作为输入，输出一个ordered set，$$p = (p_1,...,p_K) \in R^{3 \times K}$$，表示的是3D keypoints。这个keypoint predictor的encoder对于source和target是公用的，使用Siamese architecture来实现。而shape deformation function $$\Psi$$的输入是source shape $$x$$，source keypoints $$p$$和target keypoints $$p^{'}$$。在test阶段，输入一张图片，得到了其的source keypoints，用户editing之后得到了target keypoints，之后生成输入图片的deformation，整个interactive shape deformation过程如fig 2所示。

为了在deform object的过程中保持它的local shape details，我们使用最近刚出现的[differentiable cage-based deformation algorithm](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yifan_Neural_Cages_for_Detail-Preserving_3D_Deformations_CVPR_2020_paper.pdf)。cages是个很经典的shape modeling方法，其使用一个粗糙的封闭的mesh将shape包起来。deform cage mesh就会导致里面包裹的shape也发生deformation。cage-based deformation function $$\beta: (x,c,c^{\ast}) \longrightarrow x^{\ast}$$的输入是source control cage $$c$$，deformed control cage $$c^{\ast}$$，以及source shape $$x$$（也就是一开始从mesh里采样得到的point cloud）。我们通过一开始用一个球体包住source shape $$x$$，之后再将每个cage vertex $$c_V$$向object的中心推进直到它和object surface之间只有一个很小的距离这样一种方法来为每个shape都自动的获取包裹其的cage。fig 2显示了所得到的cage的样子。尽管cages对于shape-preserving deformation来说是个有用的方法，但通过deform cages来获得内部的shape的deformation并不是那么的直观，特别是对新手用户来说，因为cage vertex并不直接落在shape的表面上，并没有一个粗糙的structure，而且在不同的shape之间（同一个object或者同一类object的不同姿态的shape）并不semantically consistent。我们提出keypoints用作操纵cage deformation的方式更为合理。

为了用我们检测到的keypoints来控制object deformation，我们需要将这些keypoints和cage vertices联系起来。我们通过使用一个linear skinning function，首先计算source和target keypoints之间的relative distance，$$\delta p = p^{'}-p$$，然后将一个可学习的influence matrix，$$W \in R^{C \times K}$$乘上$$\delta p$$，在加到source cage vertices，$$c_V$$上，就获得了新的target cage vertices，$$c_{V}^{\ast}$$。其中，$$p,p^{'},\delta p$$都是$$K \times 3$$的矩阵，$$c_V, c_V^{\ast}$$是$$C \times 3$$的矩阵，而$$K$$和$$C$$分别表示keypoints和cage vertices的个数。所以deformed cage vertices，$$c_V^{\ast}$$计算方式为：

$$c_V^{\ast} = c_V + W \delta p$$

为了满足对于每个shape来说cage是唯一的这样的事实，我们将上述的influence matrix，$$W$$，设置为输入shape $$x$$的一个函数。详细的说，influence matrix是一个composition，$$W(x) = W_C + W_I(x)$$，其中$$W_c$$是对于每一类object的所有instances都共用的canonical matrix，而$$W_I(x)$$则是每个instance独自的offset，是利用influence predictor $$W_I = \Gamma(x)$$以source shape $$x$$为输入计算而来。我们同时也通过最小化其Frobenius norm来regularize这个instance specific offset，$$W_I$$，为了防止它过拟合influence matrix $$W$$。我们将这个regularizer命名为$$L_{inf}$$。最后，我们限制$$W$$使得每个keypoint最多只能影响$$M$$个最近的cage vertices来实现locality。


**2. Losses和Regularizers**

我们的KeypointDeformer是通过最小化source和target shape之间的similarity loss，再加上keypoint regularization loss和instance-specific influence matrix regularization term，利用SGD实现的end-to-end的训练。

**Similarity loss**

理想情况下，我们希望利用已知的meshes之间的correspondence来计算deformed source shape $$x$$和target shape $$x^{'}$$之间的similarity。但是这样的correspondence是不存在的，因为我们希望能在最普遍的object category CAD模型上训练。我们通过将source shape和target shape都表示为point cloud，然后再计算他们之间的Chamfer distance来近似这个similarity loss。这个loss记为$$L_{sim}$$。


**Farthest Point Keypoint regularizer**

我们提出了一个简单有效的keypoint regularizer $$L_{kpt}$$来使得预测的keypoints $$p$$是well-distributed的，也就是再object surface上，并且能保持这个shape category本身的symmetric structure。具体来说，我们设计了一个**Farthest Sampling Algorithm**来从输入的shape mesh里采样一个无序集合$$q = \{q_1,...,q_J\} \in R^{3 \times J}$$作为point cloud。采样的起始点是随机的，所以每次我们计算这个regularization loss的时候我们都使用的不同的point cloud $$q$$。给定这些随机的farthest points，regularizer最小化所预测的keypoints $$p$$和这些采样到的点$$q$$之间的Chamfer distance。也就是说，这个regularizer希望keypoint detector $$\Phi$$能够学习到那些和$$q$$分布类似的keypoints。Fig 3展示了$$q$$的特性。这些采样到的点对于提供了输入的object shape $$x$$的一个均匀的覆盖，其再不同的instances之间比较稳定，而且保持了最初input shape的symmetric结构。

![LOSS]({{ '/assets/images/DEFORM-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 3. Farthest Point Keypoint regularizer. 我们使用一个随机的初始点来做farthest point sampling，来regularize预测到的keypoints。(a) 展示了对于一个给定的点，其被farthest point sampling algorithm所选到的频率。颜色越深表明这个点被选到的概率越大。采样点的期望locations对原shape有一个很好的覆盖而且保留了原shape的symmetry特征。而且，它们的子集在不同的object instances之间保持了semantically stable。使用采样点的期望locations作为keypoint location的prior效果很好，因为keypoint predictor会学会对这些采样点里的噪声比较robust。在airplane的例子里我们可以看到，fuel tank的顶点（红圈标记）并没有被keypoint predictor用作keypoint，(b) 而wing的顶点（绿圈）则被选中为keypoint，因为其在数据集里更加consistent（大多数飞机都有wings，但很多并没有fuel tank）。* 

这个regularization的另一个intuition是我们可以将这些采样的farthest points $$q$$理解为keypoint locations的一个noisy prior。这个prior并不是完美的——在某些shape上可能会遗失某些重要的点，或者有一些不合理的点——但是neural network keypoint predictor会以一种对这些noise robust的方式学到keypoint的locations，而且会偏向于学习那些consistent的keypoints，如fig 3所示。

**Full objective**

总结来说，我们的training objective是：

$$L = L_{sim} + \alpha_{kpt}L_{kpt} + \alpha_{inf}L_{inf}$$

其中$$\alpha_{kpt}$$和$$\alpha_{inf}$$是scalar loss系数。我们的方法很简单而且并不需要对于shape deformation增加额外的shape specific regularization，比如说point-to-surface距离，normal consistency，symmetry losses。这是因为keypoints提供了一个shapes之间的low-dimensional的correspondence，而且cage deformations是这些keypoints的一个linear function，从而阻止了那些会导致local deformation的极端的deformations。


**3. Categorical Shape Prior**

因为我们利用一系列semantically consistent的keypoints来代表一个object shape，我们可以通过计算training set里的shape对应的keypoints的PCA来获取categorical shape prior。这个prior可以用来指导keypoint manipulation，也就是上面提到的$$W_C$$。比如说，如果用户想要改变一个airplane一个wing上的一个keypoint，根据寻找到能够最佳重构这个被改变的keypoint的新位置的PCA basis coefficients，其余的keypoints就会被这些basis coefficients”同步协调“。从而这些keypoints就会根据这个prior（也就是这个PCA）落到新的位置。这个prior还可以通过采样一系列新的keypoints来生成新的shapes：调整某些keypoints，然后PCA经过计算basis coefficients来对所有的keypoints位置进行调整，从而得到了新的keypoints位置，然后利用上述的deformation model来生成新的shape，就可以将这个新的shape加入已有的3D shape datasets里。




### 2. [Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point Sets](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700545.pdf)

*Clara Fernandez-Labrador, Ajad Chhatkuli, Danda Pani Paudel, Jose J. Guerrero, Cedric Demonceaux, Luc Van Gool*

*ECCV 2020*

>又是ethz Luc Van Gool大佬的作品。

从同一类别的objects的集合里自动找到category-specific的3D keypoints是一个非常有挑战性的问题，这篇文章要用一种unsupervised方法来学习上述这种3D keypoints。

从没有标签的3D数据中直接获取3D keypoints——来表示有意义的shapes和semantics——从而使得这些keypoints的作用和手动定义的那些一样，因为很困难并没有受到过多的关注。

因为objects本身就是位于三维空间内，所以3D keypoints对于geometric reasoning任务效果更好是很自然的。对于给定的3D keypoints，它们在2D图片中的counterpart可以很简单的用camera projection来实现。然而，能直接从3D数据（point clouds）上预测keypoints会有优势，因为很多时候multiple camera views或者multiple images是不可获取的。在这篇文章里，我们关注如何从3D point clouds中直接学习keypoints locations。实际上，带有keypoints的3D structures对于很多的应用包括registration，shape completion，shape modeling都是足够使用的了，并不需要它们的2D counterparts。

因为deformation或者对比同一个类别不同的objects，3D objects就会有shape variations，找到consistent的keypoints对于geometric reasoning就很重要了。我们可不可以自动找到这些keypoints，其对于同一个类别的同一个objects的deformation以及同一个类别的不同objects之间的差异都是consistent的？这是这篇文章主要想要回答的问题。更进一步，我们想要用unsupervised方法直接从3D point clouds上找到这样的keypoints。我们将这些keypoints称为category-specific，它们被期望于能描述objects的shape信息，并能够在同一类别的所有objects之间保持有序的对应关系。更加严格地说，我们将category-specific keypoints所需要满足的性质定义为：1）对于同一类别的具有不同shapes或者不同alignments的不同的objects具有很好的泛化效果，也就是说对于形状不同或者没有对齐（有旋转角度）的同一个类别的不同的objects，找到的仍然是这些keypoints；2）能做到一对一的有序的对应并且具有semantic consistency；3）在保持shape symmetry的情况下能够代表这一类别的object的形状特征。

但是在3D point clouds上学习category-specific keypoints是一个非常难的问题。而如果数据还是misaligned（可能会有旋转，因为平移对于CNN来说没有影响）并且要使用unsupervised方法，那这个问题就更难了。相关的工作并没有全部考虑这些因素，而是只考虑了一部分：[USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.pdf)使用aligned数据并且学习的不是category-specific keypoints，而是任意object的通用keypoints，[6-DoF Object Pose from Semantic Keypoints](https://arxiv.org/pdf/1703.04670.pdf)在2D图像上使用监督信息，[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf)使用aligned 3D数据并且使用多张知道pose的2D图片，其在没有显式考虑shapes的情况下获得了category specific的3D keypoints。[3D Landmark Model Discovery from a Registered Set of Organic Shapes](https://clementcreusot.com/publications/papers/creusot2012-PCP.pdf)使用预先定义好的local shape descriptors和一个template模型，而且只针对faces。

在这篇文章里，我们说明拥有上述性质的category-specific keypoints可以用unsupervised的方式，通过基于未知的linear basis shapes结合non-rigidity来对它们建模的方式学习到。在考虑具有对称性的object类别的时候，我们还在deformation模型上加入了未知的reflective symmetry。对于那些没有对称性的object类别，我们使用symmetric linear basis shapes来对所谓的symmetric deformation spaces进行建模，比如说human body deformations。我们所提出的learning方法并不需要假设shapes是aligned好的、或者预先计算好了basis shapes或者已经知道了对称平面，所有的这些值都是通过end-to-end的方式学到的。我们的模型相比较于之前NRSfM([Multiview Aggregation for Learning Category-Specific Shape Reconstruction](https://arxiv.org/pdf/1907.01085.pdf)和[Symmetric Non-Rigid Structure from Motion for Category-Specific Object Structure Estimation](https://arxiv.org/pdf/1609.06988.pdf))里的方法来说十分有效而且灵活。我们通过将一个object类别的shape basis和对称平面都作为neural network的参数来学习的办法来实现。训练的时候每次输入是一个3D point cloud，并没有使用Siamese-like的模型结构。在inference的时候，网络会预测basis coefficients和pose，再用来估计instance-specific keypoints。


**Related Work**

objects的category specific keypoints在NRSfM方法里用得很多，但是却很少有论文来研究如何找到它们。考虑模型的输出，我们的方法和[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://keypointnet.github.io/keypointnet_neurips.pdf)很像，这篇文章通过解决一个辅助任务来学习category-specific 3D keypoints，这个辅助任务是学习同一个object的不同角度的view之间的rigid transformation，而且这个方法假设图片是aligned好的。尽管这个方法在2D和3D上都显示了比较好的结果，但是其并没有显式的对shape进行建模。结果就是，这个方法需要同一个object的不同角度的照片是aligned好的，从而才能计算keypoint correspondences。在[6-DoF Object Pose from Semantic Keypoints
](https://arxiv.org/pdf/1703.04670.pdf?ref=https://githubhelp.com)里同样对于六自由度estimation考虑了类似的任务，其使用了一个low-rank shape prior来为3D keypoints提供condition。尽管low-rank shape modeling是个很有力的工具，但这篇文章还是需要对heatmap prediction进行监督，而且依赖于aligned shapes和预先计算好的shape basis。[Single Image 3D Interpreter Network](https://arxiv.org/pdf/1604.08685.pdf)同样也使用low-rank shape prior，但是它们的训练完全基于监督的方法。而且，上述所有的方法都是从images上利用heatmaps的方式学习到keypoints，再将其提升到3D空间内的。不同于上述的这些工作，[3D Landmark Model Discovery from a Registered Set of Organic Shapes
](https://clementcreusot.com/publications/papers/creusot2012-PCP.pdf)使用了deformation model和symmetry来直接从3D数据上预测keypoints，但是其需要一个face template，aligned shapes以及已知的shape basis。某一类别的objects的shape modeling在NRSfM工作中早就已经被研究透了。linear low-rank shape basis，low-rank trajectory basis，isometry或者piece-wise rigidity是NRFfM的提出的不同的方法。最近，有一些工作使用low-rank shape basis来设计可被学习的模型。另一个能被用来进行model shape category的方法是reflective symmetry，其也和object pose密切相关。尽管[Symmetric Non-Rigid Structure from Motion for Category-Specific Object Structure Estimation](https://yuan-gao.net/pdf/ECCV2016%20-%20Sym-NRSfM.pdf)说明low-rank shape basis可以使用未知的reflective symmetry进行构造，如何将其改造成可学习的NRSfM方法并不简单。还有最近的工作假设symmetry plane是从几个已知的planes里挑选出来的。这些方法都没有为non-rigidly deforming objects比如说human body构造symmetry。[Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Unsupervised_Learning_of_Probably_Symmetric_Deformable_3D_Objects_From_Images_CVPR_2020_paper.pdf)在一个warped canonical空间内概率化的考虑模型symmetry，来对不同objects进行3D reconstruction。

shape modeling是我们工作的一个重要的方面，另一个重要的方面是如何从一个unordered point set上学习到ordered keypoints。尽管在deep neural networks领域对于point sets来说有了好几个很好的成果（[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf)，[PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://proceedings.neurips.cc/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf)），但这些成果和在images上取得的巨大成功相比就不算啥了。[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf)利用一个Siamese architecture通过一个正确预测rotation的代理任务来用非监督学习的方式预测3D keypoints。[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)利用alignment作为代理任务来预测keypoints。在下面的章节里，我们将会展示如何利用low-rank symmetric shape basis来对shape instances进行建模，以及如何利用shape modeling来预测有序的category-specific keypoints。


**3. Background and Theory**

**Notations**

我们利用花体拉丁字母（比如$$\mathcal{V}$$）或者加粗大写拉丁字母（比如**V**）来表示sets和matrices。小写或者大写的普通字母，比如K来表示scalars。小写的加粗拉丁字母来表示vectors，比如**v**。小写的拉丁字母来表示indices，比如$$\mathcal{i}$$。大写的希腊字母来表示mappings或者functions，比如$$\Pi$$。我们使用$$\mathcal{L}$$来表示loss functions。operator mat将一个向量**v** $$\in R^{3N \times 1}$$转换为一个矩阵**M** $$\in R^{3 \times N}$$。

**3.1 Category-specific Shape and Keypoints**

我们的shape使用point clouds表示的，由一个无序的points集合来表示$$S=\lbrace s_1, s_2, \cdots, s_M \rbrace, s_j \in R^3, 1 \leq j \leq M$$。所有的同一个类别的这样的point clouds定义的shape组成了category shape space $$\mathcal{C}$$。我们将$$\mathcal{C}$$里的第i个category-specific shape instance记为$$S_i$$。category shape space $$\mathcal{C}$$可以是一系列离散的shapes，也可以是由deformation function $$\Psi_C$$生成的一个光滑的category-specific shapes流形。我们这篇文章关注点在于从point cloud $$S_i$$里学到有用的3D keypoints。为了达到这个目标，我们这一节定义category-specific keypoints，并且介绍生成keypoints的模型。

*category-specific keypoints* 我们将一个shape $$S_i$$的category-specific keypoints表示为一系列的points，$$P_i = (p_{i1}, p_{i2}, \cdots, p_{iN}), p_{ij} \in R^3, 1 \leq j \leq N$$。和shape $$S_i$$不同，这个集合$$P_i$$是有序的。我们的目标就是学习一个mapping $$\Pi_C: S_i \longrightarrow P_i$$来为$$\mathcal{C}$$内任意一个shape $$S_i$$学习到category-specific keypoints。在之前我们已经定义了category-specific的keypoints应该是什么样的。如果用数学语言来描述的话就是这样的：

* 1) Generalization: $$\Pi_C(S_i) = P_i, \forall S_i \in C$$
* 2）corresponding points and semantic consistency: 给定$$S_a, S_b \in \mathcal{C}$$，我们希望$$P_{aj} \iff P_{bj}$$。而且$$P_{aj}$$和$$P_{bj}$$需要有相同的semantics。
* 3) representative-ness: $$vol(S_i) = vol(P_i)$$以及$$p_{ij} \in S_i$$，其中$$vol(.)$$是一个对于shape计算volume的算子。如果$$S_i \in \mathcal{C}$$有reflective symmetry，那么$$P_i$$也得有相同的symmetry。


**3.2 Category-specific Shapes as Instances of Non-rigidity**

近期有一些工作将category shapes里的不同的instances利用non-rigid deformations来进行建模（[C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion](https://openaccess.thecvf.com/content_ICCV_2019/papers/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf)，[Single Image 3D Interpreter Network](https://dspace.mit.edu/handle/1721.1/114448)，[Multiview Aggregation for Learning Category-Specific Shape Reconstruction](https://proceedings.neurips.cc/paper/2019/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)，[Deep Non-Rigid Structure from Motion](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf)）。这个想法的基于这些shapes通常都有geometric similarities这样一个事实。结果是，存在一个deformation function $$\Psi_C: S_T \longrightarrow S_i$$，将一个global shape property $$S_T$$（shape template或者basis shapes）映射到一个category shape instance $$S_i$$。然而，对$$\Psi_C$$进行建模是很困难的，而且在很多情况下$$\Psi_C$$可能并不存在一个很简单的表述。这个问题，就是为什么dense Non-rigid Structure from Motion (NRSfM)任务这么困难的原因。从另一个角度来看，我们可以考虑一个deformation function $$\Phi_C: P_T \longrightarrow P_i$$，从global keypoints property $$P_T$$映射到每个instance的category-specific keypoints $$P_i$$。$$\Phi_C$$要满足，$$\Phi_C$$的一对对应点，也是$$\Psi_C$$的一对对应点。并且如果以点对来定义$$\Psi_C$$和$$\Phi_C$$，应该有$$\Phi_C \subset \Psi_C$$。和$$\Psi_C$$不同的是，$$\Phi_C$$的建模可能会很简单。因此，我们选择在keypoints空间内$$P = (P_1, P_2, \cdots, P_L)$$来寻找non-rigidity modeling。而所学到的$$\Phi_C$$就可以是$$\Psi_C$$的一个抽象。non-rigidity可以被用来定义prediction function $$\Pi_C$$：

$$\Pi_C(S_i;\theta) = \Phi_C(r_i;\theta) = P_i \tag{1}$$

其中$$\theta$$是$$\Pi_C$$的函数parameters，$$r_i$$是每个instance特有的vector parameter。在我们的设定下，我们希望能从$$\mathcal{C}$$的shape instances里用非监督的方式来学习到参数$$\theta$$。在NRSfM的设定里，对shape deformation进行建模的两种常见方法是low-rank shape prior和isometric prior。在这篇文章里，我们对instance-wise symmetry和deformation space的symmetry使用low-rank shape prior进行建模。

**3.3 Low-rank Non-rigid Representation of Keypoints**

NRSfM关于low-rank shape basis的方法是rigid orthographic factorization prior的一个自然的拓展，在[Recovering Non-Rigid 3D Shape from Image Streams](http://vision.jhu.edu/reading_group/Bregler2.pdf)和[Shape and motion from image streams under orthography: a factorization method](http://users.eecs.northwestern.edu/~yingwu/teaching/EECS432/Reading/Tomasi_TR92.pdf)。关键的想法是很大一部分object deformations都可以用$$K$$个不同pose的basis shapes的线性组合来表示，而且这个$$K$$是个不大的数。在rigid的情况下，这个$$K$$就是1。在non-rigid的情况下，这个$$K$$大一些，具体的数字取决于deformations的复杂程度。考虑$$\mathcal{C}$$里$$F$$个shape instances，并且在每个shape instance的keypoints instance $$P_i$$里考虑$$N$$个点。下面的式子描述了使用shape basis进行的projection：

$$P_i = \Phi_C(r_i;\theta) = R_i mat(B_C c_i) \tag{2}$$

其中$$B_C = (B_1, \cdots, B_K), B_C \in R^{3N \times K}$$构成了low-rank shape basis。vector $$c_i \in R^K$$表示对于instance $$i$$的不同的shape basis的线性系数。从而每个instance的keypoints就可以完全被basis $$B_C$$和系数$$c_i$$所表示了。之后，projection matrix $$R_i \in SO(3)$$就是instance $$i$$的rotation matrix。$$mat(.)$$表示将得到的$$3N \times 1$$的矩阵转换为$$3 \times N$$的矩阵。

在我们这个问题里，$$P_i, c_i, B_C, R_i$$都是不知道的。我们要通过上述的式子来学习，其中$$\theta$$包括了$$\Phi_C$$的function parameter，basis $$B_C$$，而$$r_i$$则包括了instance-wise pose $$R_i$$和系数$$c_i$$。


**3.4 Modeling Symmetry with Non-Rigidity**

很多object种类的shapes都有固定的reflective symmetry。为了寻求并使用这种symmetry，我们考虑两个不同的priors：instance-wise symmetry和symmetric deformation space。

*Instance-wise symmetry* 

关于一个固定平面的instance-wise reflective symmetry在很多rigid object类别上都能被观察到（比如[A Scalable Active Framework for Region Annotation in 3D Shape Collections](https://dl.acm.org/doi/pdf/10.1145/2980179.2980238)和[3D ShapeNets: A Deep Representation for Volumetric Shapes](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wu_3D_ShapeNets_A_2015_CVPR_paper.pdf))。在NRSfM里，这样的symmetry被和shape basis prior结合起来使用过。然而，用来同时学习symmetry和shapes的一个简便的representation还未被研究过。最近的learning-based的工作[Multiview Aggregation for Learning Category-Specific Shape Reconstruction](https://proceedings.neurips.cc/paper/2019/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)和[Normalized Object Coordinate Space for Category-Level
6D Object Pose and Size Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2019_paper.pdf)通过在几个平面上穷举式的寻找来使用symmetry prior，然后再用来预测symmetric dense non-rigid shapes。但是这样的方法在shapes并没有aligned的情况下是不行的。我们将公式2改造一下就可以加入instance-wise symmetry：

$$P_{i\frac{1}{2}} = R_i mat(B_{C\frac{1}{2}}c_i), P_i = \left[P_{i\frac{1}{2}}, A_C P_{i\frac{1}{2}} \right] \tag{3}$$

其中$$P_{i\frac{1}{2}} \in R^{3 \times N/2}$$表示的是一半的category-specific keypoints。$$P_{i\frac{1}{2}}$$被$$A_C \in R^{3 \times 3}$$反射一次，再和$$P_{i\frac{1}{2}}$$连起来，获得最终全部的category-specific keypoints。$$B_{C\frac{1}{2}} \in R^{3N/2 \times K}$$来表示对于一半的keypoints的shape basis，注意$$K$$并没有变化，说明shape basis并没有变少，只是我们只表示一半的keypoints。reflection operator $$A_C$$通过一个过原点的unit normal vector $$n_C \in R^3$$来定义。我们从公式2到公式3的变化，既减小了计算量，又将对称性加入了keypoints之中。

*Symmetric deformation space* 

在很多non-rigid objects里，shape instances并不是symmetric的。但是再deformation space里可能会存在symmetry，比如说，human body。假设在$$\mathcal{C}$$里的某一个shape instance $$S_k$$有着关于$$n_C$$的reflective symmetry，这样我们就可以把其分为两部分：$$S_{k\frac{1}{2}}$$和$$S_{k\frac{1}{2}}^{'}$$。而且我们可以认为对于这个类别的所有的shape instances，这个reflective symmetry都是存在的。

**Definition 1(Symmetric deformation space)** 在$$\mathcal{C}$$里的某一个shape instance $$S_k$$有着关于$$n_C$$的reflective symmetry，这样我们就可以把其分为两部分：$$S_{k\frac{1}{2}}$$和$$S_{k\frac{1}{2}}^{'}$$。如果对于任意的一半的shape deformation instance $$S_{i\frac{1}{2}}$$，其都存在一个shape instance $$S_j \in \mathcal{C}$$，使得$$S_{j\frac{1}{2}}^{'}$$和$$S_{i\frac{1}{2}}$$对称，那么称$$\mathcal{C}$$是一个symmetric deformation space。

上述定义对于keypoints shape space $$P$$来说一样成立。instance-wise symmetric space是上述定义的一个特例。但是公式3并不能描述symmetric deformation space里的keypoints instances。我们通过引入可以被非对称的加权的symmetric basis来对这种keypoints进行建模，从而：

$$P_i = R_i \left[mat(B_{C\frac{1}{2}} c_i), mat(B_{C\frac{1}{2}}^{'} c_i^{'}) \right] \tag{4}$$

其中$$B_{C\frac{1}{2}}^{'}$$是将$$B_{C\frac{1}{2}}$$利用$$A_C$$反转而得，而$$c_i^{'}$$构成第二部分basis的权重。尽管公式4增加了计算量，但是其增加了模型能够表示symmetry deformation space的能力。从而我们有了如下的proposition：

**Proposition 1** 如果$$B_{C\frac{1}{2}}^{'}$$和$$B_{C\frac{1}{2}}$$关于某个平面对称，如果系数$$c_i$$和$$c_i^{'}$$满足同一个概率分布，那么上述最新的公式就表示了一个symmetric deformation space。

有了proposition 1的结论，我们就可以利用公式4来表示non-rigid symmetric objects，并且只要我们满足$$c$$和$$c^{'}$$的分布相同这样一个条件，我们仍然可以保持symmetry的性质。


**4. Learning Category-Specific Keypoints**

在这一节里，我们通过对$$\Phi_C$$进行建模来描述使用非监督方法学习category-specific keypoints的过程。更加准确地说，我们希望将函数$$\Pi_C: S_i \longrightarrow P_i$$作为一个参数为$$\theta$$的neural network，使用从$$\Phi_C$$里获得的supervisory signal来学习。关于从point sets上学习keypoints，[USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)训练了一个Siamese network来对rigid objects的keypoints进行预测，但keypoints的顺序是不知道的，这个方法对于rotation是稳定的。我们部分的网络结构是受这篇文章的启发的，但它们的源头都是基于[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation
](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf)。我们使用的是单个输入，从而避免了Siamese network很复杂的训练过程。fig 1展示了网络结构，它的输入是一个shape $$S_i$$，且在$$SO(2)$$里是misaligned的。

![ModelssStructure]({{ '/assets/images/SYMMETRY-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 1. Network Architecture。pose and coefficient branch和additional learnable parameters生成了category-specific keypoints。node branch预测那些领导训练过程的nodes。第3节是建模，第4节是训练。

>$$SO(2)$$表示的是det=1的orthogonal空间，也就是rotation matrix组成的空间，2表示是2维的，也就是所有的2维旋转操作组成的group称为$$SO(2)$$，其可以被很简单的表示为$$SO(2) = \left[\left[cos \theta, -sin \theta \right], \left[sin \theta, cos \theta \right]\right]$$，其中$$\theta \in R$$。

这种设置是很合理的，因为point clouds一般都是沿着竖直方向对齐，所以其实它们是misaligned的。

我们下面描述一下网络的各个不同的组成部分。

*Node branch* 

这个branch预测了一个稀疏的nodes集合，它们是潜在的category-specific keypoints，但是并没有被排序。我们将其表示为$$X_i = \lbrace x_{i1}, x_{i2}, \cdots, x_{iN} \rbrace$$，其中$$x_{ij} \in R^3, j \in {1,2, \cdots, N}$$。一开始，作者利用Farthest Point Sampling (FPS)算法来从输入的point clouds里采样$$N$$个node，再使用在[So-net: Self-organizing network for point cloud analysis](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf)和[USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)提到的point-to-node grouping来对每个node选取一个包含了若干points的local neighborhood（实际上所有的points都被唯一的分给了某一个node），这样就创建了$$N$$个clusters。point cloud $$S_i$$里的每个点都和上述这些nodes里的某一个建立关联。这个branch有着两个像PointNet的networks，然后再接了一个KNN groupoing层，再之后再用MLP来输出nodes。

*Pose and coefficients branch* 

我们利用这个branch来预测$$R_i$$和$$c_i$$。对于$$R_i$$，我们可以使用一个旋转角来表示它。这个branch包含一个MLP用来预测这些参数。这个branch的输出size对于公式3和公式4描述的两种情况是不同的，后者的输出size翻倍。


*Additional learnable parameters* 

公式3或者公式4里未知的其它的量对于一个object类别$$\mathcal{C}$$来说是常量。这些量不需要对于每个instance都进行预测。我们选择将其作为网络参数来优化。这些量是shape basis $$B_{\mathcal{C}} \in R^{3N \times K}$$和对称平面的法向量$$n_{\mathcal{C}} \in R^{3}$$。我们发现shape basis的$$K$$的选取最好是在5到10之间。实际上所生成的keypoints对于$$K$$值的选取并不敏感，比较大的$$K$$就会导致比较稀疏的shape coefficient $$c_i$$。我们也可以用其它的量来代替对称平面的法向量，比如说Euler角。

在inference的时候，我们使用Non-Maximal Suppression来获得最终的$$N^{'}$$个keypoints。我们对于某个类别的object的不同instances都输出同样数量的keypoints，因为它们具有同一个geometric model。


**Training Losses**

为了符合我们在section1里所定义的category-specific keypoints所需要满足的要求，我们按照如下方式来定义loss functions。

*Chamfer loss with symmetry and non-rigidity* 

公式1表明这个网络$$\Pi_{\mathcal{C}}$$可以被最小化node predictions $$X_i$$和deformation function $$P_i = \Phi_C(R_i, c_i; B_{\mathcal{C}},n_{\mathcal{C}})$$之间的$$l_2$$loss来实现。但是正如[USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)所说，因为网络不具备预测keypoints顺序的能力，所以$$l_2$$ loss并不会收敛。但Chamfer loss可以收敛，这个loss会先对于$$X_i$$里的每个$$x_{ij}$$找到其在$$P_i$$里最近的那个点$$p_{ik}$$，然后再最小化这个距离，反之亦然。

$$\mathcal L_{chf} = \Sigma_{k=1}^N \mathop min_{p_{i,j} \in P_i} \left | \left | x_{ik} - p_{i,j} \right | \right | ^2_2 + \Sigma_{j=1}^N \mathop min_{x_{i,k} \in X_i} \left | \left | x_{ik} - p_{i,j} \right | \right | ^2_2 \tag{5}$$

公式5里的Chamfer loss保证所学习到的keypoints满足category-specific property里的generalisability——因为它们是对于这个category定义的共用的shape basis的线性组合。为了对对称性建模，公式3和公式4可以用于计算公式5里的$$P_i$$。


*Coverage and inclusively loss* 

公式5表示的Chamfer loss并不能保证keypoints符合object shape。但是我们可以加入如下限定：a) coverage loss：keypoints能够覆盖整个category shape。b) inclusivity loss：keypoints和point cloud离得不远。coverage loss可以通过计算nodes $$X_i$$和point cloud $$S_i$$的volume之间的Huber loss来获得，需要使用到singular values。但我们这里就直接用3D bounding box来算了，因为简单。

$$\mathcal L_{cov} = \left | \left | vol(X_i) - vol(S_i) \right| \right | \tag{6}$$

inclusivity loss用只有一侧的Chamfer loss就可以表示：

$$\mathcal L_{inc} = \Sigma_{k=1}^N \mathop min_{s_{i,j} \in S_i} \left | \left | x_{ik} - s_{ij} \right | \right | ^2_2 \tag{7}$$

>实际上我们发现，这篇文章里的网络结构分为两部分，一部分用来生成nodes，$$X_i$$，表示的是keypoints。而另一部分利用上述的$$P_i = \Phi_C(R_i, c_i; B_{\mathcal{C}},n_{\mathcal{C}})$$来获取keypoints。之后再将这两部分比较从而来获取非监督的signal。而生成nodes的那部分网络实际上和[USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)里的网络结构意义，但是在USIP这篇文章里，其输入是一个point cloud和将这个point cloud经过某种已知的transformation所得到的point cloud构成的一个pair，所以它的非监督label就是这两个point cloud所学习到的keypoints要相对应，应该是处于同一位置。这就是为什么这篇文章不需要使用Siamese结构，也不需要输入一对point cloud的原因，因为它利用shape basis来对keypoints重新提出了一种学习方法。

**5. Experiments**

**6. Conclusion**

这篇文章探究了如何在misaligned 3D point clouds上自动检测到那些能保证对于不同物体之间的shape variation以及同一个物体的deformation都consistent的keypoints的方法。我们发现这个问题可以用一个非监督学习的方式通过用对称的线性basis shapes来表示keypoints这个方式被解决。而且，这些被学习到的category-specific keypoints在不同的输入之间具有1对1的对应关系，并且是semantic consistent的。基于所学习到的keypoints的应用包括registration，generation，shape completion等。我们的实验表明我们的方法可以获得很高质量的keypoints，并且对于更复杂的deformation我们的方法也有潜力。未来的工作方向可以是通过非线性的方式来对更复杂的deformation进行建模（本文里的shape basis是线性组合的）。



### 3. [USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)

[Page](https://github.com/lijx10/USIP)

*Jiaxin Li, Gim Hee Lee*

*ICCV 2019*

3D interest point或者keypoint detection是考虑如何在经过任意的$$SE(3)$$ transformation之后的3D point clouds上找到稳定的并且repeatable的点。

尽管对于2D images来说已经有了很多成功的hand-crafted detectors，但对于3D point clouds来说成功的hand-crafted detectors就几乎没有。这个差异很大程度上是因为对于2D图片来说，我们有具有丰富信息的RGB channels，而对于3D point clouds来说只有点的位置的信息，这对于设计能提取有效信息的hand-crafted keypoints的算法来说是很难的。而且3D point clouds如果在经过任意的transformations之后（也就是说在不同的coordinate frame里），这个难度就更大了。

基于deep learning的3D keypoint detectors很少（实际上现在只有一篇[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)），但是3D keypoint descriptors的文章却越来越多（[PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors]([https://arxiv.org/pdf/1808.10322.pdf](https://openaccess.thecvf.com/content_ECCV_2018/papers/Tolga_Birdal_PPF-FoldNet_Unsupervised_Learning_ECCV_2018_paper.pdf))，[PPFNet: Global Context Aware Local Features for Robust 3D Point Matching](https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf)，[Learning Compact Geometric Features](https://openaccess.thecvf.com/content_ICCV_2017/papers/Khoury_Learning_Compact_Geometric_ICCV_2017_paper.pdf)，[3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.pdf)）（detector仅仅是为了找到keypoint的coordinate，其不关心也不知道keypoints的features或者它和其他数据之间的关系，而keypoint descriptors侧重于获取keypoints的features）。

这篇文章提出USIP detector：一个基于deep learning的非监督的稳定的keypoint detector，其可以在不需要任何监督数据情况下对于做了任意transformation的3D point clouds检测到高度repeatable和精确定位的keypoints。为了达到这个目的，他们提出了一个Feature Proposal Network（FPN）用来从一个输入的3D point cloud上输出一个集合的keypoints以及它们每个点的不确定性。FPN使用的是估计position的方法（也就是说keypoint可能并不是point clouds里任何一个点）来改进了keypoint localization，因为其它的方法（[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)）使用的是在point cloud里选取keypoints，所以会造成误差。在训练过程中，其使用随机生成的$$SE(3)$$ transformation来处理每个point clouds，从而得到point clouds pairs，用作FPN的输入。他们利用probabilistic chamfer loss来最小化训练数据point cloud pairs的keypoints之间的距离。另外他们们还引入了point-to-point loss来迫使keypoints足够靠近point cloud。某些定性的结果如fig 1所示。

![Mfdaf]({{ '/assets/images/USIP-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 1. USIP detector在四个数据集上找到的keypoints的例子：(a) ModelNet40，object model (b) Oxford RobotCar, outdoor SICK LiDAR (c) KITTI (在Oxford上训练的）,outdoor Velodyne LiDAR。

这篇文章的主要贡献：
* USIP detector完全是非监督的，因此避免了获取那些本来就不可能能有的监督信息（3D point cloud上的keypoints的位置）
* 分析了USIP detector可能会不管用的情况，并且提出了避免这些问题出现的解决办法
* FPN通过估计keypoint的位置而不是从point cloud里选择现有的point改进了keypoint localization
* 提出probabilistic chamfer loss和point-to-point loss来找到高度repeatable和准确定位的keypoints
* 在训练时候使用了随机生成的transformation作用在point clouds上，这使得网络对于rotation来说有很好的效果


**Related Work**

和最近很成功的基于deep learning的3D keypoint descriptors不同，大多数现有的3D keypoint detectors仍然还是hand-crafted的。Local Surface Patches（LSP）和Shape Index（SI）基于一个point的最大和最小principal曲率，如果一个point在预先定义的某个领域内是全局极值点，那么就认为这个点是一个keypoint。Intrinsic Shape Signatures（ISS）和KeyPoint Quality（KPG）选取那些沿着每个主轴都有很大的变化的那些点作为keypoints。MeshDoG和Salient Points（SP）利用类似于SIFT的Difference of Gaussian operator构建了一个曲率的scale space，有着局部最大值的那些点被选为keypoints。Laplace-Beltrami Scale-space（LBSS）通过对每个点使用一个Laplace-Beltrami operator来选取keypoints。更近的工作，LORAX提出将point set投射到一个depth map上，再利用PCA来选择那些具有普遍geometric characteristics的keypoints。所有的hand-crafted方法都是靠着point clouds的局部的几何特征来选择keypoints的。因此，这些detectors的表现会因为扰动，比如说noise，density variations或者transformations，而效果变差。

在这篇文章之前，仅有的基于deep learning的3D keypoint detector就是weakly supervised 3DFeatNet（[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)），其利用GPS/INS标注的point clouds来训练。然而，3DFeat-Net的训练很大程度上利用Siamese结构来学习具有区分性的descriptors。其并没有确保keypoint detection具有良好的效果。相比较而言，USIP被设计为可以得到高度repeatable和定位准确的keypoints。更进一步的是，USIP的方法是完全非监督的，并不依赖于任何有标注的数据集。


**Our USIP Detector**

Fig 2的(a)解释了USIP detector的pipeline。将一个point cloud记为$$X = \left[ X_0, \cdots, X_N \right] \in R^{3 \times N}$$。一个集合的transformation matrices $$\lbrace T_1, \cdots, T_L \rbrace$$，其中$$T_l \in SE(3)$$是随机生成的，其应用到point cloud $$X$$上就得到了$$L$$对训练inputs，记为$$\lbrace \lbrace X, \tilde{X_1} \rbrace, \cdots, \lbrace X, \tilde{X_L} \rbrace \rbrace$$，其中$$\tilde{X_l} = T_l X \in R^{3 \times N}$$。我们丢掉$$l$$下标，将训练input pair和它们对应的transformation matrix构成的三元组记为$$\lbrace X, \tilde{X}, T \rbrace$$。在训练期间，$$X$$和$$\tilde{X}$$分别喂给FPN，都会输出$$M$$个proposal keypoints和它们对应的不确定性，分别记为$$\lbrace Q = \left[ Q_1, \cdots, Q_M \right], \Sigma = \left[ \sigma_1, \cdots, \sigma_M \right]^T \rbrace$$，和$$\lbrace \tilde{Q} = \left[ \tilde{Q_1}, \cdots, \tilde{Q_M} \right], \tilde{\Sigma} = \left[ \tilde{\sigma_1}, \cdots, \tilde{\sigma_M} \right]^T \rbrace$$。其中$$Q_m \in R^{3}$$，$$\tilde{Q_m} \in R^3$$，$$\sigma_m \in R^{+}$$，$$\tilde{\sigma_m} \in R^{+}$$。为了提升keypoint localization的精度，不需要$$Q_m \in Q$$是$$X$$里的任何点。对于$$\tilde{Q_m}$$也是一样。

对于$$\tilde{Q}$$，我们再计算$$Q^{'} = T^{-1}\tilde{Q} \in R^{3 \times M}$$，而$$Q^{'}$$应该和$$Q$$很像。这里做一个假设，也就是每个keypoint的不确定性，在经过$$T^{-1}$$操作后不会改变，也就是说$$\Sigma^{'} = \tilde{\Sigma}$$。从经历过任意transformation的3D point clouds里检测具有高度repeatble特性以及定位准确的keypoints的任务就可以通过最小化$$Q$$和$$Q^{'}$$来实现。为了实现这个目标，文章提出loss function：$$\mathcal{L} = \mathcal{L_c} + \lambda \mathcal{L_p}$$，其中$$\mathcal{L_c}$$是probabilistic chamfer loss，用于最小化$$Q$$和$$Q^{'}$$里的对应的keypoints的probabilistic距离。$$\mathcal{L_p}$$是point-to-point loss，用来最小化所得到的keypoints和距离它最近的point clouds里的点的距离。$$\lambda$$是个超参数。

*Probabilistic Chamfer Loss*

一个最小化$$Q$$和$$Q^{'}$$之间距离的方法就是使用Chamfer loss：

$$ \Sigma_{i=1}^M min_{Q_j^{'} \in Q^{'}} \left | \left | Q_i - Q_j^{'} \right | \right | + \Sigma_{j=1}^M min_{Q_i \in Q} \left | \left | Q_i - Q_j^{'} \right | \right |  \tag{1}$$

公式1最小化一个point cloud里的点和其在另一个point cloud里最近的那个点之间的距离。然而，$$M$$个proposals并不是等重要的。如果$$Q$$里面的点$$Q_i$$的位置并不好，如果还按照上述的方式来计算，那么其也会导致$$Q_i^{'}$$的结果也不好。

为了解决上述这个问题，FPN同时也学习每个keypoint proposal的不确定性$$\Sigma$$和$$\Sigma^{'}$$，再计算一个probabilistic chamfer loss $$\mathcal{L_c}$$。对于$$Q_i$$和$$Q_j^{'}$$，$$i=1,\cdots,M$$，其上面定义的概率分布为：

$$p(d_{ij} | \sigma_{ij}) = \frac{1}{\sigma_{ij}} exp(-\frac{d_{ij}}{\sigma_{ij}}) \tag{2}$$

其中

$$\sigma_{ij} = \frac{\sigma_i + \sigma_j^{'}}{2}$$

$$d_{ij} = min_{Q_j^{'} \in Q^{'}} \left | \left | Q_i - Q_j^{'} \right | \right | \geq 0$$

$$p(d_{ij} | \sigma_{ij})$$

是一个合规的概率分布。$$d_{ij}$$越小，那么proposal keypoints $$Q_i$$和$$Q_j^{'}$$是高度repeatable以及定位准确的keypoints的概率就越高。

假设对于所有的$$d_{ij} \in D_{ij}$$，$$Q$$和$$Q^{"}$$之间的联合分布是：

$$p(D_{ij} | \Sigma_{ij}) = \Pi_{i=1}^M p(d_{ij} | \sigma_{ij}) \tag{3}$$

因为最近邻的选择不同，所以说$$d_{ij} \neq d_{ji}$$，且$$\sigma_{ij} \neq \sigma_{ji}$$。

$$Q^{"}$$和$$Q$$之间的联合分布是：

$$p(D_{ji} | \Sigma_{ji}) = \Pi_{j=1}^M p(d_{ji} | \sigma_{ji}) \tag{4}$$

其中

$$\sigma_{ji} = \frac{\sigma_i + \sigma_j^{'}}{2}$$

$$d_{ji} = min_{Q_i \in Q} \left | \left | Q_i - Q_j^{'} \right | \right | \geq 0$$

最后概率chamfer loss就定义为：

$$\mathcal{L_c} = \Sigma_{i=1}^M -lnp(d_{ij} | \sigma_{ij}) + \Sigma_{j=1}^M -lnp(d_{ji} | \sigma_{ji})$$

$$ = \Sigma_{i=1}^M (ln \sigma_{ij} + \frac{d_ij}{\sigma_{ij}}) + \Sigma_{j=1}^M (ln\sigma_{ji} + \frac{d_{ji}}{\sigma_{ji}}) \tag{5}$$

通过计算公式2关于$$\sigma_{ij}$$的导数来分析其的物理含义：

$$\frac{\partial p(d_{ij} | \sigma_{ij})}{\partial \sigma_{ij}} = 0$$

可得$$\sigma_{ij} = d_{ij}$$。

这说明，给定一个$$d_{ij} >0$$，$$\sigma_{ij} = d_{ij}$$的时候，上述概率取到最大值。假设我们有三个proposal keypoints，$$(Q_i,Q_j^{'},Q_k^{'})$$，其中$$d_{ij}$$和$$d_{ki}$$是两对keypoints pairs的最近邻距离。当$$d_{ij} \longrightarrow 0$$而且$$d_{kj}$$很大的时候，我们需要$$\sigma_k^{'}$$的值很大。这也就是说，$$\lbrace Q_i, Q_j^{'} \rbrace$$是高度repeatable且精确定位的keypoints，而$$Q_k^{'}$$不是。因此，$$\sigma_k^{'}$$比较大，说明我们的概率chamfer loss是定义正确的。


*Point-to-point loss*

为了减小keypoints localization的错误，不需要keypoints是point cloud里的任何一个点。但是这可能会让keypoints离point cloud太远。通过引入一个loss function $$\mathcal{L_p}$$来解决这个问题。

$$\mathcal{L_{point}} = \Sigma_{i=1}^M  min_{X_j \in X} \left | \left | Q_i - X_j \right | \right | + \Sigma_{i=1}^{M}  min_{\tilde{X_j} \in \tilde{X}} 
\left | \left | \tilde{Q_i } - \tilde{X_j} \right | \right | $$

其中$$X_j \in X$$是$$Q_i$$在point clouds里最近的点。


**Feature Proposal Network**

FPN的结构如fig 2 (b)所示。

**Step1** 从给定的输入point cloud $$X \in R^{3 \times N}$$使用Farthest Point Sampling采样$$M$$个nodes，记为$$S = \left[ S_1, \cdots, S_M \right] \in R^{3 \times M}$$。之后才采用[So-net: Self-organizing network for point cloud analysis](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf)里的point-to-node grouping的方法（也就是每个points，将其归属于距离它最近的那个nodes）来为每个$$S_m \in S$$构造一个points构成的neighborhood，从而我们获得了

$$\lbrace \lbrace X_1^1 |S_1, \cdots, X_1^{K_1} |S_1 \rbrace, \cdots, \lbrace X_M^1 |S_M, \cdots, X_M^{K_M} |S_M \rbrace \rbrace$$

其中$$K_1, \cdots, K_M$$表示每个nodes的neighborhood points的数量。

>Farthest Point Sampling的解释可以看[这篇博客](https://jskhu.github.io/fps/3d/object/detection/2020/09/20/farthest-point-sampling.html)

>point-to-node方法比node-to-point KNN或者radius-based ball-search要好在以下两个方面：(1) $$X$$里的每个点都唯一的归于了某个node的neighborhood，而另外两个方法可能会有些点没有归属；(2) point-to-node grouping方法对于不同的scale以及point density都可以适应，而KNN search受到density变化以及ball-search受到scale变化的影响很大。

**Step2** 为了使得FPN是translation equivariant的，我们将每个node构成的neighborhood都进行归一化，记归一化之后的结果为

$$\lbrace \hat X_m^1 | S_m, \cdots, \hat X_m^{K_m} | S_m \rbrace$$

其中$$\hat X_m^{k}  = X_m^{k} - S_m$$，$$1 \leq k \leq K_m$$。

**Step3** 将经过上述操作之后的point cloud $$X \in R^{3 \times N}$$（也就是将$$X$$按照选择出来的$$M$$个nodes分为$$M$$个clusters，然后再将每个node对应的cluster除了node以外的点都减去node的坐标实现归一化）被喂给如fig 2 (b)里所示的一个类似PointNet（[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf)）的网络结构，这样$$X$$里的每个点都有一个feature，再将每个cluster里的points的features综合起来获取一个和$$S_m$$相关的局部feature向量$$G_m$$，从而我们就得到了

$$\lbrace G_1 |S_1, \cdots, G_M |S_M \rbrace$$

**Step4** 再之后，对于每一个$$(G_m, S_m)$$，我们使用KNN来找到$$G_m$$的K个最近的向量（是在上述nodes的feature空间里找，而不是在所有的$$X$$里的points经过了之前的PointNet之后所得到的feature空间里找，也就是在

$$\lbrace G_1 | S_1, \cdots, G_M |S_M \rbrace$$

里利用KNN来搜索）。这也就是，利用KNN grouping layer应用在在局部feature向量集合里

$$\lbrace G_1 | S_1, \cdots, G_M |S_M \rbrace$$

来获取层次化的信息综合。对于每个$$(G_m, S_m)$$来说，将其找到的K个最近的node feature向量记为

$$\lbrace (G_m^1|S_m^1)|S_m, \cdots, (G_m^K|S_m^K)|S_m \rbrace$$

这些KNN的局部feature向量再次将$$S_m$$的坐标减去，从而获得了和位置无关的局部feature向量，记为

$$\lbrace (G_m^1|\hat S_m^1)|S_m, \cdots, (G_m^K|\hat S_m^K)|S_m \rbrace$$

其中$$\hat S_m^k = S_m^k - S_m$$，$$1 \leq k \leq K$$

**Step5** 之后再将其送入另一个network来获得feature向量$$\lbrace H_1, \cdots, H_M \rbrace$$。

**Step6** 再利用一个MLP来预测$$M$$个proposal keypoints，

$$\lbrace \hat Q_1|S_1, \cdots, \hat Q_M|S_M \rbrace$$

其中$$\hat Q_m \in R^3$$，以及预测点的不确定性$$\lbrace \sigma_1, \cdots, \sigma_M \rbrace$$。

**Step7** 最后，我们再将$$\hat Q_m$$还原回去，$$Q_m = \hat Q_m + S_m$$来获得最终的keypoint proposals $$\lbrace Q_1, \cdots, Q_M \rbrace$$。

我们需要注意，每个keypoint感受野的大小取决于proposal的数量$$M$$以及KNN算法里的$$K$$的大小。其也决定了每个keypoints feature的描述细节的能力。大的感受野能够使得keypoint具有描述大范围细节的能力。

![dagarf]({{ '/assets/images/USIP-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 2. (a) USIP detector的训练pipeline (b) Feature Proposal Network (FPN)的结构。


**Degeneracy Analysis**

我们将FPN记为：$$f(Y): Y \longrightarrow R^3 \times N$$，其中$$Y = \lbrace Y_1, \cdots, Y_N \rbrace \in R^{3 \times N}$$表示的是网络的输入，也就是point clouds。我们进一步将一个transformation matrix记为$$T \in SE(3)$$，而$$R \in SO(3)$$和$$t \in R^3$$分别表示$$T$$的rotation matrix和translation vector。从而对于经过$$T$$变换后的point clouds，我们记为$$Y^{'}$$，我们就有$$Y^{'} = RY+t$$。如果对于任意的$$R$$和$$t$$，都有$$f(Y^{'}) = Rf(Y)+t$$，那就认为网络是degenerate的。

*Lemma 1* 如果$$f(.)$$输出的是point cloud的centroid，也就是说$$f(Y) = \frac{1}{N} \Sigma_n Y_n$$，那么$$f(Y^{'})$$恒等于$$Rf(Y)+t$$。

*Lemma 2* 如果$$f(.)$$是translational equivariant的，i.e., $$f(x)t = f(xt)$$，而且先计算$$Y$$里所有的点的covariance matrix，再利用SVD解出U，即可得到principal axis的方向（也就是U的三个列表示的向量的方向），如果$$f(Y)$$得出的keypoints都在principal axis上，那么$$f(Y^{'})$$恒等于$$Rf(Y)+t$$。


上述两个lemma可以看出，当点在一些特殊的位置上，比如说point cloud的中心，或者说主轴上（比如说对称物体的对称轴），那么就会导致degenerate的结果。而实际上如果想让网络输出这样的结果，那我们需要网络的每个keypoint的感受野很大才行，因为不管是中心，还是主轴，这都是整个point cloud的性质，如果说keypoint只能注意到局部的特征，那么就不会出现上述的情况，也就避免了degenerate情况的发生。在这个FPN里，则是通过设置$$M$$和$$K$$值来控制感受野。较小的$$M$$和较大的$$K$$都会使得感受野变得很大，从而FPN就会degenerate。

fig 3显示了几个不同的$$K$$的值导致的degenerate的情况，在这三种情况下都有$$M=64$$。实验还表明，对于FPN来说，更加规则的物体，也就是中心或者对称轴更加好定义的物体，其出现degenerate的情况就越容易。

![degenerate]({{ '/assets/images/USIP-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 3. (a) K=9，没有degenerate情况 (b) K=24，出现了keypoints都在主对称轴上的情况 (c) K=64，keypoints全部都跑到中心点去了。



### 4. [Unsupervised Learning of Visual 3D Keypoints for Control](http://proceedings.mlr.press/v139/chen21b/chen21b.pdf)
[Code](https://github.com/buoyancy99/unsup-3d-keypoints) [Post](https://buoyancy99.github.io/unsup-3d-keypoints/)

*Boyuan Chen, Pieter Abbeel, Deepak Pathak*

*ICML 2021*

这篇文章使用了一种非监督的方式，设计了一个end-to-end的模型，直接从2D图片里学习到3D keypoints。其是通过一个multi-view consistency约束以及一个下游任务来训练网络的。

绝大多数keypoint detection的方法要么就是hand-crafted keypoint，要么就是使用监督信息进行学习。利用非监督的方法学习keypoints的最近的工作有：[Unsupervised Learning of Object Structure and Dynamics from Videos](https://proceedings.neurips.cc/paper/2019/file/d82c8d1619ad8176d665453cfb2e55f0-Paper.pdf)和[Unsupervised Learning of Object Keypoints for Perception and Control](https://proceedings.neurips.cc/paper/2019/file/dae3312c4c6c7000a37ecfb7b0aeb0e4-Paper.pdf)，但他们学习到的是2D的keypoints。但是控制机器人的话，我们需要3D的keypoints。

这篇文章使用了一种非监督的方式，设计了一个end-to-end的模型，直接从2D图片里学习到3D keypoints。其是通过一个multi-view consistency约束以及一个下游任务来训练网络的。为了让模型具有普适性并且效果好，其需要满足以下三个性质：(a) 在3D空间里的consistency，也就是说从同一个场景的不同的scenes里所学习到的3D keypoints在3D空间里应该位于同一个位置。(b) 在时间上consistency：同一个keypoint在时间上也应该是连续的 (c) Joint learning with control：因为我们是为了control任务来找的keypoints，联合训练可以使得我们找到的keypoints更加有效。

给定某个camera view的一张image，我们首先预测在image space里的keypoints locations和depth。对于这些从不同的camera views获得的这些keypoints，利用一个differentiable unprojection操作来获得每个keypoint的world coordinate。通过multi-view consistency loss来学习到的不同view之间的consistency可以使得不同view的keypoints可以映射到同一个world coordinate上。这个world coordinate再投射到每个camera view对应的image plane上来重构输入的原image。这样的设计构造了一个differentiable 3D keypoint bottleneck。我们的keypoint学习和RL任务是同时被优化的。整个过程由fig 1所示。我们的模型叫做Keypoint3D。

我们使用了一个multi-view encoder-decoder的结构不使用监督信息来学习3D keypoints。给定同一个场景的$$N$$个view，我们为每个view都给一个encoder和一个decoder。我们为学习到3D keypoints提供了三个unsupervised的signal：1) 我们通过让不同view所学习到的keypoints都能映射到同一个3D空间内的3D keypoint来迫使所学习到的keypoints具有geometrically consistent的特性。2) 我们利用reconstruction loss来惩罚decoder的不准确的reconstruction。3) 我们利用RL任务的reward来反向传播到encoder里从而训练encoder的参数。

$$I_n \in R^{H \times W \times C}$$表示相机$$n$$的输入image，$$n \in 1,\cdots, N$$，而相机$$n$$具有extrinsic matrix $$V_n$$，和intrinsic matrix $$P_n$$。$$K$$是我们keypoints的个数。对于一个3D空间里的点$$\left[x,y,z\right]^T$$和camera $$n$$，我们可以使用extrinsic matrix $$V_n$$和perspective intrinsic matrix $$P_n$$来将其投射到camera coordinate $$\left[u, v, d\right]^T$$，其中$$u,v \in \left[0,1\right]$$是camera plane上归一化后的coordinate，$$d>0$$是depth value，也就是那个点距离camera plane的距离。operator $$\Omega_n: \left[x,y,z\right]^T \longrightarrow \left[u,v,d\right]^T$$表示上述的这种投射，而其的inverse记为$$\Omega_n^{-1}$$。$$\Omega_n, \Omega_n^{-1}$$都是differentiable的，而且可以被解析表示。


**Step1 Keypoint Encoder**

对于每个camera $$n$$，我们将$$I_n$$喂给一个fully convolutional encoder $$\phi_n$$来获得$$k$$个confidence maps，$$C_n^k \in R^{S \times S}$$，以及depth maps，$$D_n^k \in R^{S \times S}$$。对于每个confidence map，我们使用一个spatial softmax来计算一个probability heatmap $$H_n^k$$：

$$H_n^k(i,j) = \frac{exp(C_n^k(i,j)}{\Sigma_{p=1}^S \Sigma_{q=1}^{S} exp(C_n^k(p,q))}$$

heatmap $$H_n^k \in R^{S \times S}$$里的每个值表示的是一个3D keypoint $$k$$出现在从camera $$n$$的角度产生的2D image plane上的这个点的概率。而depth map $$D_n^k$$表示的是在每个camera plane的位置，这个3D keypoint距离camera plane的距离。

然后我们就可以计算keypoint $$k$$在camera $$n$$的camera plane下的坐标了：

$$E\left[u_n^k\right] = \frac{1}{S} \Sigma_{u,v} u H_n^k(u,v)$$

$$E\left[v_n^k\right] = \frac{1}{S} \Sigma_{u,v} v H_n^k(u,v)$$

$$E\left[d_n^k\right] = \Sigma_{u,v} u D_n^k(u,v) H_n^k(u,v)$$

注意到，对于$$u,v$$的计算都除以了$$S$$，也就是图片尺寸，是因为正如我们之前提到的，我们计算的是归一化之后的相机坐标下的keypoint的坐标。

记$$\left[\hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T = \left[E\left[u_n^k\right], E\left[v_n^k\right], E\left[d_n^k\right]\right]^T$$。

为了进一步增加我们的方法的可靠性，我们并不直接将上述的encoder的结果作为keypoint的camera plane的坐标值，而是利用一个高斯分布，其均值为这个值，方差为1，在整个image平面上随机选取，进一步增加了模型的可靠性。


**Step2 Attention**

在预测了每个camera coordinate frame下每个keypoints的坐标之后，我们要想办法将每个keypoint的$$n$$个不同camera下的坐标统一起来。一个最简单的方法就是取平均。但是在某些角度下的keypoints可能被遮挡，从而预测效果并不好。为了解决这个问题，我们利用之前的confidence maps来设计一个加权平均。这使得我们对于那些不那么自信的view里获得的keypoint的权重要小一些，从而不影响整体的效果。

我们可以为每个camera $$n$$获取的keypoint $$k$$设置一个confidence score，其和confidence map $$C_n^k$$的平均值成比例，而且对于$$K$$个keypoints，还做了归一化处理：

$$A_n^k = \frac{exp(\frac{1}{S^2}\Sigma_{p=1}^S \Sigma_{q=1}^S C_n^k(p,q))}{\Sigma_{i=1}^K exp(\frac{1}{S^2}\Sigma_{p=1}^S \Sigma_{q=1}^S C_n^i(p,q))}$$

这个就可以被理解为，对于camera $$n$$来说，其对于每个估计到的keypoint分配的概率，这$$K$$个keypoints的概率总和为1。


**Step3 Extracting world coordinates**

给定之前由encoder预测到的camera plane内的keypoints $$\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T$$，$$n= 1,\cdots, N$$，$$k=1, \cdots, K$$，我们可以将其反投射回world coordinates：$$\left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T = \Omega_n^{-1}(\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T)$$。这个就是从camera $$n$$获取到的keypoint $$k$$的world coordinate。对于每个keypoint，我们都有$$N$$个预测的结果，我们利用之前计算的$$A_n^k$$来为每个keypoint计算一个加权的world coordinate：

$$\left[\bar x^k, \bar y^k, \bar z^k \right]^T = \Sigma_{n=1}^N \frac{A_n^k}{\Sigma_{m=1}^N A_n^m} \left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T$$

**Step4 Keypoint Decoder**

我们在decoder之前，还需要将$$K$$个keypoints都再投射回camera plane来增强模型的学习能力。对于每个camera $$n$$和每个keypoint，我们有$$\left[\bar u, \bar v, \bar d \right]^T = \Omega_n(\left[\bar x^k, \bar y^k, \bar z^k \right]^T)$$。为了获取空间结构信息，对于每个camera和每个keypoint，我们都构建一个高斯分布$$G_n^k \in R^{S \times S}$$，均值为$$\left[\bar u, \bar v \right]$$，方差为$$I_2 / \bar d$$。这个分布使得离得近的那些keypoint，在camera plane上具有更分散的分布。

我们记$$\bar A^k = \frac{1}{N} \Sigma_{n=1}^N A_n^k$$为对于所有view的平均attention，每个camera的decoder $$\psi_n$$就将stacked的高斯maps $$G_n$$作为输入来重构输入image $$I_n$$，而$$G_n = K stack(\left[G_n^1 \bar A^1, \cdots, G_n^K \bar A^K \right])$$。


![Model Structure]({{ '/assets/images/CONTROL-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. Overview of our Keypoint3D algorithm. (a) 对于每个camera view，一个CNN将输入image编码为$$K$$个heatmaps以及depth maps；(b) 我们将这些heatmaps当作概率来计算camera plane下keypoint横纵坐标的。我们同时也用heatmap和depth map来计算每个keypoint的深度d，这些$$\left[u,v,d\right]$$再被反投射回到world coordinate里；(c) 我们利用之前的heatmaps来计算每个camera view对于每个keypoint的置信概率，然后对于每个keypoint，我们计算出一个加权的world coordinate；(d) 我们再将每个keypoint的world coordinate投射到每个camera plane上；(e) 从而对于每个camera plane，都有$$K$$个这样的投射，建立$$K$$个高斯map，将它们叠起来，作为decoder的输入，来重构原输入图片；(f) 除了上述的这些loss，我们还将所学习到的3D keypoint的world coordinate与下游任务相结合，来共同优化这个网络。*


### [6-DoF Object Pose from Semantic Keypoints](https://arxiv.org/pdf/1703.04670.pdf?ref=https://githubhelp.com)

*Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstantinos G. Derpanis, Kostas Daniilidis*

*ICRA 2017*

这篇文章解决的是从单张image来估计一个object的6自由度姿态，也就是3维空间内的translation和rotation。

我们的方法将描述appearance的statistical model和object的3D shape layout结合起来用于pose estimation。这个方法包括两个stages，首先通过一个集合的2D semantic keypoints来推出3D object投射到2D图片上的shape是什么样的，然后利用这些keypoints来估计3D object的pose。整个过程在fig 1里被详细描述。在第一个stage，我们使用一个CNN来预测一个集合的semantic keypoints。这里，我们利用了CNN的利用层次化设计能够获得很大感受野的信息的特性来获得semantic keypoints。在第二个stage，我们利用这些semantic keypoints来显式推出intra-class shape variability以及用camera model描述的camera pose。pose estimates通过最大化deformable model和2D semantic keypoints之间的geometric consistency来实现。

![Pipeline]({{ '/assets/images/6DOF-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1 方法的pipeline。给定一个object的一张RGB image (a)，我们使用stacked hourglass结构的CNN来预测一系列keypoints。这一步的结果是一系列的heatmaps，在(b)里合起来用作可视化。在(c)里，绿色的点表示我们网络输出的keypoints的位置，蓝色的点表示ground truth keypoints的位置。(d)显示了object的6自由度pose.*

这篇文章的贡献在于：

* 提出了一个高效的方法，将通过一个CNN学习到的semantic keypoints和一个deformable shape model结合起来用来估计一个object的3D pose信息（也就是是6自由度pose，3D translation和rotation）。
* 通过实验证明我们的方法在很复杂的环境中也能有效的学习到准确的6自由度pose。

**Method**

我们的pipeline包括了object detection，keypoint localization和pose optimization。因为object detection已经被研究的很好了，比如Fast R-CNN，所以我们利用现成的方法获取object的detection box，主要聚焦于后面两部分。

*Keypoint localization*

keypoint localization所用的CNN结构是[Stacked Hourglass Network for Human Pose Estimation](https://arxiv.org/pdf/1603.06937.pdf)。

网络结构如fig 2所示，网络的输入是一个image，而输出是一系列的heatmaps，每个heatmap对应一个keypoint，而heatmap每个点的值表示该keypoint出现在这个点的概率。我们有真实的keypoints location作为训练的监督数据，将每个真实的keypoint location也做成heatmap，其是以真实的location为中心，以1为标准差的高斯分布。再将这些真实的heatmaps与网络输出的heatmaps计算$$l_2$$ loss来训练网络。

![Model Structure]({{ '/assets/images/6DOF-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2 stacked hourglass结构示意图。我们这里使用了两个hourglass modules叠起来。hourglass的结构设计可以允许在每个hourglass的前半部分bottom-up处理（从高分辨率到低分辨率），再在每个hourglass的后半部分实现top-down处理（从低分辨率到高分辨率）。这里将两个hourglass叠起来使用，而训练的时候可以同时对第一个hourglass的输出以及第二个hourglass的输出同时进行监督，从而提供更丰富的监督信号。*

*Pose Optimization*

我们对于每个object类别，使用3D CAD构建了具有标注keypoints的deformable shape model。更加准确地说，一个3D object model上的$$p$$个keypoint locations被记为$$S \in R^{3 \times p}$$，以及$$S = B_0 + \Sigma_{i=1}^k c_i B_i$$，其中$$B_0$$是这个3D model的平均shape，$$B_1, \cdots, B_k$$是通过PCA计算出来的几种可能的shape variability。

给定图片中所检测到的2D keypoints，记为$$W \in R^{2 \times p}$$，我们的目标是要估计object和camera frame之间的rotation $$R \in SO(3)$$以及translation $$T \in R^3$$，和shape deformation的参数$$c = \left[c_1, \cdots, c_k\right]^T$$。

上述可以综合为以下的optimization问题：

$$ min_{\theta} \frac{1}{2} \left| \left| \xi (\theta) D^{\frac{1}{2}} \right| \right| + \frac{\lambda}{2} \left| \left| c \right| \right| \tag{1}$$

为了将2D keypoint预测的不确定性考虑进来，我们定义了一个对角权重矩阵$$D \in R^{p \times p}$$：

$$\begin{pmatrix}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & \cdots & \vdots \\
0 & 0 & \cdots & d_p
\end{pmatrix}$$

其中$$d_i$$表示keypoint $$i$$的localization的确信值。我们直接将$$d_i$$的值设置为每个keypoint的heatmap的峰值。

前面的$$\xi (\theta)$$是fitting residual，衡量hourglass网络得出的2D keypoints和3D keypoints之间的差异。我们考虑两种相机模型。

(1) *Weak Perspective Model*

如果camera的intrinsic parameters是未知的，就采用weak perspective model，当camera距离object较远的时候也是对full perspective model的一个较好的近似。在这种情况下：

$$\xi (\theta) = W - s\bar R \left( B_0 + \Sigma_{i=1}^k c_i B_i \right) - \bar T 1^T $$

其中$$s$$是一个scalar，$$\bar R \in R^{2 \times 3}$$和$$\bar T \in R^2$$分别表示$$R$$和$$T$$的前两行，从而我们未知的参数就是$$\theta = \lbrace s, c, \bar R, \bar T \rbrace$$。

(2) *Full Perspective Model*

如果我们知道camera intrinsic parameters，那么我们就可以构建full perspective camera model，从而：

$$ \xi (\theta) = \tilde W Z - R \left( B_0 + \Sigma_{i=1}^k c_i B_i \right) -T 1^T$$

其中$$\tilde W \in R^{3 \times p}$$表示的是2D keypoints的normalized homogeneous coordinates，$$Z$$是一个对角矩阵：

$$\begin{pmatrix}
z_1 & 0 & \cdots & 0 \\
0 & z_2 & \cdots & 0 \\
\vdots & \vdots & \cdots & \vdots \\
0 & 0 & \cdots & z_p
\end{pmatrix}$$

其中$$z_i$$表示3D keypoint $$i$$距离camera plane的距离，也就是depth。从而未知的参数就是$$\theta = \lbrace Z, c, R, T \rbrace$$。


### 5. [UKPGAN: A General Self-Supervised Keypoint Detector](https://arxiv.org/pdf/2011.11974.pdf)

[code](https://github.com/qq456cvb/UKPGAN)

*Yang You, Wenhai Liu, Ynajie Ze, Yong-Lu Li, Weiming Wang, Cewu Lu*

*CVPR 2022*


传统的hand-crafted 3D keypoint detectors包括[Harris 3D: a robust extension of the Harris operator for interest point detection on 3D meshes](http://ivan-sipiran.com/papers/SB11b.pdf)，[A concise and provably informative multi‐scale signature based on heat diffusion](http://ki-www.cvl.iis.u-tokyo.ac.jp/class2013/2013w/paper/correspondingAndRegistration/02_Sun.pdf)，[Sparse points matching by combining 3D mesh saliency with statistical descriptors](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.7406&rep=rep1&type=pdf)，[Mesh saliency](https://web.archive.org/web/20170829211638id_/http://www.cs.princeton.edu/courses/archive/fall10/cos526/papers/lee05.pdf)，[Intrinsic shape signatures: A shape descriptor for 3d object recognition](https://www.researchgate.net/profile/Yu-Zhong-23/publication/224135303_Intrinsic_shape_signatures_A_shape_descriptor_for_3D_object_recognition/links/00b4952b8550b1c21c000000/Intrinsic-shape-signatures-A-shape-descriptor-for-3D-object-recognition.pdf)，[Volumetric image registration from invariant keypoints](https://web.stanford.edu/group/rubinlab/pubs/Rister-2017a.pdf)和[Scale-dependent 3D geometric features](https://www.researchgate.net/profile/Ko-Nishino/publication/224297791_Scale-Dependent_3D_Geometric_Features/links/5591f49d08ae1e1f9bb00e4d/Scale-Dependent-3D-Geometric-Features.pdf)。这些方法都过于依赖人为定义的参数，仅仅考虑local geometric information而没有global semantic information，和现在的基于学习的方法效果不能比，和真人更是差得远。

最近有一些基于学习的3D keypoint detectors被提了出来，比如[D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features](https://openaccess.thecvf.com/content_CVPR_2020/papers/Bai_D3Feat_Joint_Learning_of_Dense_Detection_and_Description_of_3D_CVPR_2020_paper.pdf)和[USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)。USIP从分割之后的局部clusters里学习到keypoint locations，而且利用了一个probabilistic chamfer loss。其使用了farthest point sampling的方法而且会产生并不和任何point cloud中的点重合的keypoint。D3Feat对于point cloud里的每个点都计算saliency值以及descriptors。USIP和D3Feat都是通过解决预测两个输入的point clouds之间的rotations这样一个代理任务来预测3D keypoints的位置。它们在训练中都需要3D point cloud，而且对于输出的keypoints并没有过多的控制。

这篇文章采用了完全不同的方式来获取3D keypoints，叫做unsupervised keypoint GANeration (UKPGAN)。对于detector network，这个方法使用了一个keypoint saliency distribution，并且使用了一个新的GAN loss来控制这个distribution的稀疏性。之后，为了使得学到的keypoints更加semantic，文章使用了一个salient information distillation的方法从这些检测到的稀疏的keypoints来重构原输入的keypoint cloud，从而构成了一个encoder-decoder的结构。

这篇文章提出的方法可以被看成一个information compression方法，也就是使用最少量的keypoints来保持object的point cloud里最多的信息。这个方法背后的道理很简单有效：我们应该可以从稀疏的keypoints上重构一个object的结构。

和之前的方法相比，UKPGAN有以下几个优势：1) 不需要任何数据增强操作，通过先估计一个local reference frame，这个detector就可以做到rotation invariant，而且keypoint的descriptors也是rotation invariant的。2) 所检测到的keypoints是类内高度consistent的，对于rigid和non-rigid objects都是。3) 我们的模型是在干净的数据集上训练的（比如ModelNet），推广到真实世界的point cloud上仍然可行。


**Related work**

hanf-crafted 3D keypoint detectors前面已经列举了。重点关注learning-based keypoint detectors，对于2D来说，关注那些unsupervised的方法，对于3D来说，都要关注。

对于2D unsupervised keypoints detectors来说，[Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)通过将target image通过一个窄的bottleneck来提取object的geometry的方式来学到semantically有意义的keypoints。[Unsupervised Discovery of Object Landmarks as Structural Representations](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.html)使用的是autoencoder框架，并且利用channel-wise softmax来检测keypoints。[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://papers.nips.cc/paper/2018/hash/24146db4eb48c718b84cae0a0799dcfc-Abstract.html)通过要求multi-view consistency来从2D images里找到latent 3D keypoints。[End-to-end learning of keypoint detector and descriptor
for pose invariant 3D matching](https://openaccess.thecvf.com/content_cvpr_2018/papers/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.pdf)使用一个Siamese结构再加上一个sampling层和一个score loss来在depth maps上检测keypoints。

在3D keypoints detectors领域，SyncSpecCNN方法[]()以及deep functional dictionaries方法[]()需要ground-truth的keypoints locations作为监督信号。对于unsupervised learning方法，USIP方法[]()利用将point cloud分割为clusters，再在clusters上学习到keypoints，并且结合了probablistic chamfer loss的方法来学习keypoints locations。D3Feat方法[]()为每个point cloud的点都计算一个saliency score和descriptor。USIP和D3Feat都是依赖于一个检测一对point cloud之间的rotation这样一个代理任务来学习keypoints的，其并没有多关心semantic information。而还存在另一条line of research：[Unsupervised Learning of Intrinsic Structural Representation Points](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Unsupervised_Learning_of_Intrinsic_Structural_Representation_Points_CVPR_2020_paper.pdf)，[Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point Sets](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700545.pdf)和[KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control](https://openaccess.thecvf.com/content/CVPR2021/papers/Jakab_KeypointDeformer_Unsupervised_3D_Keypoint_Discovery_for_Shape_Control_CVPR_2021_paper.pdf)，它们对rigid transformation不稳定，而且并不能拓展到真实世界的数据上。


**Method**

*Overview*

给定一个point cloud集合

$$X = \lbrace x_n | x_n \in R^3, n = 1,2,\cdots,N \rbrace$$

这些$$x_n$$是从某个流形$$\mathcal M$$上采样得来的。我们想要获得一个子集$$\tilde X \subset X$$，这个子集就是keypoints的集合，

而

$$\left| \tilde X \right|$$

表示的就是keypoints的个数。

>所以这里还是从point cloud中来选择keypoint，和USIP的想法截然相反。

文章采用的是一个unsupervised encoder-decoder的结构。在encoder里，point cloud里的每个点都会被预测一个keypoint probability $$s$$。为了使得所检测到的keypoints是稀疏的，文章还使用了GAN-based keypoint稀疏性控制。decoder同时也是一个reconstruction network，我们在decoder里使用salient information distillation以一种非监督的方式来重构输入的point cloud。其直觉是，一个好的keypoints集合应该含有一个object的point cloud的独特的信息，从而有能力仅仅基于这些keypoints来重构原输入的point cloud。

整个方法的流程示意图如fig 1所示。

![Model Structure]({{ '/assets/images/UKPGAN-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1 keypoint和embedding生成的pipeline。我们先得到每个点的rotation invariant features，然后利用两个MLP分别输出每个点的keypoint probability和每个点的semantic embedding。GAN被用来控制keypoint稀疏性，salient information distillation被用来获取最显著的features。之后再利用一个decoder来基于学到的keypoints集合重构原始输入的point cloud。*

*Rotation Invariant Feature Extraction*

为了在rigid transformation的情况下仍然robust，我们对于每个点$$x$$的球形领域

$$\mathcal S = \lbrace x_i: \left| \left| x_i - x \right| \right| \leq r \rbrace$$

计算covariance eigendecomposition，从而生成一个Local Reference Frame (LRF)。然后在这个球形邻域内的点$$x_i \in \mathcal S$$对于计算的LRF，变换到它们的canonical position $$x_i^{'}$$。然后我们使用PerfectMatch（[The Perfect Match: 3D Point Cloud Matching with Smoothed Densities](https://openaccess.thecvf.com/content_CVPR_2019/papers/Gojcic_The_Perfect_Match_3D_Point_Cloud_Matching_With_Smoothed_Densities_CVPR_2019_paper.pdf)）里的方法将这些点离散化到一个Smoothed Density Value (SDV) grid，以$$x$$为中心，并且与LRF对齐。voxelization是基于高斯光滑kernel的。从而，我们对于每个点$$x$$就获得了一个voxelized descriptor $$\mathcal F(x) \in R^{W \times H \times D}$$。每个点的3D descriptors再组合起来，喂给3D convolution层。因为LRF，这一步可以让我们获得local rotation-invariant features，其让我们的方法对于rotation具有robustness。

*Dual Branches on Estimating Probabilities and Embeddings*

在上一步获取了每个点的rotation invariant feature之后，我们使用两个MLP来分别预测每个点的keypoint salient probability $$\Phi(x) \in \left[0, 1\right]$$和一个高维的embedding $$h(x) \in R^F$$，其会被用来做reconstruction。

(1) **$$\Phi(x)$$的稀疏性**

为了能将整个point cloud压缩到一个keypoints的小集合，$$\Phi(x)$$需要是稀疏的。而如何使得$$\Phi(x)$$稀疏呢？$$L_1$$ regularization可以实现稀疏性。但是它只能做到输出更多一点的0附近的值，却不能控制那些非0的值的大小（也就是说对于那些我们需要它是1，从而表示keypoints的点，这个值会比1要小很多）。为了输出具有辨识度的keypoints，并且压制住那些没有意义的points，我们希望$$\Phi(x)$$只取0和1附近的值。一个最直观的办法就是直接把这个point cloud上的distribution定义为本身就是在0和1附近聚集的distribution（也就是说这个distribution本身就是在0和1附近值很大，在其他位置值很小），然后迫使这个MLP的输出尽可能地去拟合这个distribution。我们采用Beta distribution来作为这样一个prior distribution。在Beta distribution里有两个参数，$$\alpha$$和$$\beta$$，分别控制1和0的聚集程度。

(2) **GAN-based Keypoint Sparsity Control**

一个最直观的控制稀疏性的方法就是计算上面的MLP输出的keypoint probabilities和beta distribution之间的KL divergence。但是因为我们并没有显式的输出$$\Phi(x)$$的参数，只是输出了每个point的keypoint probability，所以$$\Phi(x)$$和beta distribution之间的KL divergence的解析形式并不存在。我们利用一个adversarial loss来替代KL divergence来解决这个问题。

>但实际上，利用$$\Phi(x)$$的输出结果可以构建一个empirical distribution，也就是先将0到1的坐标轴均匀分割为若干份，然后统计落入每个区间内的点与所有的点的比例，作为这个区间的概率值，从而获得0到1范围内$$\Phi(x)$$的一个近似，区间分割的越小，近似的就越准确。之后我们就可以对这个近似的分布与beta distribution之间计算KL divergence了。

我们采用将我们的模型看作GAN来获取MLP生成的keypoint distribution $$\Phi(x)$$和beta distribution $$p(x)$$之间的一个loss。我们认为$$\Phi(x)$$生成假的keypoint distribution，其需要和我们的prior beta distribution很像。还需要一个discriminator网络$$D$$来判断distribution的真假。distriminator $$D$$的输入是一个point cloud经过MLP输出的所有可能的keypoint distribution构成的集合。在实际操作上，我们使用WGAN-GP而不是最初的GAN，因为这个更加的robust：

$$\mathcal L_{GAN} = min_{\Phi} max_{D} E_{\mathcal M} \left[D(\lbrace p(x) | x \in \mathcal{M} \rbrace)\right] - E_{\mathcal M} \left[D(\lbrace \Phi (x) | x \in \mathcal{M} \rbrace)\right] + \lambda (||\nabla D||-1)^2$$

>这篇文章里说，因为MLP输出的是$$\Phi(x)$$，是point cloud里每个点的keypoint probability，而没有显式的得出这个probability的参数，所以没办法和beta distribution计算KL divergence，从而采用的GAN的方法。但实际上，按照之前批注里的说法也是可以计算的。而且在看了代码之后发现，作者对于这里的实现是，利用一个sigmoid将每个点经过了若干层MLP之后的值压缩到0到1之间，从而代表每个点的keypoint probability，即$$\Phi(x)$$，对于每个点都有这样一个值，从而所有的点构成了一个大小为$$batch \times N$$的矩阵，其中$$N$$是输入point cloud里点的个数，而batch指的是batch size。而之后，再从beta distribution里sample一个一样大的矩阵，再将这个矩阵和之前那个矩阵都送给discriminator来判断。因为进行的是这样的操作，所以实际上其并不能控制keypoints会在哪个位置出现，因为采样的beta distribution不能保证位置信息，但$$\Phi(x)$$是有位置信息的，因为其代表的是每个位置的点的keypoint probability。所以说文章中那样的设置，只会控制$$\Phi(x)$$里0和1的数量，而无法控制其出现的位置。其出现的位置是由其它的loss来控制的。

*Reconstruction network*

给定一个point cloud的keypoint probability distribution

$$\lbrace \Phi(x) \in R | x \sim \mathcal M \rbrace$$

以及高维的embeddings

$$\lbrace h(x) \in R^F | x \sim \mathcal M \rbrace$$

使用一个decoder来重构原输入的point cloud。将这个decoder记为：$$\Psi: R^N \times R^{N \times F} \longrightarrow R^3$$，重构loss如下：

$$\mathcal L_{recon} = CD(\Psi(\lbrace \Phi(x) \in R | x \sim \mathcal M \rbrace, \lbrace h(x) \in R^F | x \sim \mathcal M \rbrace), X)  \tag{1}$$

其中CD是Chamfer distance。

(1) Salient Information Distillation

在公式1里，$$\Psi$$的输入是point cloud的keypoint distribution和高维embedding。我们的目的是找到一个具有显著keypoints的稀疏的keypoint集合，而且能够重构原输入point cloud。为了达到这个目标，我们从PointNet（）的max operation里吸取经验，设计了一个salient information distillation模块。这个模块使得网络能够给出可能性大的点的值大的feature：

$$\Psi = TopNet(max_{x \sim \mathcal M} \left[\Phi(x) h(x)\right])$$

因为$$\Phi(x)$$和$$h(x)$$维度不一样，我们上面的乘法用到了broadcasting。上述的max操作也是针对每个element的，从而$$max_{x \sim \mathcal M} \left[\Phi(x) h(x)\right] \in R^F$$。TopNet是decoder的结构，用了类似[TopNet: Structural Point Cloud Decoder](https://openaccess.thecvf.com/content_CVPR_2019/papers/Tchapmi_TopNet_Structural_Point_Cloud_Decoder_CVPR_2019_paper.pdf)里的设计。

而且对于$$h(x)$$，我们考虑其的绝对值（也就是说$$h(x)$$里的非常大的负数不应该被忽略），从而最终的decoder设计为：

$$\Psi = TopNet(max_{x \sim \mathcal M} \left[\Phi(x) max(h(x), 0) \right], max_{x \sim \mathcal M} \left[\Phi(x) max(-h(x), 0) \right])$$

其中$$max_{x \sim \mathcal M} \left[\Phi(x) max(h(x), 0) \right]$$和$$max_{x \sim \mathcal M} \left[\Phi(x) max(-h(x), 0) \right]$$被连起来，再输入TopNet。

直觉上，我们的decoder使得网络去选择那些显著的并且semantic-rich（也就是$$h(x)$$和$$\Phi(x)$$乘积大的分量）的点的feature。也就是说，不显著的点，或者feature的值并不大的点就都被忽略了。


*Symmetric Regularization*

尽管我们一开始就获取了rotation invariant local descriptors，但是它们并不是symmetric invariant的（也就是说对于对称的点，其的descriptor并不是类似的）。对打大多数的objects，我们都可以认为其检测到的keypoints和features都是对称的，从而：

$$\mathcal L_{sym} = \frac{1}{|S|} \Sigma_{(x,x^{'}) \in S} (||\Phi(x) - \Phi(x^{'})|| + ||h(x) - h(x^{'})||)$$

其中$$S$$是所有的对称的点对构成的集合。这个loss仅仅在训练的时候被使用。

从而最终的loss是：

$$\mathcal L = \eta_1 \mathcal L_{recon} + \eta_2 \mathcal L_{GAN} + \eta_3 \mathcal L_{sym}$$


>这篇文章的一大亮点在于可以通过控制beta distribution里的参数$$\alpha$$和$$\beta$$来控制我们想要的keypoints的个数（假设keypoint probability>0.5就认为其是keypoint）。


### 6. [Single Image 3D Interpreter Network](https://arxiv.org/pdf/1604.08685.pdf)

*Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B. Tenenbaum, Antonio Torralba, Willtian T. Freeman*

*ECCV 2016*

从单张2D的RGB图片中学习3D物体的结构是很困难的。以往的方法都是要么给定2D keypoints的位置，然后用某种optimization方法来推测3D信息，要么就直接在有ground truth的3D信息的生成数据上训练。

而这篇文章提出一个end-to-end的框架，叫做3D Interpreter Network (3D-INN)，其既利用2D keypoints标注信息又利用生成的3D数据作为训练数据，来学习预测2D keypoints的位置以及3D的结构信息。

这篇文章提出了两个技术创新。首先，提出了projection layer，将预测到的3D结构project到2D空间里，从而3D-INN能够通过2D keypoints的监督数据来对3D结构进行学习。其次，2D keypoints的heatmaps作为连接真实和生成数据之间的中间表示，使得3D-INN能够使用充足的生成的3D物体来训练。

虽然说现在deep learning在对物体类别进行识别的任务上做的已经很好了，但是这还不够，我们还需要了解每个类别众多物体之间的差别，比如说对于椅子这一类别，我们需要学习到intrinsic性质，比如说椅子高度、腿长、座椅宽度、材质等，以及其extrinsic性质，比如说椅子在图片中摆放的角度。

这篇文章利用3D skeleton（也就是3D keypoints和keypoints之间的连接）来表示一个物体的3D的结构，而不使用3D mesh或者depth map。正如fig1 (c)所示。

![Model Structure]({{ '/assets/images/UKPGAN-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 1. 3D-INN的流程。*

在这篇文章里，他们还假设每个类别的物体，比如说chair，sofa，human等，都有一个定义好了的skeleton model。


### 7. [D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features](https://arxiv.org/pdf/2003.03164.pdf)

*Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, Chiew-Lan Tai*

*CVPR 2020 Oral*

[code](https://github.com/XuyangBai/D3Feat.pytorch)

一个成功的point cloud registration通常都要依靠具有区分度的3D feature desciptors之间的稀疏的匹配。尽管基于学习的3D feature descriptors发展得很快，但是3D feature detectors（也就是keypoint）发展的很慢，将这两个任务结合起来的研究就更少了。在这篇文章里，作者对于3D point cloud，利用一个3D的CNN，提出一些新颖有效的学习方法，从而能够对于3D point cloud里的每个点都给出一个description feature以及一个detection score。具体来说，文章提出了一个keypoint挑选的策略，克服了point cloud存在的density不同的困难，并且提出了一个由feature matching引导的自监督的detector loss。这篇文章的方法在indoor和outdoor的数据上都进行了测试，包括3DMatch和KITTI数据集。


point cloud registration（点云配准）的目的是在两个部分重合的point cloud fragements之间找到一个最优的transformation，其在SLAM等任务里面是很基础并且重要的。keypoint detection和description对于获取鲁棒的point cloud alignment的结果是最重要的两点。

>point cloud matching，registration和alignment指的是同一个任务。

最近关于3D local feature descriptions的研究开始转向基于学习的方法。因为获取ground truth标注数据的困难，现在绝大多数关于point cloud matching的工作都忽略了keypoint detection learning这个流程，只是随机采样了一个点的集合用来作为feature description。显然这种做法会有一些问题。首先，随机采样的点一般位置都不好，在geometric verification的过程中会导致不准确的transformation的估计。其次，这些随机采样的点很可能位于non-salient的位置（非显著）比如说某些光滑的表面上，这会导致indiscriminative的descriptors，对之后的matching来说效果不好。第三，为了采样的点能够覆盖描述整个场景，往往需要采样很多的点，这导致matching过程的效率变低。实际上，只需要一小部分的keypoints，其就能很好的做好matching任务了，而且定位准确的keypoints还能提高registration的精度。detector和descriptor之间不平衡的发展，让我们想要设计一个模型来联合学习它们。

基于学习的3D keypoint detector并没有得到多少关注。[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)预测了patch-wise的detection score，所以并没有考虑point cloud里所有的点的feature信息。[USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.pdf)采用了一种非监督的方式来对经过任意rigid transformation的一对point cloud points输入预测consistent的3D keypoints，但因为其并没有联合训练detector和descriptor，所以说学习到的detector和用其他方式得到的descriptor可能不匹配，从而并不能对于point cloud给出合适的稀疏的3D feature。在这篇文章里，作者采用了一个联合学习的框架，不仅能稠密的预测keypoints，而且还能够将detector与descriptor整合在一起（共享参数），从而做到快速inference。

为了实现这个目的，我们从[D2-Net: A Trainable CNN for Joint Description and Detection of Local Features](https://arxiv.org/pdf/1905.03561.pdf)里吸取了灵感，这篇文章是关于在2D领域的detector和descriptor的联合学习。然而将D2-Net拓展到3D point clouds上并不容易。首先，需要一个能够预测dense feature的网络结构，而不是之前基于patch的结构。在这篇文章里，我们采用[KPConv: Flexible and deformable convolution for point clouds](https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf)的方法，这篇论文提出了一个可以用在3D point cloud上的卷积运算，可以直接在unstructured的3D point cloud上构建一个CNN网络。其次，我们将D2-Net改造的能对于同一类别的不同object的不同point cloud的variation也能提出高度repeatable的3D keypoints。第三，D2-Net里的原始loss并不能保证在我们的问题里收敛，我们提出利用feature matching来指导一个新的self-supervised detector loss，从而使得detection scores和keypoints的可信赖度统一。

本篇文章的贡献有三点：

* 采用了基于KPConv的CNN框架，构建了一个联合学习3D local feature的detection和description的框架，来进行快速的inference。
* 我们提出了一个新颖的density-invariant keypoint选择策略，这是对于3D point clouds能获取高度repeatable keypoints的关键。
* 提出了一个从feature matching那得到信息的self-supervised detector loss，从而使得联合训练detector和descriptor能够收敛。

这是第一篇为3D point cloud同时学习3D local features的dense detection和description的文章，这个框架叫做D3Feat。


**Related Work**

*3D Local Descriptors*

早期的3D local descriptors都是hand-crafted的，对于noise和occlusion都是不鲁棒的。最近的研究方向转移到了基于学习的方法，这也是本文所关注的。绝大多数的基于学习的local descriptors需要输入为point cloud patches。基于学习的方法目前分为两大类，一类是patch-based networks，一类是fully-convolutional networks。对于第一类，几种3D数据表示被提出用来在3D数据里学习local geometric features：[Learning and matching multi-view descriptors for registration of point clouds]()，[The perfect match: 3d point cloud matching with smoothed densities]，[Ppfnet: Global context aware local features for robust 3d point matching]，[Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors]，[3Dmatch: Learning local geometric descriptors from rgb-d reconstructions]。但是基于point cloud patch的方法效率低。对于第二类，将CNN用在3D local descriptor学习并不是很多见。[Fully convolutional geometric features]，其采用了[3d spatio-temporal convnets: Minkowski convolutional neural networks]里的框架。

*3D Keypoint Detector*

目前绝大多数的3D keypoint detectors都是hand-crafted的。其过于依赖point cloud的局部几何特征，从而对于真实数据里的噪音、遮挡等都不具有鲁棒性。[]提出了一个非监督的方式来检测3D keypoints。但是USIP并不能对每个点都输出一个detection scores，而且如果要求输出的keypoint数很小的时候可能会效果不好。

*Joint Learned Descriptor and Detector*

在2D image matching领域，有几篇文章采用了joint learning detection和description的方法：[Lift: Learned invariant feature transform]，[Superpoint: Self-supervised interest point detection and description]，[Geodesc: Learning local descriptors by integrating geometry constraints]，[Lf-net: Learning local features from images]，[Unsuperpoint: End-to-ned unsupervised interest point detector and descriptor]，[R2d2: Repeatable and reliable detector and descriptor]和[Contextdesc: Learning local descriptors by integrating geometry constraints]。但是将这些方法应用到3D领域并不容易，也没有什么研究。[3dfeat-net: Weakly supervised local 3d features for point cloud registration]是唯一一篇对于3D point cloud联合学习detector和descriptor的文章。然而，他们的方法更加侧重于学习feature descriptor，而只用一个attention layer来估计每个point的salience，也就是detection是作为description网络的副产物存在的，所以他们方法里的keypoint detector的效果是不能保证的。另外，他们的方法使用point patches作为输入，其并没有直接只用point cloud作为输入效率高效果好。相反的，我们使用同一个forward pass来检测keypoint locations和每个点的feature。


**Joint Detection and Description Pipeline**

受到D2-Net的启发，不像之前的方法单独训练两个keypoint detection和description的网络，我们设计了一个网络来完成dense feature descriptor和feature detector两项任务。但是将D2-Net的思路用到3D领域并不简单，因为3D point cloud的不规则性和稀疏性。下面我们先介绍如何在3D point cloud上进行feature description和feature detection，之后再解释我们解决3D数据稀疏性的策略。

*Dense Feature Description*

为了解决再point clouds上如何进行convolution以及如何更好的获取局部几何信息的问题，[KPConv: Flexible and deformable convolution for point clouds](https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf)提出了Kernel Point Convolution(KP-Conv)，使用kernel points来表示convolution weights，从而模仿2D convolution里的kernel pixels，从而就可以在原始的3D point cloud上定义convolution操作了。我们这篇文章采用KPConv作为backbone来进行dense feature extraction。我们先来介绍一下KPConv的公式。

给定一个点集，$$P \in R^{N \times 3}$$，以及一个集合的features，$$F_{in} \in R^{N \times D_{in}}$$，表示成一个矩阵的形式，$$x_i$$和$$f_i$$分别表示$$P$$里的第$$i$$个点以及其在$$F_{in}$$里对应的feature。从而点$$x$$位置的kernel $$g$$的convolution计算如下：

$$(F_{in} \ast g) = \Sigma_{x_i \in N_x} g(x_i-x)f_i \tag{1}$$

其中$$N_x$$是点$$x$$的领域，$$x_i$$是这个领域内别的点。kernel function $$g$$定义为：

$$g(x_i - x) = \Sigma_{k=1}^K h(x_i-x, \hat x_k)W_k \tag{2}$$

其中$$h$$是kernel point $$\hat x_k$$和点$$x_i$$之间的correlation function，$$W_k$$是kernel point $$\hat x_k$$的weight matrix，而$$K$$是kernel points的个数。

原论文里的方法对于point cloud里的point density并不是不变的。所以这篇文章加上了一个density normalization term：

$$(F_{in} \ast g) = \frac{1}{\lVert N_x \rVert} \Sigma_{x_i \in N_x} g(x_i-x)f_i \tag{1} \tag{3}$$

基于上述的normalized kernel point convolution，文章使用了UNet的结构（[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf)）来构建一个网络，如fig1所示。

![Model Structure]({{ '/assets/images/D3FEAT-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 1. (Left)D3Feat的网络结构。每一个block都是一个ResNet block，其使用KPConv来替代原本的image convolution。除了最后一层，每层都接了一个batch normalization和ReLU。(right)Keypoint Detection。在dense feature extraction之后，我们通过计算saliency score和channel max score来进行keypoint detection score的计算。*

不同于只能获取稀疏feature description的patch-based方法，我们的网络可以获取dense feature description。我们网络的输出是一个dense feature map，$$F \in R^{N \times c}$$，其中$$c$$是feature向量的维数，$$N$$是point cloud里点的数量。点$$x_i$$相关的descriptor记为$$d_i$$：$$d_i = F_{i:}, d_i \in R^{c}$$。每个点的descriptor都被$$L_2$$标准化到单位长度。

*Dense Keypoint Detection*

D2-Net里通过找到feature map的局部的spatial和channel的最大值来检测到2D keypoints，并且使用一个softmax来衡量这个局部最大值点的值。因为CNN的结构以及images本身是一个2维矩阵，而CNN的feature maps也都是高维tensor，所以每个pixel的neighborhood就直接是它相邻的pixels就可以。

为了将D2-Net里的方法拓展到3D上，可以用radius neighborhood来替代来解决point clouds里的点并不是均匀分布的问题。但是每个点的radius neighborhood里的点的数量差别很大。在这种情况下，如果我们直接使用一个softmax函数来在spatial维度衡量局部最大值，那些有着很少的点的布局区域就会有很高的值（比如说对于indoor场景来说的边缘位置，或者对于outdoor场景来说远离Lidar中心的位置）。为了解决这个问题，我们提出一个density-invariant saliency值来衡量一个点和其邻居点相比的saliency。

我们有dense feature map，$$F \in R^{N \times c}$$，我们将$$F$$看作一系列3D responses $$D^k, k=1, \cdots, c$$的集合：

$$D^k = F_{:K}, D^k \in R^N$$

$$x_i$$是一个keypoint的标准就是：

$$x_i$$ is a keypoint $$\iff k=argmax_{t} D_i^t, i = argmax_{j \in N_{x_i}} D_j^k$$

其中$$N_{x_i}$$是$$x_i$$的radius neighborhood。这表明一个点$$x_i$$如果想成为keypoint，首先找到具有最大值的那个通道，然后对于这个通道，$$x_i$$是其邻域内值最大的那个。在实际训练的过程中，我们将上述过程条件放松，从而使得能够训练，引入了两个scores，如fig1右边所示。下面详细介绍这两个scores。

* Density-invariant saliency score

这个值用来衡量每个点相对于其邻域内的其它点有多么的salient。在D2-Net里，衡量局部最大值的score是：

$$\alpha_i^k = \frac{exp(D_i^k)}{\Sigma_{x_j \in N_{x_i}} exp(D_j^k)}$$

这个公式对于稀疏性来说并不是不变的。稀疏的区域比稠密的区域自然的就会有更高的score，因为这个值是被总和归一化过的。从而我们设计一个density-invariant saliency score：

$$\alpha_i^k = ln(1 + exp(D_i^k - \frac{1}{\lVert N_{x_i} \rVert} \Sigma_{x_j \in N_{x_i}} D_j^k))$$

在上述公式里，每个点的saliency是通过这个点和其neighborhood点的feature的均值的差来表示的。而且使用均值而不是总和可以避免值受到neighborhood里的point的数目的影响。

* Channel max score

这个score是用来对于每个point挑选最重要的通道用的：

$$\beta_i^k = \frac{D_i^k}{max_t (D_i^t)}$$

最后，这两个值被综合考虑为最终的keypoint detection score:

$$s_i = max_k (\alpha_i^k \beta_i^k)$$

在我们获取了整个输入的point cloud的keypoint score map之后，我们就可以选取那些有最高值的那些point为keypoints了。



**Joint Optimizating Detection & Description**

设计一个有效的supervision signal是联合学习一个descriptor和一个detector的关键。在这一节里，我们先介绍descriptor的metric learning loss，然后再设计一个自监督的detector loss。

* Descriptor loss

为了优化descriptor网络，很多工作都使用metric learning方法，比如contrastive loss或者triplet loss。我们将会使用contrastive loss因为实验证明效果更好。至于如何找到有效的采样方法来选择训练数据对，我们采用[Working hard to know your neighbor's margins: Local descriptor learning loss](Working hard to know your neighbor's margins: Local descriptor learning loss)里说的hardest in batch策略来使得网络集中注意力于hard pairs。

给定一对部分重合的point cloud fragments，$$P$$和$$Q$$，以及$$n$$个对应的3D keypoints对组成的集合。假设$$(A_i, B_i)$$是两个point cloud中对应的点对，并且他们有相应的descriptors，$$d_{A_i}$$和$$d_{B_i}$$，以及scores $$s_{A_i}$$和$$s_{B_i}$$。一个positive pair之间的距离被定义为它们descriptors之间的欧氏距离：

$$d_{pos}(i) = \lVert d_{A_i} - d_{B_i} \rVert_2$$

一个negative pair的距离为：

$$d_{neg}(i) = min \lbrace \lVert d_{A_i} - d_{B_j} \rVert_2 \rbrace s.t. \lVert B_j - B_i \rVert_2 > R$$

其中$$R$$是safe radius，$$B_j$$是真实的对应关系的safe radius外hardest negative点。contrastive margin loss被定义为：

$$L_{desc} = \frac{1}{n} \Sigma_i \left[ max(0, d_{pos}(i) - M_{pos}) + max(0, M_{neg} - d_{neg}(i))\right]$$

其中$$M_{pos}$$和$$M_{neg}$$分别是positive和negative pairs对应的margin的值。


* Detector loss

为了优化detection的效果，我们找到一个loss，其对于容易匹配的点对具有更高的keypoint detection score，对于难匹配的点对则具有较低的keypoint detection score。在D2-Net里，作者提出了一个triplet margin loss的拓展来用于同时优化detector和descriptor：

$$L_{det} = \Sigma_{i} \frac{s_{A_i}s_{B_i}}{\Sigma_{i} s_{A_i}s_{B_i}}max(0, M + d_pos(i)^2 - d_neg{i}^2)$$

其中$$M$$是triplet margin。D2-Net认为为了最小化loss，网络对于最容易匹配的那些点（也就是$$d_{pos}$$很小，而$$d_{neg}$$很大的那些点）应该具有较高的keypoint detection score。但是本文发现他们的方法对于3D keypoint来说不好使。

因此本文也涉及了一个loss项来显式的引导scores的gradient。从一个自监督的角度来看，我们使用feature matching的结果来衡量每对对应点的辨识度，其将会引导每个keypoint的score的gradient flow。

$$L_{det} = \frac{1}{n} \Sigma_i \left[(d_{pos}(i) - d_{neg}(i))(s_{A_i} + s_{B_i})\right]$$

直觉上，如果$$d_{pos}(i) < d_{neg}(i)$$，这说明使用最近邻搜索就能找到正确的匹配点，上述loss鼓励这样的点的$$s_{A_i}$$和$$s_{B_i}$$尽量大一些，也就是被选为keypoints的可能性大一些。相反的，如果$$d_{pos}(i) > d_{neg}(i)$$，那么对应的点对对于当前的网络来说就不够那么具有辨识度来使得正确的对应关系被建立，所以上述的loss会使得这样的点成为keypoints的可能性变小。为了最小化上述loss，网络需要对那些匹配的点给出高keypoint detection scores，对于那些不匹配的给出低scores。



### 8. [Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Weakly-Supervised_Discovery_of_Geometry-Aware_Representation_for_3D_Human_Pose_Estimation_CVPR_2019_paper.pdf)
*Xipeng Chen, Kwan-Yee Lin, Wentao Liu, Chen Qian, Liang Lin*

*CVPR 2019*



This work proposed a method to learn 3D coordinates of human body joints in order to do human pose estimation. This model is based on skeleton extracted from the raw RGB images, not an End-to-end framework.

Hyperparameter: Number of Keypoints $$K$$.

**Step1** Inputs are source image $$I_s$$ and target image $$I_t$$, and the rotation matrix are known due to the parameters of cameras. First, they use existing skeleton algorithm to extract skeleton maps of $$I_s$$ and $$I_t$$.

**Step2** Instead of a traditional encoder-decoder framework, they use a novel view synthesis method, i.e., source image $$I_s$$ are encoded and combined with rotation matrix $$R_{s \rightarrow t}$$, target image $$I_t$$ are reconstructed from the decoder. The 3D keypoint coordinates are the output of the encoder, as a geometry-aware representation, as explained in the paper. They also design the bidirectional encoder-decoder framework, which hinges on two encoder-decoder networks with same architecture to perform view synthesis in the two directions simultaneously, i.e., from $$I_s$$ to $$I_t$$ and from $$I_t$$ to $$I_s$$. These two reconstructions will involve two losses.

**Step3** They believe that the 3D keypoints of these two images should be the same. There are two encoders and the outputs should be the same, thus involve a new loss.

![Model Structure1]({{ '/assets/images/weak-supervised.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1 The framework of learning a geometry representation for 3D human pose in a weakly-supervised manner. There are three main components. (a)Image-skeleton mapping module is used to obtain 2D skeleton maps from raw images. (b)View synthesis module is in a position to learn the geometry representation in latent space by generating skeleton map under viewpoint $$j$$ from skeleton map under viewpoint $$i$$. (c) Since there is no explicit constrain to facilitate the representation to be semantic, a representation consistency constrain mechanism is proposed to further refine the representation.*



### 9. [Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://keypointnet.github.io/keypointnet_neurips.pdf)
*Supasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, Mohammad Norouzi*

*NeurIPS 2018 Oral*

[Review](https://papers.nips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Reviews.html)

**Abstract**

这篇文章提出了KeypointNet，一个end-to-end的geometric reasoning框架，来学习一批最优的category-specific 3D keypoints。给定一张图片，KeypointNet为下游任务提取最优的3D keypoints。我们在3D pose estimation任务上应用我们的框架，我们提出一个能找到一个object的两个view之间的相对姿态的differentiable的目标函数。我们的模型能在一类有着不同视角和不同instances的object图片种学习到geometrically和semantically都consistent的keypoints。我们发现，我们所提出的这个框架并没有用到任何标注（监督信息），却在pose estimation任务上比使用同样框架的监督方法效果要好。


**1. Introduction**

CNN已经说明同时优化feature extraction和classification可以很大程度的改进object recognition任务的效果。而现有的解决geometric vision problems（比如说3D reconstruction，shape alignment等）的方法，包含了两个stage，也就是一个单独的keypoint detection模块，和后来的geometric reasoning操作。在这篇文章里，我们探索是否可以对于geometric vision problem提出一种end-to-end的框架，在其中keypoints作为下游任务的latent variables被优化出来。

考虑确定一张图片里一辆汽车的3D pose这样一个问题。一个标准的做法是先检测一个稀疏的category-specific keypoints的集合，之后再利用这些keypoints使用某种geometric reasoning框架（比如说，PnP algorithm）来重建3D pose或者相机角度。我们可以利用很强的监督信息，对于每个object category，利用很多张标有keypoints位置的该类object的图片，来学习一个针对该类object的keypoint detector。或者使用model-based fitting方法来学习这样的keypoint detector。研究者已经建立了标有keypoints的face，hands，human body的datasets。但是，对于某个object category的图片里的keypoints进行选取和标注的过程是十分复杂而且ill-defined的。为了设计合理的keypoint set，需要将这个keypoint set要用在什么下游任务这个信息考虑进来。直接利用下游任务来优化keypoints的选择会自然的鼓励网络学到对于这个下游任务最有用的那些keypoints。

这篇文章提出了KeypointNet，一个end-to-end的geometric reasoning框架，来对于一个特别的下游任务，学习一个最优的category-specific 3D keypoints。我们的方法相对于先前工作的创新点在于我们利用一个任意的下游任务来将keypoints作为latent variable学习出来。我们的框架对于任何目标函数对于keypoints positions是可微分的下游任务都是可以使用的。我们这篇文章使用的是3D pose estimation作为下游任务，我们的最关键的技术贡献在于：1）一个新的对于keypoint positions可微分的pose estimation目标函数；2）一个multi-view consistent的loss function。这个pose estimation任务的目标函数旨在为恢复同一个object的两个views之间的relative pose任务找到最优的keypoints集合。multi-view consistency loss估计在一个object的3D transformations之间找到consistent的keypoints。值得注意的是，我们是从一些2D图片里检测3D keypoints（2D points加上深度），而且我们的pose estimation和multi-view consistency都是针对3D keypoints detection设计的。

我们的KeypointNet对于一个给定的object类别可以在含有不同角度和不同instances的这个类别的object的一系列图片种学习到geometrically和semantically都consistent的keypoints。某些学习到的keypoints对应到很有意思而且semantically有意义的地方，比如说car的wheels，并且这些keypoints还可以不通过object geometry直接获取3D的信息。我们在ShapeNet dataset上对于不同的object类别做了三个实验。对于3D pose estimation，我们还和有标注的监督方法进行了对比。令人惊讶的是，我们的end-to-end方法在没有keypoints标注的情况下取得了更好的结果。


**2. Related Work**

2D和3D的keypoint detection任务都是CV领域的被研究了很久的问题，而keypoint inference在object localization任务上一直被用作一个早期的stage。比如说，使用CNN在monocular RGB图片上检测2D的human joint position的任务。因为在HCI，motion capture和security应用领域都有广泛的需求，有很多工作都在研究keypoint detection和object localization结合的任务：[A coarse-fine network for keypoint localization](https://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_A_Coarse-Fine_Network_ICCV_2017_paper.pdf)，[Stacked Hourglass Networks for Human Pose Estimation](https://arxiv.org/pdf/1603.06937.pdf)，[Towards Accurate Multi-person Pose Estimation in the Wild](https://openaccess.thecvf.com/content_cvpr_2017/papers/Papandreou_Towards_Accurate_Multi-Person_CVPR_2017_paper.pdf)，[DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pishchulin_DeepCut_Joint_Subset_CVPR_2016_paper.pdf)，[Learning Feature Pyramids for Human Pose Estimation](https://openaccess.thecvf.com/content_ICCV_2017/papers/Yang_Learning_Feature_Pyramids_ICCV_2017_paper.pdf)

和我们这篇论文更加有关系的工作，就是有一批利用CNN从monocular RGB图片中检测3D human keypoint的论文，它们使用了各种各样的CNN结构、各种监督目标函数、以及各种3D structural prior knowledge来学习一批标记好位置的3D joint locations，[Adversarial Learning of Structure-Aware Fully Convolutional Networks for Landmark Localization](https://arxiv.org/pdf/1711.00253.pdf)，[DensePose: Dense Human Pose Estimation In The Wild
](https://openaccess.thecvf.com/content_cvpr_2018/papers/Guler_DensePose_Dense_Human_CVPR_2018_paper.pdf)，[Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision](https://arxiv.org/pdf/1611.09813.pdf?source=post_page---------------------------)，[Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB](https://arxiv.org/pdf/1712.03453.pdf)，[VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera](https://vcai.mpi-inf.mpg.de/projects/VNect/content/VNect_SIGGRAPH2017.pdf)。还有一些工作利用所学习到的2D keypoint detector结合3D priors来做2D-to-3D-lifting，[3D Human Pose Estimation = 2D Pose Estimation + Matching](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_3D_Human_Pose_CVPR_2017_paper.pdf)，[A simple yet effective baseline for 3d human pose estimation](https://openaccess.thecvf.com/content_ICCV_2017/papers/Martinez_A_Simple_yet_ICCV_2017_paper.pdf)，[Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Sparseness_Meets_Deepness_CVPR_2016_paper.pdf)。或者从depth图片里找到data-to-model的对应关系，[Metric Regression Forests for Correspondence Estimation](http://www.cs.toronto.edu/~jtaylor/papers/IJCV2015-MetricRegressionForests.pdf)。[Improving landmark localization with semi-supervised learning](https://openaccess.thecvf.com/content_cvpr_2018/papers/Honari_Improving_Landmark_Localization_CVPR_2018_paper.pdf)通过引入semi-supervised任务比如说attribute prediction或者equivalent landmark prediction来改进landmark prediction。相反的是，我们的keypoints set并不是被定义为了prior，而是作为下游的geometric estimation任务的latent variable被优化出来。[Single Image 3D Interpreter Network](http://people.csail.mit.edu/tfxue/papers/eccv2016_3DINN.pdf)也是相关的文章。

在CNN feature representation中设计latent structure在很多领域都被研究过了。比如说，capsule网络以及它的变种将隐藏层的输出的activation结果的大小和方向结合起来用作构建更高层的features。我们的KeypointNet的输出可以看作一个latent 3D feature，其因为multi-view consistency loss和后续下游任务的relative pose目标函数的原因而倾向于表示的是图片里object的3D keypoint locations。

最近的很多工作研究了同一个类别的object的不同的形状和外表的图片的2D keypoints之间的对应关系。。比如说，[Universal correspondence network](https://proceedings.neurips.cc/paper/2016/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf)使用了一个基于appearance的contrastive loss来encode geometry和semantic的相似性。[SCNet: Learning semantic correspondence](https://openaccess.thecvf.com/content_ICCV_2017/papers/Han_SCNet_Learning_Semantic_ICCV_2017_paper.pdf)提出了一个新的SCNet网络用来学习2D keypoints之间的semantic correspondence。[Multi-image semantic matching by mining consistent features](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.pdf)通过解决一个feature selection和labeling的问题利用deep features在一系列图片上做一个multi-image matching的任务。[Unsupervised learning of object frames by dense equivariant image labelling](https://arxiv.org/pdf/1706.02932.pdf)使用ground truth transforms（图片对之间的optical flow）和point-wise matching来对于不同角度以及不同appearance的图片之间学习一个dense的以object为中心的coordinate frame。[Learning to see by moving](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Agrawal_Learning_to_See_ICCV_2015_paper.pdf)使用图片之间的egomotion prediction以一种自监督的方式来学习feature representations，并且以这种方式所学习到的representations和以监督学习方式得到的representations对很多下游任务来说效果是差不多的。

还有一些工通过不同的监督信号来学习latent 2D或者3D features。[Constructing implicit 3D shape models for pose estimation](http://www.weizmann.ac.il/math/ronen/sites/math.ronen/files/uploads/arie-nachimson_basri_-_constructing_implicit_3d_shape_models_for_pose_estimation.pdf)为rigid objects建立了3D models之后利用这个model来从一张2D图片和一些3D的latent features里估计3D pose。受到cycle-consistency对于学习correspondence的作用的启发，[Learning dense correspondence via 3d-guided cycle consistency](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Learning_Dense_Correspondence_CVPR_2016_paper.pdf)训练了一个CNN来对同一个类别的object的不同instance的图片之间预测correspondence，数据是由CAD模型提供的。独立于我们的文章，[Unsupervsied discovery of object landmarks as structural representations](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)利用一个reconstruction的目标函数来为已知类别的object的图片检测稀疏的2D landmarks，这些2D landmarks显式的作为structure representations被使用。相似的，[Conditional image generation for learning the structure of visual objects]()利用conditional图像生成和一个reconstruction目标函数来学习2D keypoints，这些2D keypoints含有训练图片对之间的geometric变化的信息。[Unsupervised geometry-aware representation for 3d human pose estimation](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper.pdf)使用一个multi-view consistency loss，和我们的相似，来为human pose estimation任务学习3D latent variables。不同于上述提到的最后的四篇文章，我们的latent keypoints是为一个下游任务来优化的，这种方法会产生对于下游任务更有用的keypoints。通过在真实的物理3D结构中表示我们所学到的keypoints，我们发现我们的方法甚至可以发现图片中被遮住部分的keypoints之间的correspondence，而且这些图片里的物体还具有很大的姿态差异，比如说尽心了很大程度的旋转。

寻找3D keypoints之间的correspondence的工作也在持续的开展。[Learning a descriptor-specific 3d keypoint detector](https://openaccess.thecvf.com/content_iccv_2015/papers/Salti_Learning_a_Descriptor-Specific_ICCV_2015_paper.pdf)将3D keypoint detection任务看作一个points之间的binary classification任务，而这些points的ground truth similarity label是由一个预先定义好的3D descriptor提供的。[Unsupervised domain adaptation for 3d keypoint prediction from a single depth scan](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Xingyi_Zhou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.pdf)使用view-consistency作为监督信号来预测3D keypoints，但仅仅是在depth images上使用。相似的，[Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3D model views](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Render_for_CNN_ICCV_2015_paper.pdf)使用synthetically rendered模型来预测object viewpoint，而这是通过使用CNN viewpoint embedding来将其与真实世界里的图片匹配从而实现的。除了keypoints，基于geometric和motion reasoning的自监督方法也被用来预测其它形式的输出，比如说[Self-supervised learning of motion capture](https://proceedings.neurips.cc/paper/2017/file/ab452534c5ce28c4fbb0e102d4a4fb2e-Paper.pdf)里提到的3D shape。


**3. End-to-end Optimization of 3D Keypoints**

给定一张含有一个已经知道类别的object的图片，我们的模型可以预测一个3D keypoint的有序的list，定义为pixel coordinates和相对应的depth values。这样的keypoints要求对于这个类别的object的不同instances以及不同角度的图片具有geometrically和semantically consistency，在fig 1能看到。

![RESULT]({{ '/assets/images/DOWNSTREAM-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. KeypointNet在ShapeNet测试集上对于cars，planes和chairs的结果。我们的网络能够泛化到没见过的appearance和shape变化上去，并且还能consistently的预测被遮住的部分比如说wheels和chair legs。*

我们的KeypointNet有N个head从而可以提取N个keypoints，而且它们是有顺序的，同一个head用来提取同一个semantic的keypoint。这些keypoints可以作为基于稀疏点的feature representations的构建模块，对于geometric reasoning和pose-aware，pose-invariant object recognition任务会有用。

和利用监督方法学习一个从图片到list的keypoint positions的映射的方法不同，我们并不将keypoint positions用作一个priori。取而代之的是，我们利用一个下游的任务来联合选取keypoints。我们在训练的时候使用的是relative pose estimation任务，这个任务是给定同一个object的两个views，而且直到这两个views之间的rigid transformation $$T$$，我们要在两个views里预测最优的两个3D keypoints lists，$$P_1$$和$$P_2$$，从而使得一个view能最佳的match另一个view。过程如fig 2所示。

![PIPELINE]({{ '/assets/images/DOWNSTREAM-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. 在训练过程中，同一个object的两个views都会给KeypointNet作为输入。两个views之间的rigid transformation $$(R,t)$$也作为监督信号被提供。我们学习一个3D keypoints的ordered list，在两个views里是consistent的而且能够让我们复原transformation。在inference的时候，KeypointNet从一张单独的输入图片提取3D keypoint positions。*

我们设计了一个目标函数$$O(P_1，P_2)$$，基于此我们可以优化一个从一张图片到一个3D keypoint的ordered list的parametric mapping（也就是我们的KeypointNet）。我们的目标函数有两个主要的组成部分：

* 一个multi-view consistency loss，在知道transformation的情况下用来衡量两个3D keypoint ordered lists之间的差异；
* 一个relative pose estimation loss，使得用orthogonal procrustes从$$P_1$$和$$P_2$$还原出的rotation $$R^{'}$$和真实的rotation $$R$$相同。

我们认为这两个terms使得模型可以检测重要的keypoints，其中某些是人类会为不同object类别选取的semantically meaningful的locations。注意到我们并没有直接去寻找那些semantically meaningful的keypoins，因为它们可能对于下游的任务来说并不是最好的，而且会很难去挑选。下面我们将会先解释我们的目标函数，之后介绍KeypointNet的结构。

**notation**

每个training tuple有一对图片$$(I,I^{'})$$，其是同一个object的两个不同角度的图片，并且有它们的relative rigid transformation $$T \in SE(3)$$，将潜在的3D shapes从$$I$$变换到了$$I^{'}$$。$$T$$有着以下的matrix form：

![MATRIX]({{ '/assets/images/DOWNSTREAM-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}

其中$$R$$和$$t$$分别代表一个3D rotation和一个3D translation。我们将通过优化一个目标函数$$O(f_{\theta}(I), f_{\theta}(I^{'}))$$，来学习一个函数$$f_{\theta}(I)$$，参数是$$\theta$$，将一个2D的图片$$I$$映射到一个3D keypoint locations的list，$$P=(p_1,p_2,...,p_N)$$，其中$$p_i=(u_i,v_i,z_i)$$。


**3.1 Multi-view consistency**

我们的multi-view consistency loss的目的就是确保keypoints能在不同的views里跟踪consistent的部分。两个views里的3D keypoints应该分别在3D空间里对应到object的同一个3D像素点。我们假设已知相机的global focal length $$f$$。下面我们使用$$\left[x,y,z\right]$$代表3D坐标，使用$$\left[u,v\right]$$代表pixel坐标。照片$$I$$的keypoint $$\left[u,v,z\right]$$映射到照片$$I^{'}$$的计算由下面的projection operators给出（从$$I^{'}$$映射到$$I$$也给出）：

$$\left[\hat{u}, \hat{v}, \hat{z}, 1\right]^T  \sim \pi T \pi^{-1}(\left[u,v,z,1\right]^T)$$

$$\left[\hat u^{'}, \hat v^{'}, \hat z^{'}, 1 \right]^T  \sim \pi T^{-1} \pi^{-1}(\left[u^{'},v^{'},z^{'},1 \right]^T)$$

其中，$$\hat{u}$$表示$$u$$映射到第二张图片里的坐标，$$\hat u^{'}$$表示$$u^{'}$$映射到第一张图片里的坐标。$$\pi:R^4 \rightarrow R^4$$表示将一个homogeneous的在camera coordinates下的3D coordinate $$\left[x,y,z,1 \right]^T$$映射到一个pixel position再加上depth的perspective projection操作：

$$\pi(\left[x,y,z,1 \right]^T) = \left[\frac{fx}{z}, \frac{fy}{z}, z, 1 \right] = \left[u,v,z,1 \right]^T$$

我们定义一个symmetric multi-view consistency loss如下：

$$L_{con} = \frac{1}{2N}\Sigma_{i=1}^N ||\left[u_i, v_i, u_i^{'}, v_i^{'} \right]^T - \left[\hat u_{i}^{'}, \hat v_{i}^{'}, \hat u_{i}, \hat v_{i} \right]^T||^2$$

我们仅仅在可见到的图片空间（也就是2D图片）上衡量error，所以上述$$L_{con}$$不包含深度信息$$z$$，因为深度信息从来就不能直接被观察到，而且和$$u$$，$$v$$具有不同的取值范围。但是我们要注意，在我们的计算中，只有有了$$z$$我们才能够计算两个views之间的projection $$\pi$$。

multi-view consistency对于在不同的views间获取一个consistent的2D keypoint positions集合已经是足够了。但是只有这个loss会导致一个崩坏的结果：所有的keypoints都会集中到同一个位置，这是没有作用的。我们可以定义某种diversity来分开这些keypoints，但是对于multi-view consistency，仍然存在无数种解满足这个条件。所以说我们采取了利用某个下游的任务来选取keypoints的方式。我们使用pose estimation作为我们的任务。

**3.2 Relative pose estimation**

keypoint detection的一个重要的应用就是还原一对图片里的object之间的relative transformation。从而，我们定义一个可微分的目标函数用来衡量预测的relative rotation $$\hat{R}$$（用两个keypoints集合利用Procrustes'alignment计算得来）和ground truth rotation $$R$$之间的差异。因为我们的KeypointNet具有translation equivariance性质，并且我们已经有了上述的multi-view consistency loss，我们在这个loss里就不衡量translation loss了。从而这个pose estimation目标函数定义为：

$$ L_{pose} = 2arcsin(\frac{1}{2\sqrt{2}}||\hat{R}-R||)$$

这衡量了从两个keypoints集合里计算出的least-squares estimate $$\hat{R}$$和ground truth relative rotation matrix $$R$$之间的角度差异。这个目标函数可以用可微分的运算来实现。

为了估计$$\hat{R}$$，$$X$$和$$X^{'} \in R^{3 \times N}$$表示两个views的camera coordinates的3D keypoint locations.也就是说，假设$$X = \left[X_1,X_2,...,X_N \right]$$以及$$X_i = (\pi^{-1}pi)\left[:3 \right]$$。$$X^{'}$$也有类似的定义。$$\tilde{X}$$和$$\tilde{X}^{'}$$表示$$X$$和$$X^{'}$$分别将自身的mean减掉之后的矩阵。从而optimal least-squares rotation $$\hat{R}$$就用以下的方式计算而得：

$$\hat{R} = Vdiag(1,1,...,det(VU^T))U^T$$

其中$$U, \Sigma, V^T = SVD(\tilde X \tilde X^{'T})$$。这个计算rotation $$\hat{R}$$的方式叫做orthogonal Procrustes problem。为了确保$$\tilde{X}\tilde{X}^{'T}$$是invertible的，并且为了增强keypoints的鲁棒性，我们在$$X$$和$$X^{'}$$内加入Gaussian噪音。

从实验结果来看，pose estimation目标函数很大程度的帮助网络产生合理的keypoints，导致网络可以自动的找到一些有意思的地方比如说wheels，飞机的wings，chair的legs等。我们相信这是因为这些部分在一个object类型里是geometrically consistent的（比如说所有的car都有圆形的wheels）。这些keypoints还在空间中分散开。这些对于下游任务都是有好处的。


**4. KeypointNet Architecture**

从图片到keypoints的映射的一个重要的性质就是在pixel层面上的translation equivariance。也就是，如果我们将整个input图片，比如说，向左移动一个像素，这个网络输出的所有的keypoint locations也都会向左移动一个像素。训练一个没有这个性质的标准的CNN需要有一个很大的training set，其中包含在各个地方的objects，而且在inference的时候还不能确保equivariance（因为inference时候的object可能会出现在training图片从没出现过的位置上）。

我们使用下面很简单的方式来获得这个equivariance。我们并不是让网络直接输出keypoints的locations，而是让网络输出一个probability distribution map，$$g_i(u,v)$$，表示的是keypoint i在pixel位置$$(u,v)$$地方可能出现的概率，而$$\Sigma_{u,v}g_i(u,v)=1$$对于所有的$$i$$都成立。我们使用一个spatial softmax来对整个image pixels生成这样的distribution。我们之后再计算这个spatial distribution的期望来获得一个pixel coordinate：

$$\left[u_i, v_i\right]^T = \Sigma_{u,v} \left[ug_i(u,v), vg_i(u,v)\right]^T$$

对于$$z$$ coordinates来说，我们同样再每个pixel的位置预测一个深度值，叫做$$d_i(u,v)$$，然后：

$$z_i = \Sigma_{u,v} d_i(u,v) g_i(u,v)$$

为了我们输出的feature map和输入的图片尺寸相同（这样我们才能用上述的计算方式来表示原始输入图片里的keypoint的位置），我们使用的是stride=1的fully convolutional architecture（从[这篇文章](https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)用来的），同样也用于semantic segmentation。为了增加网络的感受野，我们将几层dilated convolutions堆叠起来，和[这篇文章](https://d1wqtxts1xzle7.cloudfront.net/56029055/wavenet-with-cover-page-v2.pdf?Expires=1653489733&Signature=NZ8YPKKd4QYPyTFT8LDppJaaJyMLmGidAbnGDeuBHMw7LnvM2cWmiw0YEjxvua5Fjb3hEKU36z0WjnfGZBF3tGJwG1juJvWNULa0y7d2p96v3ab4vrgWhAiBYU3xVT-uiXvoNwVTWmiXaPOARFGc2ulOjfKBS4wpMvgYSHdhZT9IJ4dLmRIQFZpcmOC9~65DuTsKslExG9l37v9-Chzd18dKCYk8yPmza3ke~tNxXcvI2X4COKBSeYxYTdfViS1nivA8GflrCmuPZidn3jAPtdv4O3S1U9EKbtzicuQYvoAQ6mGgCIBwE0sqVtbvI7yKAJi-LBdijyjVgfTtkPaiXg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)类似。

我们上述这种对于equivariant neural network的设计不仅帮助极大的减小了所需要达到好的泛化效果的训练图片的数量，也减少了camera coordinates到image coordinates的计算量，从而网络可以更专心地做infering depth的任务。

**Architecture details**

所有层的kernel都是$$3 \times 3$$大小的，我们将13个dilated convolutions层堆叠在一起，dilation rates分别是1，1，2，4，8，16，1，2，4，8，16，1，1，每层的输出通道都是64，除了最后一层的输出通道是$$2N$$，一半是$$g_i$$，一半是$$d_i$$。我们对于每层都使用leakyRelu和batch normalization，除了最后一层。最后一层，也就是输出层，是不经过activation function的，前$$N$$个通道作为$$N$$个keypoint的$$d_i$$直接输出，而后$$N$$个通道作为keypoint的$$g_i$$还需要经过一个spatial softmax的计算。最后利用之前的公式计算出每个keypoints的u,v,z。

**Breaking symmetry**

很多object类别在至少一个对称轴上是对称的，比如说一辆轿车的左边和右边很类似。这对于网络来说是个难点，因为不同的part会显示出同样的视觉特征，这只能通过了解全局信息来解决。比如说，分清楚左轮和右轮需要了解更多的相对位置信息。

为了帮助打破这种对称性，我们可以让keypoint prediction依赖于某种对于pose粗糙的量化。这样一种coarse-to-fine的keypoint detection的方法在[这篇文章](https://openaccess.thecvf.com/content_cvpr_2015/papers/Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper.pdf)里使用过。


**5. Additional Keypoint Characteristics**

除了上面已经说过了的主要的目标函数，对于很多下游的任务来说，还有很多常见的keypoints的性质可以起到帮助的作用，比如说：1）没有两个keypoints能对应到同一个3D location；2）keypoints需要在一个object的轮廓内。

Separation loss对于两个靠的太近的keypoints进行惩罚：

$$L_{sep} = \frac{1}{N^2}\Sigma_{i=1}^{N}\Sigma_{j \neq i}^N max(0, \delta^2-||X_i- X_j||^2)$$

和consistency loss不一样，这个loss是3D loss，也就是说即使两个pixel占据了同一个像素点，但只要它们的深度z不同，那么它们就可以这样分布。

理想情况下的keypoints不应该有这种separation loss。但是因为我们的keypoint并没有它们具体positions的监督数据，如果我们不加上这个loss，会有keypoints到同一个点的问题。

Silhouette consistency鼓励keypoints落在object的轮廓内。正如我们上面所说，我们网络对于第$$i$$个keypoint的position的预测是通过计算$$g_i$$的期望得来的。一个确保silhouette consistency的方法就是只在silhouette内让概率非零，设计一个单峰的low variance的spatial distribution。

在训练过程中，我们对于object还有其mask的信息，也就是$$b(u,v) \in \{0,1\}$$，其中1表示前景object，0表示背景。从而silhouette consistency loss可以被定义为：

$$L_{obj} = \frac{1}{N}\Sigma_{i=1}^N -log\Sigma_{u,v}b(u,v)g_i(u,v)$$

同时还有个loss用来使得variance比较小：

$$L_{var} = \frac{1}{N}\Sigma_{i=1}^N \Sigma_{u,v} g_i(u,v)||\left[u,v\right] - \left[u_i, v_i\right]||^2$$

这一项使得distribution峰比较尖，从而帮助控制keypoint落在object的边界内。


**6. Experiments**

我们的training data是从ShapeNet里生成的，其是一个很大的dataset，包含大约51000个3D模型，有超过270个种类的object。我们为多个object种类都建立了不同的training datasets，包括car，chair，plane等。

**6.1 Comparison with a supervised approach**

**6.2 Generalization across views and instances**

**7. Conclusion & Future work**

我们探索了基于一个end-to-end的geometric reasoning framework，而不是keypoint locations的监督数据，来学习一个3D keypoints的稀疏集合的可能性。我们显示，通过使用两个新的目标函数：relative pose estimation loss和multi-view consistency loss，我们可以在含有不同object instances以及不同的视角的同一个object类别的图片中找到consistent的3D keypoints。我们的keypoints要比直接利用keypoints locations的监督信息学习到的keypoints对于rigid 3D pose estimation任务效果更好。

我们通过训练具有随机背景的ShapeNet图片说明我们的网络对于真实世界的图片也是有用的。最近的在domain adaptation领域的工作可以用来改进我们的结果。同时，也可以直接用带有relative pose labels的真实世界的图片来训练KeypointNet。而这种relative pose labels也可以通过Structure-from-motion来自动估计出来。另一个有意思的方向是利用coarse pose initialization或者联合求解relative transformation，这样就可以不需要pose annotations了。

我们的方法还可以被拓展到处理任意数量的keypoints。比如说，我们可以为每个keypoint都输出一个confidence value，然后设定一个阈值来判断是否需要是真的keypoint，同时在没有顺序的keypoints集合上加一个loss。




### 10. [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://openaccess.thecvf.com/content_cvpr_2017/papers/Simon_Hand_Keypoint_Detection_CVPR_2017_paper.pdf)
*Tomas Simon, Hanbyul Joo, Iain Matthews, Yaser Sheikh*

*CVPR 2017*

This paper proposed a framework to learn 3D keypoints of hand. 

The input is a keypoint detector $$d_0$$ trained on a small labelled dataset $$T_0$$, a sequence of images $$\left\{I_v^f, v=1,2,...,V, f=1,2,..,F\right\}$$, with $$v$$ denote the camera view and $$f$$ denote the time frame.

Hyperparameter: Number of Keypoints $$K$$.

**Step1** First use $$d_0$$ to calculate the image coordinates (no depth) and confidence of each keypoint $$k$$ of $$I_v^f$$, denoted as $$x_v^{f,k}$$ and $$C_v^{f,k}$$. Then use the random sample consensus to pick inliers out of each set $$\left\{(x_v^{k}, C_v^{k})\right\}$$ for each time frame $$f$$. Then the 3D wolrd coodinates are computed as:

$$X_v^{k} = argmin_X \Sigma_{v \in I_v^{k}} ||P_v(X)-x_v^k||^2_F$$

where $$I_v^k$$ is the inlier set, with $$X_f^k \in R^3$$ the 3D triangulated keypoint $$k$$ in frame $$f$$, and $$P_v(X) \in R^2$$ denotes projection of 3D point $$X$$ into camera view $$v$$. They use calibrated cameras, thus $$P_v$$ are known.

**Step2** Then they use a window through the time frame, and pick the frame with highest score. The score is defined as the sum of $$C_v^k$$, thus the frame that has the biggest confidence of all keypoints detection from all camera views.

**Step3** After picking this frame, they add the labelled images into the orignal training dataset and train a new keypoint detector, $$d_1$$. And so on.



### 11. [Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry](https://openaccess.thecvf.com/content_CVPR_2019/html/Kocabas_Self-Supervised_Learning_of_3D_Human_Pose_Using_Multi-View_Geometry_CVPR_2019_paper.html)

*CVPR 2019*


### 12. [Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild](https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_Unsupervised_Learning_of_Probably_Symmetric_Deformable_3D_Objects_From_Images_CVPR_2020_paper.html)

*CVPR 2020*

### 13. [Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Weakly-Supervised_Discovery_of_Geometry-Aware_Representation_for_3D_Human_Pose_Estimation_CVPR_2019_paper.html)

*CVPR 2019*


### 14. [Self-Supervised 3D Hand Pose Estimation Through Training by Fitting](https://openaccess.thecvf.com/content_CVPR_2019/html/Wan_Self-Supervised_3D_Hand_Pose_Estimation_Through_Training_by_Fitting_CVPR_2019_paper.html)

*CVPR 2019*


### 15. [Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects](https://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html)

*CVPR 2022*


### 16. [End-to-End Learning of Multi-category 3D Pose and Shape Estimation](https://arxiv.org/pdf/2112.10196.pdf)

*Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, Luc Van Gool*

*Arxiv 2021*

>这篇文章来自于ETHZ的CVLab，是大佬Luc Van Gool指导的论文。

**Abstract**

在这篇文章里，我们通过keypoints来研究物体的形状和姿态的representation。因此，我们提出了一个端到端的方法来从一张图片里检测2D keypoints，并将其lift到3D keypoints。我们的方法能学习到3D keypoints，但是只有2D keypoints的标注。我们的方法除了是端到端的（输入为2D图片，输出为3D keypoints），还使用了同一个网络来处理多个类别的物体（也就是不同category的images的keypoints可以用同一个网络来学习）。我们使用了基于Transformer的网络来检测keypoints，以及综合图片里的视觉信息。而这些视觉信息之后被用来将2D keypoints lift到3D keypoints.我们的方法可以处理含有遮挡的情况，而且对很大一部分的物体类别都是适用的。在三个benchmarks上的实验证明了我们的方法要比sota的效果好。


**1. Introduction**

基于keypoints的shape和pose的representations是很不错的，因为它足够简单也足够好进行后续的操作。基于keypoints的shape和pose的representations的应用案例包括，3D reconstruction，registration，human body pose analysis，recognition，generation等等。而keypoints通常是作为2D keypoints出现的，因为在图像上标注2D keypoints比较容易。然而，2D keypoints往往不能满足后续任务的需要。在很多应用里，既需要3D shape，也需要3D pose，比如说augmented reality任务。

因此，我们需要来预测3D keypoints，正如这些文章里做的那样：[Augmented autoencoders: implicit 3d orientation learning for 6d object detection]()，[Discovery of latent 3d keypoints via end-to-end geometric reasoning]()，[Learning deep network for detecting 3d object keypoints and 6d poses]()。然而，这些方法都有着以下两个问题之一或者都有：（1）需要3D keypoints、pose或者多个角度的图片来作为监督数据；（2）并没有直接相对于一个固定frame来做pose reasoning。而这篇文章提出的方法，可以直接从单张图片里学习到3D keypoints和pose，从而为一系列下游任务，比如说scene understanding，augmented reality等提供帮助。另一类方法，基于template的从单张图片学习的方法，也可以从2D keypoints里获取3D keypoints和pose：[Optimal pose and shape estimation for category-level 3d object perception]()，[In perfect: Certifiably optimal 3d shape reconstruction from 2d landmarks]()。然而，基于template的方法，除了需要templates之外，还对自遮挡非常敏感，[3d registration for self-occluded objects in context]()给出了解释。因此，我们并不使用基于template的方法，而是使用基于学习的方法，来从单张图片里推断物体的3D keypoints和pose。

在这篇文章里，我们在training和inference的时候，对于每个物体，都只有一张图片。这个设定让我们可以使用十分广泛的数据集，甚至直接从网上下载图片就可以。为了方法具有更好的可扩展性，我们只需要2D keypoints和物体的类别来作为监督数据。也就是说，我们希望我们的模型能够从单张图片里学到物体的3D keypoints和pose，而这些图片里的物体是不同类别，不同个体，甚至我们都不需要同一个物体不同角度的views。

现有的那些能够从单张图片里仅仅通过物体类别这个信息就能学到3D shape和pose的方法叫做deep non-rigid structure-from-motion（NrSfM）。这些方法可以被分为两类：每个模型只能针对一个物体类别的图片：[Deep non-rigid structure from motion]()，[Procrustean regression networks: Learning 3d structure of non-rigid objects from 2d annotations]()，[Procrustean autoencoder for unsupervised lifting]()，[Pr-rrn: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion]()，以及每个模型可以针对多个物体类别的图片：[C3DPO: canonical 3d pose networks for non-rigid structure from motion]()，也就是single和multi-category两个类别。multi-category方法显然更有意思，因为（1）计算复杂度更低，仅仅需要一个神经网络就可以从含有不同类别物体的图片里学习keypoints和pose；（2）有可能能够建立不同类别物体之间的关联性。后面这个特性不仅可以让我们来衡量不同类别物体之间的相似性，而且使得模型的泛化效果更好。

现有的这些方法，[Deep non-rigid structure from motion]()，[Procrustean regression networks: Learning 3d structure of non-rigid objects from 2d annotations]()，[Procrustean autoencoder for unsupervised lifting]()，[Pr-rrn: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion]()，[C3DPO: canonical 3d pose networks for non-rigid structure from motion]()，[Unsupervised 3d pose estimation with geometric self-supervision]()，都是由两个stages组成的：2D keypoints extraction和3D shape and pose estimation。这两个stages一般都是独立处理的。但我们认为，这两个stage是相关的，而且可以从互相那里获得帮助，从而所获取的2D keypoints就更加适合下游的3D reasoning了。为了实现这个目标，我们在提取2D keypoints的keypoint extraction network里加入结构，从而增加一部分信息，也就是伴随着2D keypoints的位置输出，还输出visual context information。然后，这个visual context information和2D keypoints一起被用于后续将2D keypoints lift到3D的network里。我们的实验证明了在获取3D pose和shape时使用visual context information的有效性。


类似于这篇文章（[C3DPO: canonical 3d pose networks for non-rigid structure from motion]()）里的做法，我们利用dictionary learning的方法来表示3D shape，其中所使用的shape basis是通过学习得到的（这个shape basis集合包含各个类别的物体的shape basis）。然后，利用shape basis coefficients就可以恢复每个物体的shape了。然而，正如[C3DPO: canonical 3d pose networks for non-rigid structure from motion]()和[Deep non-rigid structure from motion]()所说，shape basis的size需要很仔细地被设置。在multi-category的情况下，所有的object categories共享一个latent space（也就是shape basis构成的space），而每个category可以有不同长度的shape basis size。而且，如果直接使用shape basis coefficients来将shape basis重构为object shape，会导致模型对于细节或者小扰动过于敏感，文章里提出了一个简单的方法来解决这个问题：学习一个threshold vector来cut-off shape basis coefficients。这个简单的方法能达到很好的效果，要比直接使用稀疏字典学习的方法简单很多。


这篇文章的主要贡献如下：

* 单个端到端的网络，能够对多种不同类别的objects重构它们的3D shape和pose。
* 提出使用image context information来辅助3D shape和pose的学习，而不仅仅只有2D keypoints.
* 我们的方法在multi-category的设置下取得了sota的结果，而且效果好了很多。


**2. Related Work**

从单张图片里将可变形物体（deformable obejcts）的2D keypoints lift到3D keypoint这个问题一般在NrSfM任务框架里被研究。而在NrSfM框架里，任务是从物体随着时间变化而获取的若干张图片里获取物体的poses或者viewpoints。已经有非常多的研究工作利用各种方法来改进NrSfM任务的效果，包括使用sparse dictionary learning（[]()，[]()）、low-rank constraints（[]()）、union of local subsapces（[]()）、diffeomorphism（[]()）以及coarse-to-fine low-rank reconstruction（[]()）。如果我们将同一个类别的不同的object的图片当作同一个object的在不同时刻拍下来的图片的话，我们也可以使用NrSfM框架来构造一个category-specific模型来估计pose和viewpoint，比如说这些工作就是这么干的：[]()，[]()，[]()。

从单张图片里获取一个物体的3D结构并没有过多的被研究。在[]()里，instance segmentation数据集被用来训练一个模型，从而能够在给一张图片的情况下输出3D mesh reconstructions。[]()通过建立2D和3D keypoints之间的联系来改进了结果。尽管最近一些结果已经可以预测viewpoint和non-rigid meshes，但它们针对的都是变化比较小的物体，比如说人脸：[]()，[]()，[]()。

跟我们工作最相近的一条line of research考虑的任务是为不同的输入类别构建单个模型。C3DPO（[]()）的方法是学会将object deformation和viewpoint change这两个因素分解开。它们使用一个网络学到物体相对于canonical shape（规范化shape）的rotation。[]()使用Procrustean regression来确定motion和shape。他们还提出了使用CNN从图片输入里输出human的3D keypoints。然而，他们的方法并不能处理multi-category的情况，也不能处理被遮挡住的keypoints。而且，这个方法还需要时序信息。[]()也研究了human pose estimation，他们提出了一个cyclic-loss，使用了GAN并加入了额外数据集来训练这个GAN。但是，他们的方法仅限于human pose estimation。最近，[]()将Procrustean regression方法进一步加上了autoencoders，这样的方法就可以不需要时序信息就能获取3D shapes了。然而，他们的方法在Procrustean alignement的基础上，还需要额外两个autoencoders，从而在测试的时候很慢。上述所有的这些方法都是以2D keypoints作为他们的输入，而不是图片，所以他们使用了一个keypoint detector，比如说stacked hourglass network。


**3. Multi-category from a Single View**

我们通过获取3D keypoints的方式来表示3D structures，而输入仅仅是某个category的object的一张图片。在训练过程中，我们只有2D keypoints的位置信息以及category类别信息。为了简单起见，我们将方法分为两个stages：（1）从图片种获取2D keypoints和category；（2）将2D keypoints lift到3D。下面，我们会先介绍如何将2D keypoints lift到3D。我们的方法是在NrSfM框架下进行描述的。我们先介绍lifter network，然后再介绍2D keypoint extractor以及如何将这两部分连接在一起成为一个端到端的网络的。


**3.1 Preliminaries - NrSfM**

$$\pmb Y_i = \left[ \pmb y_{i1}, \cdots, \pmb y_{ik} \right] \in \mathbb R^{2 \times k}$$表示的是对于第$$i$$个view，由$$k$$个2D keypoints摞起来的矩阵，也就是每一列表示一个keypoint的2D location。从而，我们将第$$i$$个view的3D结构表示为$$\pmb X_i = \alpha_i \pmb S$$，其中$$\pmb S \in \mathbb R^{d \times 3k}$$是shape basis，$$\alpha_i \in \mathbb R^d$$是coefficients。为了简单起见，我们的相机参数里的projection model是标准的，也就是$$\Pi = \left[\pmb I_{2 \times 2}, \pmb 0 \right]$$。给定相机旋转矩阵$$\pmb R_i \in SO(3)$$，从而式子就写为：$$\pmb Y_i = \Pi \pmb R_i (mat(\alpha_i \pmb S_i))$$，其中$$mat(\cdot)$$操作是将一个$$\mathbb^{1 \times 3k}$$的矩阵变成一个$$\mathbb^{3 \times k}$$的矩阵的操作。从而，如果我们有$$n$$张图片，并且有这$$n$$张图片里2D keypoints的location的数据，也就是$$Y_i$$，在NrSfM框架里，目标函数就是：

$$ min_{\alpha_i, \pmb S, \pmb R_i \in SO(3)} \Sigma_{i=1}^n \mathcal L(\pmb Y_i, \Pi \pmb R_i (mat(\alpha_i \pmb S_i))) \tag{1}$$

其中$$\mathcal L$$表示的是某种loss函数。

上述问题是ill-posed的，因此有一系列关于$$\alpha_i$$和$$\pmb S$$的假设就被提了出来。最常见的是：low_rank（[]()）,finite-basis（[]()），以及sparse combination（[]()）。在这篇文章里，我们更感兴趣的是如何通过学习的方法来解决公式(1)。但是和别的NrSfM问题不一样的是，我们的输入是单张图片，而且我们的单个网络需要能够处理不同object的图片。


**3.2 Multi-category Formulation**

我们考虑的是multi-category的情况，这对应着multi-class NrSfM。因此，$$mat(\alpha_i \pmb S) \in \mathbb R^{3 \times k}$$需要能够对于不同的category有不同数量的keypoints，也就是$$k$$不是固定的。$$\pmb Z$$表示object categories的集合，而$$z_i$$表示的是sample $$i$$的category。$$\pmb Z$$里的每个category $$z$$的keypoints数量记为$$k_z$$，因此，一共有$$k = \Sigma_z k_z$$个keypoints。为了对每个类别找到正确的keypoints的数量，我们使用了一个subset selection vector $$\zeta_z \in {0,1}^k$$，用来标记这些keypoints里哪个是category $$z$$的。有了上述这些定义之后，我们可以将公式1重新描述为：

$$ min_{\alpha_i, \pmb S, \pmb R_i \in SO(3)} \Sigma_{i=1}^n \mathcal L(\pmb Y_i \ast \zeta_{z_i}^T, \Pi \pmb R_i(\alpha_i \pmb S) \ast \zeta_{z_i}^T) \tag{2}$$

上述公式里的$$\ast$$表示这个操作是element-wise乘法，而不是矩阵乘法，而且使用了broadcasting。在公式2里，$$\pmb R_i$$和$$\alpha_i$$是依赖于输入的，而$$\pmb S$$是所有图片所有category共用的。为了将公式2变为可学习的，我们让$$\alpha_i$$是某个网络的输出，而这个网络的输入是$$\pmb Y_i$$，也就是$$\alpha(\pmb Y_i)$$。我们再将$$\alpha$$函数分解为两个函数，$$\alpha(\pmb Y_i) = g(f(\pmb Y_i))$$，其中$$g$$是一个affine function，$$g(v) = W_g v + b_g$$，而$$v \in \mathbb R^{F}$$，$$W_g \in \mathbb R^{D \times F}$$，$$b \in \mathbb R^{D}$$。我们对$$f$$不做任何限制，它就是一个输入为$$\pmb Y_i$$，输出为一个维度为$$F$$的向量的函数。同样的，我们也将$$\pmb R_i$$表示为$$\pmb Y_i$$的函数。既然我们使用learning的方法来实现上述目标函数，记网络的参数为$$\theta$$，那么公式2就写为：

$$ min_{\theta} \Sigma_i \mathcal L(\pmb Y_i \ast \zeta_{z_i}^T, \Pi \pmb R_i(\pmb Y_i) mat((W_g f(Y_i) + b_g) \pmb S) \ast \zeta_{z_i}^T) \tag{3}$$

从而，shape basis coefficient $$\alpha$$就隐式的用$$W$$和$$b$$来表示了，而$$W$$和$$b$$是网络参数，所以是所有图片共享的。


**3.3 Cut-off Shape Coefficients**


### 17. [Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Weakly-Supervised_Discovery_of_Geometry-Aware_Representation_for_3D_Human_Pose_Estimation_CVPR_2019_paper.html)

*CVPR 2019*


### 18. [Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects](https://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html)

*CVPR 2022*


### 19. [Unsupervised Temporal Learning on Monocular Videos for 3D Human Pose Estimation](https://arxiv.org/pdf/2012.01511.pdf)

*Arxiv 2022*


### 20. [Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Weakly-Supervised_Discovery_of_Geometry-Aware_Representation_for_3D_Human_Pose_Estimation_CVPR_2019_paper.html)

*CVPR 2019*


### 21. [Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction](https://arxiv.org/pdf/2203.10212.pdf)

*Arxiv 2022*


### 22. [Learning deep network for detecting 3D object keypoints and 6D poses](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Learning_Deep_Network_for_Detecting_3D_Object_Keypoints_and_6D_CVPR_2020_paper.pdf)
*Wanqing Zhao, Shaobo Zhang, Ziyu Guan, Wei Zhao*

*CVPR 2020*


### 23. [SNAKE: Shape-aware Neural 3D Keypoint Field](https://arxiv.org/pdf/2206.01724.pdf)

*Arxiv 2022*


### 24. [Semi-automatic 3D Object Keypoint Annotation and Detection for the Masses](https://arxiv.org/abs/2201.07665)

*Arxiv 2022*

[CODE](https://github.com/ethz-asl/object_keypoints)



## 3D Keypoint Learning

### 1. [DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_DenseFusion_6D_Object_Pose_Estimation_by_Iterative_Dense_Fusion_CVPR_2019_paper.html)

*CVPR 2019*

### 2. [PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2020/html/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.html)

*CVPR 2020*

### 3. [KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent Objects](https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_KeyPose_Multi-View_3D_Labeling_and_Keypoint_Estimation_for_Transparent_Objects_CVPR_2020_paper.html)

*CVPR 2020*

### 4. [Learning Deep Network for Detecting 3D Object Keypoints and 6D Poses](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Learning_Deep_Network_for_Detecting_3D_Object_Keypoints_and_6D_CVPR_2020_paper.html)

*CVPR 2020*

### 5. [Photo-Geometric Autoencoding to Learn 3D Objects from Unlabelled Images](https://arxiv.org/pdf/1906.01568.pdf)

*Arxiv 2019*

### 6. [Structured Domain Adaptation for 3D Keypoint Estimation](https://ieeexplore.ieee.org/abstract/document/8885979)

*3DV 2019*


### 7. [Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images](https://arxiv.org/pdf/2204.10776.pdf)

*Arxiv 2022*


### 8. [3D Sketch-Aware Semantic Scene Completion via Semi-Supervised Structure Prior](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_3D_Sketch-Aware_Semantic_Scene_Completion_via_Semi-Supervised_Structure_Prior_CVPR_2020_paper.html)

*CVPR 2020*











## 2D Keypoint Learning

### 1. [OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)
*Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh*

*CVPR2017 and TPAMI 2019*

我们所说的是TPAMI的那个版本，比CVPR的那一版多了一些内容。

**1. Title**
realtime表明这个算法本身运行速度快，multi-person 2D pose estimation清晰的说明了这个算法是用来干什么的，而part affinity field则是说明所用的主要方法。而openpose是给这个算法起个好听的名字，可以发现并不是常见的标题名字的简写组成的名字，openpose后来成为了一个成熟的软件。这种起标题的方式简明，清晰，直接说明白了用什么办法干了什么事情，是一种很好的起标题的方式。


**2. Abstract**
两句话介绍了这篇论文所要解决的问题的背景，也就是multiple 2D person pose estimation的背景，并说明了这篇论文利用的是PAF来做到实时的效果。然后介绍了一下之前利用PAF的论文效果如何，为什么这篇论文利用PAF效果好。然后介绍了实验部分，最后说明这篇文章还发布了Openpose这样一个开源软件。

**3. Introduction**

3.1. 这篇文章的目的是要做multi-person pose estimation，而对于单人的pose estimation有很多论文已经做得很好了，但对于多人来说，有以下几个困难的地方：1）首先，我们并不知道图里到底有几个人，而且每个人所在的位置、大小都不清楚；2）其次，人与人之间可能存在干涉，比如说遮挡、关节的旋转等等，很难分清楚到底哪部分属于哪一个人；3）以往的论文里的算法的复杂度都会随着图里人数量的增加而增加，从而很难实现实时。

3.2. 对于multi-person 2D pose estimation，top-down的方式很常见，也就是先检测图片里有几个人，然后对每个人实行pose estimation，因为单人的pose estimation已经做得很好了，这个算法并不复杂。但它存在着两个很大的问题：首先如果一开始检测人的时候就检测错了或者遗漏了，那之后是没有补救办法的；其次，这样的方法需要对检测出来的每个人都做单人pose estimation，这会使得算法的复杂度和人的数量成正比。所以说，bottom-up的方法也被提了出来，这种方法有能够解决上述两个问题的潜力。但之前的bottom-up方法仍然效率不高，因为它们在最后还是需要利用全局信息来辅助判断，从而要花不少时间。

3.3. 我们在这篇文章里利用Part Affinity Field实现了实时的multi-person 2D pose estimation，而且在几个数据集上效果都是很具有竞争力的（这样表达说明它们的效果不一定比其他论文好，它们的卖点主要是快）。Part Affinity Field是一个2D向量的集合，表示的是四肢的位置和方向信息。我们之后会说明，利用bottom-up的方式，将detection和association结合起来（模型有两个主要部分，一个是PAF refinement，另一个是body part prediction refinement，而PAF就是实现association的方式）逐步推进，可以在利用很小的计算资源的情况下达到很好的效果。

3.4. TPAMI的这一版比CVPR的那一版多的内容有：1）说明了其实PAF refinement才是效果好的主要原因，不可或缺，而body part prediction refinement没那么重要。文章里也做了实验，加深了网络深度，而去掉了body part prediction refinement的部分，效果变好了，速度也变快了；2）我们又提出了一个有标注的foot数据集；3）为了显示我们这个模型很强的generality，我们在vehicle keypoint detection的任务上也做了实验；4）我们这篇论文是我们的开源软件OpenPose的说明文档，OpenPose是第一个实时的body, foot, hand和facial的keypoint识别软件，我们还和Mask R-CNN, Alpha-Pose等著名keypoint识别算法在运行时间上作了比较。


**4. Related Work**

**Single Person Pose Estimation**。单人的pose estimation，因为人是关节性的，所以说传统的方法都是通过将对于身体部位的布局的检测和它们之间的空间关系联合起来看，然后使用某种inference的方式来学习。而关节型的pose estimation的各部位之间的空间关系的表示方法，要么就是1）tree-structured graphical models，在相邻的部位之间做一些encoding；或者是2）non-tree models，在第一种tree-structured model的基础上加一些别的连线，从而实现对遮挡、对称、以及长途关系的表示（因为tree-structured model只对邻居部分有表示）。而对于身体各个部位的检测，CNN就起到了主要的作用，其在body part estimation里比传统的方法要好很多。

> 也就是说，其实single person pose estimation主要就分为两大部分，先想办法检测出身体的各个部分，然后再想办法利用一种好的表示方式来将这些部分连起来，从而实现pose estimation，也就是keypoint detection。

后来也有文章直接构建一个深度的graphical model将两部分放在一起解决；还有文章通过使用感受野比较大的CNN来学习身体部分之间的空间关系。也有文章是multi-stage的方式，每次利用全局信息来进一步优化每个身体部分所在位置的信息。但所有的上面的这些方法都是针对单人的，人的位置和scale在数据集里都是差不多的（不存在非常边缘或者大小差别过大的情况）。

**Multi-person Pose Estimation**。对于多人的pose estimation来说，绝大多数方法使用的是top-down的方式，即首先检测人，然后再单独的在每个检测到人的区域实行单人pose estimation算法。尽管这种方法使得成熟的单人pose estimation算法可以得到利用，但它不仅会因为早早的就预设了每个人的问题而存在问题，也会因为人和人之间也会存在空间依赖关系而出现问题（比如说遮挡等）。有一些文章已经开始考虑引入人与人之间的依赖关系了。有篇bottom-up的文章就没有使用person detection，而是将每个部分联系到每个人，但这个算法需要解决一个integer linear programming的问题，这个问题的计算复杂度很高，所以处理一张照片的时间要个把小时。后续这篇文章的跟进工作改进了身体部分的检测算子，并且改进了优化算法，从而使得检测一张照片的时间变成了几分钟，但最多只能处理150个身体部分。

在这篇文章的早期版本里，也就是CVPR那个版本里，我们介绍了part affinity field，它是一个representation，是由一系列的flow field组成的，而这些flow fields包含了每两个所检测到的身体部分之间的信息（可能是属于不同人的，也可能是一个人的）。和我们上面提到的两个引入人与人之间的依赖关系的论文不同，我们不需要训练就可以直接从PAF里获取pairwise的信息。而这些pairwise的信息对于multi-person pose estimation就已经是足够的了。

在我们的那篇CVPR文章提出之后，又有了一些新的工作。有篇文章进一步简化了各个身体部位的关系graph从而能更快的inference，其还将关节化的人的tracking形式化的表示为身体部分的spatio-temporal的grouping。还有篇文章提出了利用associative embedding来为每个keypoint打上标签，从而可以将标签相似（也就是说embedding之间的距离小）的keypoint归属于同一个人。还有一篇文章提出检测每个keypoint和它的相对位移，再计算每个keypoint属于哪个人。还有一篇文章提出了Pose Residual Network，其输入是keypoint和所检测到的人，而输出则是将keypoint归属到每个人。

我们这篇文章，对于之前的那篇CVPR文章做了一些扩充。我们证明PAF refinement是必要的且是最重要的，还做了实验去除了body part detection refinement的部分，速度增加了，效果变好了。我们同时还提出了第一个body和foot keypoint联合检测的detector，并且提出了一个foot keypoint数据集。我们证明将它们两联合检测不仅会减少inference的时间，还会保持准确率。最后，我们提出了OpenPose，是第一个实时进行body, foot, hand和facial keypoint detection的实时开源软件。


**5. Method**

![overview]({{ '/assets/images/OPENPOSE-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. Overall pipeline. (a) Our method takes the entire image as the input for a CNN to jointly predict (b) confidence maps for body part detection and (c) PAFs for part association. (d) The parsing step performs a set of bipartite matchings to associate body part candidates. (e) We finally assemble them into full body poses for all people in the image.*

Fig 1显示了我们的算法的一个整体的流程。整个系统的输入是一张大小为$$w \times h$$的RGB图片，而输出为图中每个人的生理结构上的2D keypoints的位置。首先，一个feedforward网络输出一个集合$$S$$，用来表示各个身体部位位置的2D confidence maps，和一个part affinity fields（也就是2D的向量）的集合$$L$$用来表示各个身体部位之间的从属程度。集合$$S = (S_1, S_2, ..., S_J)$$有J个confidence maps，每个对应一个身体部位，其中$$S_j \in R^{w \times h}$$，$$j = \lbrace 1,2,...,J \rbrace$$。而集合$$L = (L_1, L_2, ..., L_C)$$有$$C$$个向量，每个对应一个肢体，其中$$L_c \in R^{w \times h \times 2}$$，而$$c \in \lbrace 1,...,C \rbrace$$。为了简洁，我们将身体部位的pairs描述为肢体（因为这里的身体部位就是一个keypoint，而keypoint pair就是将两个keypoint连起来，就表示了一部分肢体），但是某些身体部位的pair并不是肢体，比如说face，我们就笼统的这么说了。我们可以从fig 2看到，$$L_c$$里的每个点都是一个2D的vector。最终，confidence maps和PAFs（也就是集合S和集合L）通过greedy inference联系了起来用于输出图中所有人的2D keypoints位置。

![algorithm]({{ '/assets/images/OPENPOSE-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. 上方：多人pose estimation。同一个人的身体部分被连了起来，也包括了脚的keypoints（大脚趾，小脚趾和脚后跟）。下左：关于连接右手肘和手腕的肢体的PAFs。颜色表明了方向。下右：在关于连接右手肘和手腕的肢体的PAFs的每个像素点处的2D向量包含了肢体的位置和方向信息。*

**5.1. Network Architecture**

![architecture]({{ '/assets/images/OPENPOSE-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 3. multi-stage CNN的结构。前一部分的stage用来预测PAFs$$L^t$$, 而后一部分的stage用来预测confidence maps $$S^t$$。每个stage的输出和图片feature连接起来，作为下一个stage的输入。.*

我们的模型结构，如fig 3所示，迭代的预测包含身体部位之间关系的PAFs，如fig 3里蓝色的部分，之后再来检测confidence maps，如fig 3里的米色。这样一种迭代的方式使得预测更加的准确（注意中间的迭代stage也都是有supervision的）。

在我们CVPR的那版里，我们用了$$7 \times 7$$的卷积核，而我们这篇论文里，用3个$$3 \times 3$$的卷积核来替代从而减小计算量。如同DenseNet里那样，这3个卷积层的输出也连接起来了，从而能既有高层feature又有低层的。

**5.2 Simultaneous Detection and Association**

输入的图像通过一个CNN（用的是VGG-19的前10层）来生成一系列的feature maps $$F$$，之后再输出到第一个stage当中。在这个stage里，网络输出一个集合的PAFs $$L^1 = \phi ^ 1 (F)$$，其中$$\phi ^ 1$$表示stage1里用来inference的CNN。在之后的stages里，前一个stage的输出和原始的图像feature maps $$F$$连接起来作为输入，用来输出更加精确的结果

$$L^t = \phi ^ t (F, L^{t-1}), 2 \leq t \leq T_p$$

其中$$\phi ^ t$$表示stage t里用来inference的CNN，$$T_p$是PAF stage的总数。在$$T_p$$个PAF stage之后，再来计算confidence maps，利用的是最新的PAF结果：

$$S^{T_p} = \rho^t (F, L^{T_p}), t = T_p$$

$$S^t = \rho^t (F, L^{T_p}, S^{t-1}), T^p < t \leq T_p + T_C$$

其中$$\rho^t$$指的是stage t用来inference的CNN，而T_C是confidence maps所需要循环的次数。

这个方法和我们的CVPR那个版本里的方法有很大的不同，在那个版本里，每个stage都要进行PAF和confidence maps的计算。因此我们这个版本的计算量减少了一半。我们通过实验发现，增加PAF循环的次数，可以使得效果变好，而增加confidence maps的循环的次数并不会。直觉上来看，通过PAF的结果，我们可以猜到每个身体部位（也就是每个keypoint）的位置，但通过confidence maps的结果，我们无法知道每个身体部位属于哪个人。

![PAF]({{ '/assets/images/OPENPOSE-4.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 4. 右前小臂肢体的PAF。尽管在一开始的stage里还有一些不清晰，但在之后的stage里可以看到PAF很清晰。.*

Fig 4显示了随着stage的增加，PAF计算结果逐渐被精确的过程。confidence maps的计算基于最后一个PAF stage输出的结果，而随着confidence maps stage的推进，结果区别并不大。为了让模型能够预测PAF和body part的confidence maps，我们在每个stage的结尾都使用一个loss function。我们使用的是输出的结果和实际上的body part的confidence maps和PAF之间的$$L_2$$ loss。我们这里给loss function加了权重，因为有些数据集并没有标注完整。对于PAF的stage $$t_i$$和confidence map的stage $$t_k$$的loss function是：

$$ f_L^{t_i} = \Sigma_{c=1}^C \Sigma_p W(p) ||L_c^{t_i}(p) - L_c^{\ast}(p)||^2_2 $$

$$ f_S^{t_k} = \Sigma_{j=1}^J \Sigma_p W(p) ||S_j^{t_k}(p) - S_j^{\ast}(p)||^2_2 $$

其中$$L_c^{\ast}$$是PAF的ground truth，$$S_j^{\ast}$$是身体部分confidence map的ground truth，$$W$$是一个非0即1的二分掩码，如果某个位置没有标注就是0，有标注就是1。这个$$W$$是用来避免因为没有标注而导致的错误训练。我们在每个stage都使用loss function，用来解决梯度消失的问题，因为每个stage结尾都有loss function，对梯度的值进行了补充。从而整体的的loss function就是：

$$ f = \Sigma_{t=1}^{T_p} f_L^t + \Sigma_{t=T_p + 1}^{T_p + T_C} f_S^t $$


**5.3. Confidence Maps for Part Detection**

为了能够在训练过程中计算上述的$$f_S$$，我们从有标注的2D keypoints上生成confidence maps $$S^{\ast}$$的ground truth。一个confidence map是一个2D的矩阵，用来表示一个keypoint出现在一个像素点的概率值。理想状态下，如果图里只有一个人，那么每个confidence map应该只有1个峰（该keypoint没有被遮挡住的情况下）;如果有多个人，那么每个confidence map对于没有被遮挡住的这一类keypoint都应该有峰（比如说某个confidence map专门表示人的nose的keypoint）。

我们先来对于每个人$$k$$生成confidence map，$$S_{j,k}^{\ast}$$。$$x_{j,k} \in R^2$$表示人$$k$$的身体部分$$j$$在图中位置的ground truth。从而$$S_{j,k}^{\ast}的位置$$p \in R^2$$处的值就是：

$$ S_{j,k}^{\ast}(p) = exp(-||p-x_{j,k}||^2_2 / \sigma^2) $$

其中$$\sigma$$控制峰的大小。从而整个图片（可能包含多个人）的ground truth就是：

$$ S_j^{\ast}(p) = max_k S_{j,k}^{\ast}(p) $$

**5.4 Part Affinity Fields for Part Association**

![cal]({{ '/assets/images/OPENPOSE-5.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 5. 身体部位association策略。（a) 几个人的两种身体部分（也就是keypoint）分别用红蓝点表示，而所有的点之间都连上了线。（b) 利用中间点进行连线。黑线是正确的，绿线是错误的，但他们都满足连接了一个中间点。(c) 利用PAF来连接，黄色的箭头就是PAF的结果。利用肢体来表示keypoint的位置和keypoint之间的方向信息，PAF减少了错误association的可能性。.*

给定一些已经检测到了的body parts（Fig 5里的红色和蓝色的点），那我们该如何将它们组合起来从而构建未知数量的人的肢体呢？我们需要对每一对body part keypoints都有一个confidence measure，也就是说，measure它们是否属于同一个人。一种可能的方式就是检测这一对keypoints的中间是否还有附加的midpoint。但是当人聚集在一起的时候，很容易出错。这种方式之所以不好是因为1）它仅仅有位置信息，并没有一对keypoint之间的方向信息；2）它仅仅用了midpoint，而不是这两个keypoints之间的所有部分当成一个肢体来使用。

Part Affinity Fieds (PAFs)解决了这些问题。它对于每一对keypoints构成的肢体提供了位置和方向信息。每一个PAF都是一个2D的vector field，在Fig 2里有显示。对于每个肢体的PAF的每个位置的值，其都是一个2D的向量，包含了位置和这个肢体一个keypoint指向另一个keypoint的方向。每个类别的肢体都有一个对应的PAF（由对应的body part keypoints对组成）。

![fig]({{ '/assets/images/OPENPOSE-6.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 6.*

考虑fig 6所示的一个简单的肢体。$$x_{j_1, k}$$和$$x_{j_2, k}$$是人$$k$$的身体部位$$j_1$$和$$j_2$$的ground truth，而这两个部位组成了肢体$$c$$。对于肢体$$c$$上的一点$$p$$，$$L_{c,k}^{\ast}(p)$$是一个单位向量，从$$j_1$$指向$$j_2$$；对于其它的点，$$L_{c,k}^{\ast}(p)$$的值都是0。

为了能在训练过程中计算$$f_L$$的值，我们需要定义PAF的ground truth，也就是对于人$$k$$，$$L_{c,k}^{\ast}$$在$$p$$点的值为$$L_{c,k}^{\ast}(p) = v$$ if $$p$$ on limb $$c, k$$ and $$0$$ otherwise。

这里

$$v = (x_{j_2, k} - x_{j_1, l}) / ||x_{j_2, k} - x_{j_1, l}||$$

是肢体的有方向的单位向量。一个肢体上的点不仅仅只有两个keypoints连线上的，而是有一个距离阈值，比如说：

$$ 0 \leq v (p - x_{j_1,k}) \leq l_{c,k}$$ 和 $$|v_{verticle} (p - x_{j_1,k})| \leq \sigma_l$$

其中肢体宽度$$\sigma_l$$自定义的，肢体长度由两个keypoints决定，也就是

$$l_{c,k} = ||x_{j_2, k} - x_{j_1, k}||$$

$$v_{verticle}$$是垂直于$$v$$的。


而整个图片的PAF的ground truth是对于所有人取了均值：

$$ L_c^{\ast}(p) = 1/n_c(p) \Sigma_k L_{c,k}^{\ast}(p) $$

在测试过程中，我们通过计算连接两个keypoints的线段间的PAF的积分来衡量这两个keypoints是否构成了一个肢体。对于两个身体部分$$d_{j_1}$$和$$d_{j_2}$$，我们计算：

$$ E = \int_{u=0}^{u=1} L_c(p(u)) (d_{j_2} - d_{j_1})/||d_{j_2} - d_{j_1}|| du $$

其中$$p(u) = (1-u) d_{j_1} + u d_{j_2}$$。在实践中，我们通过等距离采样来近似这个积分值。


**5.5. Multi-Person Parsing using PAFs**

对于每个身体部位keypoint的location，我们都有好几个备选的值，这是因为图中有多个人或者因为计算错误。而这些keypoints组成的肢体就会有很多种可能了。我们用5.4里定义的积分来计算每个肢体的积分值。从而问题变成了，如何在众多的有着不同积分值（也就是score）的肢体集合中，选择合适的肢体并将其正确连接起来，而这是个NP-hard的问题，如fig 7所示。

![fig]({{ '/assets/images/OPENPOSE-7.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 7. Graph matching。(a) 原始的图片，已经有了身体部位keypoint标注了。(b) K-partite graph。(c) 树状结构。(d) 二分图。*

在这篇文章里，我们使用一种greedy relaxation的方法，持续性的产生高质量的匹配。我们猜测这种方法有效的原因是上述计算的积分值（也就是每个肢体的score）潜在的含有global信息，因为PAF的框架具有较大的感受野。

具体来说，首先，我们获得整张图片身体部分keypoint的集合，$$D_J$$，其中$$D_J = \lbrace d_{j_m}: j \in \lbrace 1, ..., J \rbrace, m \in \lbrace 1, ..., N_j \rbrace \rbrace$$，$$N_j$$是身体部位$$j$$的候选数量，而$$d_j^m \in R^2$$是身体部位$$j$$的第$$m$$个候选位置。我们需要将每个身体部位keypoint连接到属于同一个人的同一个肢体的其它身体部位keypoint上，也就是说，我们还需要找到正确的肢体。我们定义$$z_{j_1, j_2}^{m,n} \in \{0, 1\}$$来表示两个身体部位keypoint的候选，$$d_{j_1}^m$$和$$d_{j_2}^n$$是否连在一起，我们的目标是为$$Z = \{z_{j_1, j_2}^{m,n}: j_1, j_2 \in \{1, ..., J\}, m \in \{1, ..., N_{j_1}\}, n \in \{1, ..., N_{j_2}\}$$找到最优的值。

如果我们考虑一个特定的keypoint的pair $$j_1$$和$$j_2$$（比如说neck和right-hip），叫做c-肢体，而我们的目标是：

$$ \max\limits_{Z_c} E_c = \max\limits_{Z_c} \Sigma_{m \in D_{j_1}} \Sigma_{n \in D_{j_2}} E_{m,n} z_{j_1, j_2}^{m,n}$$

$$s.t., \forall m \in D_{j_1}, \Sigma_{n \in D_{j_2}} z_{j_1, j_2}^{m, n} \leq 1$$

$$ \forall n \in D_{j_2}, \Sigma_{m \in D_{j_1}} z_{j_1, j_2}^{m,n} \leq 1$$

其中，$$E_c$$是所有的c-肢体的积分值的和（可能有多个c-肢体，因为可能有多个人），$$Z_c$$是$$Z$$的只关于c-肢体的子集，$$E_{m,n}$$是keypoint $$d_{j_1}^m$$和$$d_{j_2}^n$$之间的定义的积分值，上述要优化的目标的条件，使得我们所学习到的结果里不会有两个肢体公用同一个keypoint。我们可以用Hungarian算法来获取上述优化的结果。

现在我们考虑所有的肢体，那么上述优化的式子即是需要考虑整个$$Z$$并且需要计算所有肢体的所有可能结果，计算$$Z$$是一个K-维的匹配问题（K是肢体的数量）。这个问题是个NP-hard的问题，有很多relaxations的算法存在。在我们这篇论文中，我们添加了两个relaxation。首先，完整的图会对于每两个不同类别的keypoint都有edge，而我们将这个图简化为其能表示人的pose的spanning tree就可以，而多余的edge就不要了。其次，我们将上述K-维的匹配问题解构为一系列二分匹配的子问题并且独立的解决这些问题，所利用的就是每个spanning tree的相邻的两个node所对应的值以及它们之间的连线，所以说是独立的。第二个relaxation之所以可行，直觉上来说，spanning tree里相邻的两个node之间的关系是由PAF网络学习到的，而非相邻的两个node之间的关系是由CNN网络学习到的。

有了上述两个relaxations，我们的问题被简化为：

$$ \max\limits_{Z} E = \Sigma_{c=1}^C \max\limits_{Z_c} E_c $$

从而我们将这个优化问题分解为独立的每个pair的优化问题，而这个在之前所述，可以用Hungarian算法解决。我们再将有共同keypoint的肢体联合起来，这样其就表示出了一个完整的人的pose，或者说骨架。我们的第一个relaxation，将完整的图简化为spanning tree使得整个算法获得了很大程度的加快。

我们目前的模型仍然有多余的PAF连接（比如说耳朵和肩膀的连接，手腕和肩膀的连接等）。这样冗余的连接使得我们的算法对于人群很密集的时候准确度较高。对于冗余的PAF连接，也就是有冗余的肢体，我们在5.5里的parsing算法进行一些简单的修改就行。


**6. OpenPose**

现在有一系列cv和ml的应用需要2D人的pose estimation作为系统的输入。为了帮助研究者们加速它们的工作，我们公布了OpenPose，是第一个实时的只通过一张输入图片联合检测多人的body，foot，hand和face keypoints（一共135个）的系统。

**6.1. System**

目前的2D人的pose estimation库，比如Mask R-CNN或者Alpha-Pose，需要用户自己运行模型，而且也需要自己运行数据处理。而且，目前的face和body的keypoint检测子并未被联合起来，需要用不同的库来实现。而OpenPose解决了所有的这些问题。它可以运行在不同的平台上，包括Ubuntu，Windows，Mac OSX，以及嵌入式系统（比如Nvidia Tegra TX2)。OpenPose还为不同的硬件提供了支持，比如CUDA GPUs，OpenCL GPUs，以及仅有CPU的设备。用户还可以使用image，video，webcam，以及IP camera流作为输入。我们还可以选择是直接展示结果还是将结果存在硬盘里，允许或者不允许body, foot, face和hand的keypoint检测，控制所使用的GPU的数量，跳过某些frame等操作。

OpenPose包括三个不同的组件：(a) body+foot检测；(b) hand检测；(c)face检测。核心组件是body+hand检测。除了使用我们这论文里的模型，你还可以选择使用我们那篇CVPR论文里的模型，而且是在COCO和MPII数据集上训练过的。当我们有了body的keypoint后，face的bounding box就可以通过body的keypoint来确定了，特别是ears，eyes, neck, nose等这些点。而同样的，hand的bounding box也可以通过arm keypoint来确定。这种思想传承于我们在introduction里提到的top-down的keypoint检测算法。OpenPose还提供3D的人的pose estimation，是通过使用non-linear Levenverg-Marquardt refinement来实现3D triangulation实现的，而输入是多个同步的相机。

**6.2. Extended Foot Keypoint Detection**

现存的人的pose数据集包含有限的body part类型。MPII数据集标注了ankles, knees, hips, shoulders, elbows, wrists, necks, torsos以及head tops。而COCO数据集还包括了一些face keypoints。对于这两个数据集，都不含foot keypoints,仅仅有ankles keypoints。但是很多图形学的应用要求foot keypoints，至少要有big toe和heel。如果没有foot keypoints，有很多图形学的应用就会出现floor penetration，foot skate，candy wrapper effect等问题。而我们重新标注了很多foot keypoint的数据。

使用我们的数据集，我们训练了一个foot keypoint检测算法。一个naive的foot检测子可以通过先训练一个body keypoint检测子从而获取脚部的bounding box，之后再在bounding box上训练foot keypoint检测子。但这个方法仍然有我们在introduction里就说过的问题。在我们的论文里，我们使用检测body keypoint的模型来检测body+foot keypoint。我们的body+foot检测模型还包含了两个hip keypoint之间的那个点，为了在upper torso看不到的情况下仍然有好的效果。我们在实验中发现，加入了foot keypoint也有助于body keypoint的学习，特别是ankle。fig 8显示了有些情况下如果没有foot keypoint，无法预测ankle keypoint。

![foot]({{ '/assets/images/OPENPOSE-8.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 8. Foot keypoint analysis。(a) foot keypoint标注，包括big toes，small toes和heels。(b) 仅仅检测body keypoint的模型并未成功检测到右脚ankle这个keypoint。(c) body+foot keypoint联合检测的模型成果检测到了右脚ankle keypoint，因为foot keypoint信息帮助了其的检测。*



**7. Datasets and Evaluations**

我们在multi-person pose estimation的三个benchmarks上验证我们的方法：(1) MPII human multi-person dataset，其包括了3844个training和1758个testing图片，每张图片都是多人的图片，标注了14个body keypoints；(2) COCO keypoint challenge dataset，每张图片也包括了多人，每个人标注了17个keypoints（5个面部的，12个身体的）；(3) 文中提出的foot dataset，是由COCO dataset加上我们自己的标注生成的，15K张图片。这三个数据集包括了各种各样场景下的图片，而且还包括了人群，人的scale不同，遮挡，以及接触等众多情况。我们的方法在COCO 2016 keypoints challenge上获了第一名，在MPII数据集上远超其它方法。我们还与Mask-RCNN和Alpha-Pose进行了对比，量化了我们这个系统的效率，并分析了失败的案例。


**在MPII，COCO和自己生成的数据集上的实验省略不说了，效果都很好。论文也和Mask-RCNN等方法进行了对比，在效果差别很小的情况下，速度要快了很多。我们重点来看看在vehicle pose estimation上应用论文中的模型的内容。**


**7.1. Vehicle Pose Estimation**

我们这篇论文里的方法不仅限于human body或者foot的keypoint检测，还可以拓展到任意的keypoint检测任务。为了说明这个，我们用同样的网络结构在vehicle keypoint detection任务上也进行了运行。还是用在object keypoint similarity (OKS)上定义的mean average precision (mAP)来衡量效果。效果很不错，fig 9显示了在数据集上的效果。这个实验用的是Intersection dataset，从这篇文章来的：https://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf

![car]({{ '/assets/images/OPENPOSE-9.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 9. 验证集里的vehicle keypoint检测的结果，vehicle的keypoint在很具有挑战性的情况下仍然被检测出来，这些情况包括遮挡、vehicles之间的overlap，不同的scale等。*


**8. Conclusion**

实时的multi-person 2D pose estimation是使得机器能够理解和推断人和人之间的互动的关键点。(1) 在这篇文章里，我们展示了一个keypoint association的explicit nonparametric representation，其包含了人的肢体的position和orientation的信息。(2) 我们设计了一个联合学习keypoint和keypoint association的模型。(3) 我们使用了一个greedy parsing的算法来产生高质量的人的body pose的结果，而且对于多人的情况仍然效率很高。(4) 我们证明了PAF refinement要比keypoint detection refinement重要得多，从而让我们的模型相对于之前的CVPR版本要快很多。(5) 将body和foot keypoint检测联合起来会在效果和效率上都有提升。我们构造了一个包含了15K张图片的foot keypoint数据集。(6) 我们将这篇论文的结果开源作为OpenPose，是第一个实施的body, foot, hand和face keypoint检测系统。OpenPose在很多地方都有应用，且已经被收入了OpenCV库里。


### 2. [D2-Net: A Trainable CNN for Joint Description and Detection of Local Features](https://openaccess.thecvf.com/content_CVPR_2019/papers/Dusmanu_D2-Net_A_Trainable_CNN_for_Joint_Description_and_Detection_of_CVPR_2019_paper.pdf)

*Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torri, Torsten Sattler*

*CVPR 2019*

这篇文章解决了在复杂图片条件下找到可靠的像素点之间的对应关系的问题。这篇文章提出了一个简单的CNN网络用来完成两个任务：dense feature descriptor和feature detector。通过将detection安排在后续操作中，这篇文章方法所获得的keypoints和其它基于low-level features的早期detection的效果相比要好很多。这个模型的训练label仅仅需要像素对应关系这样一个annotations就可以。

在images之间建立像素点的对应关系是一个很基础的CV问题，其可以被应用在3D CV，视频压缩，跟踪，定位等等任务里。稀疏的局部特征是预估对应关系的一个重要方法。这些方法基于一种detect-then-describe的模式，先利用一个feature detector来找到一批keypoints，然后根据这些keypoints和其周围的点构建一些image patches，再利用descriptor对这些image patches给出feature description，作为这些keypoints的features。稀疏的局部特征方法有以下几个优势：1）对应关系可以通过nearest neighbor search和欧式距离很快的被找到。2）而且稀疏的特征所消耗的内存小，使得方法能够在大规模问题上应用。3）而且基于这种方法的keypoint detector一般都会考虑low-level的图像信息，比如说corners。从而局部特征可以被精确的在图像中定位，这对于很多后续任务比如说3D reconstruction来说是很重要的。

稀疏局部特征方法在很多图像条件下都得到了很好的使用。但是他们在极端的图像appearance变化的情况下就不好使了，比如说白天和夜里，或者季节变化，或者很弱纹理的场景。研究表明稀疏局部特征方法在这些情况下不好使的原因是keypoint detector检测到的keypoint的repeatability很差：因为按照上述描述的detector-then-describe流程，local descriptor会考虑keypoint和周围点的一块较大范围内的信息，从而潜在的会encode更high-level的结构，但keypoint detector只会考虑一个keypoint点的信息，范围太小。从而，这些keypoints的detections在appearance变化很大的时候就不稳定了。这是因为low-level信息会被图像里的low-level statistics影响更大，比如说像素点的intensity。

但是不管怎样，实验表明即使keypoints detection不是那么的稳定，local descriptors仍然可以被较好的匹配。因此，那些放弃了detection stage转而使用dense descriptors（也就是每个点都学习一个feature descriptor）并进行dense匹配的方法在复杂的条件下效果更好。当然，这种dense的方法会导致计算量增大。

在这篇文章里，作者旨在同时做好这两件事情，也就是，找到在复杂条件下也能有很好效果的稀疏的特征集合，而且设计高效的匹配算法。为了达到这个目标，作者提出了一个describe-and-detect的方法来进行稀疏局部特征detection和description：和之前先利用low-level信息进行feature detection的方法不同，这个方法将detection stage放在了后面。该方法首先利用CNN计算feature maps，再利用这些feature maps来计算descriptors（在每个特定的像素点，直接取所有通道的值构成的向量为这个像素点的feature description）和检测keypoints（在feature maps上找到局部最大值）。通过这样操作，feature detector和feature descriptor就紧密相连了。由detector检测到的keypoints就会是那些匹配效果很好的descriptors了。同时，使用CNN深层输出的feature maps使得我们能够基于high-level的信息来计算feature detection和feature description。实验表明这种方法和dense方法相比所需要的内存要小得多，同时其也比之前的detect-then-describe方法对于复杂条件下的images的匹配效果要好得多，和dense方法效果差不多甚至更好。如fig 1所示。

![performance]({{ '/assets/images/D2-NET-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. 通过D2-Net方法获得的匹配结果的例子。文章所提出的方法对于复杂图像条件下的匹配效果仍然很好，比如说描述风格的变化（上），因为移动导致的图片模糊（中）以及昼夜变化（下）*


当然，这篇文章的方法也是有缺点的：和传统的detection-then-describe的稀疏局部特征方法相比，文章的方法效率不高，因为需要dense descriptors。而且基于high-level的detection可以获得稳定的输出但却获得不了精确的定位，当然文中的方法对于视觉定位等后续任务来说已经足够精确了。


**Related work**

*Local features*

稀疏局部特征方法最常见的操作是，先进行feature detection，再在以每个keypoint为中心的邻域构成的patch上进行feature description。keypoint detector应该要提供对于scale，rotation或者viewpoint变化都鲁棒或不变的keypoints。为了效率考虑，feature detector一般只考虑局部的图像区域，而且会集中于low-level的图像结构，比如说corners，blobs等。然后descriptor通过考虑keypoints周围的邻域构成的patch来在更high-level的信息上进行feature description。代表文章有：[Benchmarking 6DoF outdoor visual localization in changing conditions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sattler_Benchmarking_6DOF_Outdoor_CVPR_2018_paper.pdf)，[InLoC: Indoor visual localization with dense matching and view synthesis](https://openaccess.thecvf.com/content_cvpr_2018/papers/Taira_InLoc_Indoor_Visual_CVPR_2018_paper.pdf)，[Evaluating Local Features for Day-Night Matching](https://zhhoper.github.io/paper/zhou_ECCVW_2016.pdf)。相反的是，这篇文章的方法使用的是只有一个网络的describe-and-detect方法来学习稀疏特征。从而，这篇文章的方法能够得到那些high-level结构所对应的keypoints，以及局部特殊的descriptors。和这篇文章最接近的工作是SuperPoint（[SuperPoint: Self-Supervised Interest Point Detection and Description](https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.pdf)），其也是detection和description共用同一个深度representation。但是它们的detector和descriptor用了不同的decoder branch而且使用了不同的loss来分别训练。但是这篇文章的方法在detector和descriptor之间共用所有的参数，并使用一个联合的公式来同时对这两个任务进行优化。

>这一段提到的很多基于稀疏局部特征的方法都是十分早年的文章（2012年以前），那时候deep learning都还没火起来，所以就不写出来了，参考价值不大。只将最近两年的文章列了出来。

*Dense descriptor extraction and matching*

上述detect-then-describe方法的一个替代方法就是完全不考虑detection stage，而直接在整张图片上进行dense description（每个像素点都计算一个feature description）。代表文章有：[Universal Correspondense Network](https://proceedings.neurips.cc/paper/2016/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf)，[Hierarchical Metric Learning and Matching for 2D and 3D Correspondences](https://openaccess.thecvf.com/content_ECCV_2018/papers/Mohammed_Fathy_Hierarchical_Metric_Learning_ECCV_2018_paper.pdf)，[Matching neural paths: transfer from recognition to correspondence search](https://proceedings.neurips.cc/paper/2017/file/a8ecbabae151abacba7dbde04f761c37-Paper.pdf)，[Semantic Visual Localization](https://openaccess.thecvf.com/content_cvpr_2018/papers/Schonberger_Semantic_Visual_Localization_CVPR_2018_paper.pdf)。在实践过程中，这个方法要比基于稀疏特征的方法效果好，特别是在照明条件变化很大的情况下。这表明detection stage在detect-then-describe方法里是薄弱的一环，这也启发了这篇文章的方法。

*Image retrieval*

image retrieval这个任务（[NetVLAD: CNN architecture for weakly supervised place recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)，[End-to-End Learning of Deep Visual Representations for Image Retrieval](https://arxiv.org/pdf/1610.07940.pdf)，[Largescale image retrieval with attentive deep local features](https://openaccess.thecvf.com/content_ICCV_2017/papers/Noh_Large-Scale_Image_Retrieval_ICCV_2017_paper.pdf)，[Finetuning CNN Image Retrieval with No Human Annotation](https://arxiv.org/pdf/1711.02512.pdf)，[Particular object retrieval with integral max-pooling of cnn activations](https://arxiv.org/pdf/1511.05879.pdf%EF%BC%89)，[24/7 Place Recognition by View Synthesis](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Torii_247_Place_Recognition_2015_CVPR_paper.pdf)）同样也会处理在复杂条件下找到图片之间的对应关系这样的问题。有些方法（[NetVLAD: CNN architecture for weakly supervised place recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)，[Largescale image retrieval with attentive deep local features](https://openaccess.thecvf.com/content_ICCV_2017/papers/Noh_Large-Scale_Image_Retrieval_ICCV_2017_paper.pdf)，[Particular object retrieval with integral max-pooling of cnn activations](https://arxiv.org/pdf/1511.05879.pdf%EF%BC%89)，[24/7 Place Recognition by View Synthesis](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Torii_247_Place_Recognition_2015_CVPR_paper.pdf)）先从dense descriptors开始，再将这些descriptors综合起来构建一个image-level的descriptor用来做retrieval。[Largescale image retrieval with attentive deep local features](https://openaccess.thecvf.com/content_ICCV_2017/papers/Noh_Large-Scale_Image_Retrieval_ICCV_2017_paper.pdf)在dense description上面加上一个attention module来进行keypoint selection。然而他们的方法的目的是只输出几个keypoints，从而减小retrieval时的false positive matching率。而本文中的方法则可以输出很多repeatable的keypoints来做到很好的对应。[Particular object retrieval with integral max-pooling of cnn activations](https://arxiv.org/pdf/1511.05879.pdf%EF%BC%89)隐式的以feature maps的global maxima来找到keypoints的集合，然后再将这个keypoints集合送到一个global图像descriptor里去。这篇文章启发了我们通过检测feature maps的local maxima来找到keypoints。

*Object detection*

本文提出的describe-and-detect方法同样也在概念上类似object detection。object detection同样也是从一个dense feature extraction步骤开始，然后再对找到的这些region proposals打分。之后再利用某种non-maximal-suppression的方法找到局部最显著的那些proposals，再对这些proposal进行分类从而判断object的类别。尽管这篇文章的方法和object detection的这种方法概念上很类似，但是它们解决的是完全不一样的问题，他们的方法并不能直接拿来获取图像之间的像素对应关系。


**Joint Detection and Description Pipeline**

和传统的使用了两个stages的pipeline的detect-then-describe方法不一样，这篇文章提出使用dense feature extraction来获得一个同时是detector和descriptor的representation。因为detector和descriptor使用了同样的representation，文章将这个方法叫做D2。方法流程如fig 2所示。

![performance]({{ '/assets/images/D2-NET-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. detect-and-describe (D2) network。用一个feature extraction CNN $$\mathcal F$$来获取图片的feature maps，这个feature maps起到两个作用：(i) 在每个位置$$(i,j)$$将所有通道的值连起来，获取了这个点的local descriptor $$d_{ij}$$；(ii) detections通过在feature map上使用一个non-local-maximum suppression，再在每个descriptor间使用一个non-maximum suppression来获得。keypoint detection score $$s_{ij}$$是通过一个soft local maximum score $$\alpha$$和一个每个descriptor的ratio-to-maximum score $$\beta$$计算得来的。*

第一步是在输入图片$$I$$上使用一个CNN $$\mathcal F$$来获取一个3D的tensor $$F = \mathcal F (I), F \in R^{h \times w \times n}$$，其中$$h \times w$$是feature maps的resolution，而$$n$$是通道数。

*Feature Description*

关于这个3D tensor $$F$$的最直观的解释就是一个descriptor向量$$\pmb{d}$$的dense集合：

$$d_{ij} = F_{ij}, \pmb{d} \in R^n$$

其中$$i=1, \cdots, h, j=1, \cdots, w$$。利用这些descriptors之间的欧式距离可以很容易的计算两张图片之间的像素对应关系。在训练阶段，这些descriptors被训练为不同图片对应的同样的点具有相似的descriptors，即使是图片有很大的appearance变化。在实践中，每个descriptors被除以了它的$$L_2$$ norm来归一化这个descriptor。

*Feature Detection*

对于3D tensor $$F$$的另一个不同的解释就是将它们看成一系列2D responses $$D$$的集合：

$$D^k = F_{::k}, D^k \in R^{h \times w}$$

其中$$k=1,\cdots,n$$。在这个解释里，feature extraction函数$$\mathcal F$$可以被看成$$n$$个不同的feature functions $$\mathcal D^k$$，每个都输出一个2D的feature map，$$D^k$$。再之后，我们利用这些feature maps来选择keypoints的locations，如下所述。

* Hard feature detection

在传统的feature detectors里，比如说DoG，detection map会通过一个空间的non-local-maximum suppression来被稀疏化。但是在这篇文章的设定下，有多个detection maps $$D^k, k=1,\cdots,n$$，在他们中的任何一个上面都可以进行detection。因此，对于一个点$$(i,j)$$，如果它想要成为一个keypoint，则要满足：

$$(i,j)$$ is a keypoint $$\iff D_{ij}^k$$ is a local max in $$D^k$$，而且$$k = argmax_t D{ij}^t$$

也就是说，对于每个像素点$$(i,j)$$，上述设置会让我们找到最显著的detector $$D^k$$（channel selection），然后再验证其在$$D^k$$上是否是一个局部最大值点。

* Soft feature detection

在实践中，上述的hard detection procedure会被softened来满足back propagation。首先我们定义一个soft local-max score：

$$\alpha_{ij}^k = \frac{exp(D_{ij}^k)}{\Sigma_{(i^{'},j^{'}) \in \mathcal N(i,j)} exp(D_{i^{'}j^{'}}^k)}$$

其中$$\mathcal N(i,j)$$是像素点$$(i,j)$$周围包括自己一共9个点构成的集合。

然后我们再来定义soft channel selection，对于每个descriptor计算一个ratio-to-max来模仿每个通道的non-maximum suppression：

$$\beta_{ij}^k = D_{ij}^k / max_t D_{ij}^t$$

为了将上述两个criteria都考虑进来，我们对于所有的feature maps来最大化上述两个scores的积来获得一个简单的score map：

$$\gamma_{ij} = max_k (\alpha_{ij}^k \beta_{ij}^k)$$

最后，点$$(ij)$$的soft detection score $$s_{ij}$$通过一个image-level的归一化来得到：

$$s_{ij} = \gamma_{ij} / \Sigma_{(i^{'}, j^{'})} \gamma_{i^{'}j^{'}}$$


* Multiscale Detection

尽管CNN feature extractor $$\mathcal F$$本身就具有一定的scale invariance的能力，但是其本身对于scale并不是invariant的，而且匹配也会在视角有大的变化的情况下失败。

为了获取对于scale变化鲁棒的features，这篇文章使用了image pyramid（其在hand-crafted local feature detectors里常用）。而image pyramid仅仅在测试的时候使用。

给定一张图片$$I$$，构造一个包含了三种不同resolutions $$\rho = 0.5, 1, 2$$（对应于一般，不变，两倍分辨率的情况）的image pyramid $$I^{\rho}$$，并且在这三个分辨率的图像上都提取feature maps，记为$$F^{\rho}$$。

然后再得到$$\tidle F^{2} = F^{2} + F^{1} + F^{0.5}, \tilde F^{1} = F^{1} + F^{0.5}, \tilde F^{0.5} = F^{0.5}$$，在$$\tilde F^{\rho}$$上提取features。而不同分辨率的图片会得到大小不一样的feature map，从而在加之前使用bilinear interpolation来使得尺寸相同。

在实际操作的时候，对于最小分辨率的图片，标注下来keypoints检测的位置，然后上采样到上一个分辨率的位置上，如果在这个分辨率下keypoint检测的位置不在这个区域里（$$2 \times 2$$的区域），那就忽略这个keypoint。


### 3. [Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters](https://openaccess.thecvf.com/content_ICCV_2019/html/Barroso-Laguna_Key.Net_Keypoint_Detection_by_Handcrafted_and_Learned_CNN_Filters_ICCV_2019_paper.html)

*ICCV 2019*

### 4. [Joint Learning of Semantic Alignment and Object Landmark Detection](https://openaccess.thecvf.com/content_ICCV_2019/html/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.html)

*ICCV 2019*

### 5. [Conditional Link Prediction of Category-Implicit Keypoint Detection](https://openaccess.thecvf.com/content/WACV2021/html/Yi-Ge_Conditional_Link_Prediction_of_Category-Implicit_Keypoint_Detection_WACV_2021_paper.html)

*WACV 2021*


### 6. [Stacked hourglass networks for human pose estimation](https://arxiv.org/pdf/1603.06937.pdf)

*Alejandro Newell, Kaiyu Yang, Jia Deng*

*ECCV 2016*

[CODE](http://www-personal.umich.edu/~alnewell/pose)


![hourglass0]({{ '/assets/images/HOURGLASS-0.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}


**Abstract**

这篇文章为human pose estimation任务介绍了一个新的CNN结构。features在多个尺度上被处理、合并，从而能够最好的获取human body各部分的各种不一样的空间关系。作者展现了重复的bottom-up、top-down模块结合对中间特征的监督处理是如何有效的提高网络的效果的。作者将这样的结构称为stacked hourglass（堆叠的沙漏结构）网络，其有着多个pooling和upsampling操作最终才得到输出结果。FLIC和MPII benchmarks上达到的sota效果表明了所提出的网络的有效性。


**1. Introduction**

理解图片和视频里的human的一个关键步骤是准确的pose estimation。给一张RGB图片，我们希望能够准确的定位human body的重要的keypoints的位置。获取一个人的姿态和肢体关节的信息对于更高层次的任务比如说action recognition来说很有作用，而且其在一些领域比如说human-computer interaction以及animation起到了很基础的作用。

作为一个在CV领域已经定义好了的问题，pose estimation已经受到了研究者们很长时间的关注。一个好的pose estimation系统需要对遮挡以及严重的变形、很少见或者从没见过的姿势、因为光照或者衣服带来的很大的appearance的变化等，也有很好的效果。早期的工作使用复杂的handcrafted features和精心设计的structured predictions来获取好的结果，前者是用来获取良好的局部信息，而后者可以获取良好的全局信息。

而CNN的出现颠覆了传统handcrafted方法的效果。CNN使得非常多的CV任务获得了极大的效果提升。最近的pose estimation系统使用CNN作为他们的框架，替换掉了之前的handcrafted features和graphical models，这样的设计也使得pose estimation的效果在很多benchmarks上获得了极大的提升。

本文沿着上述使用CNN来进行pose estimation这样一个路线，设计了一个新的stacked hourglass network，用来进行human pose estimation。这个网络获取并且合并了图片多个尺度的信息。作者说之所以这个网络叫做hourglass，是因为网络里存在多层pooling+upsampling的操作，看起来就像沙漏。如同很多基于CNN的方法的输出都是pixel-wuse的一样（也就是说输出的分辨率和输出一样），本文的hourglass network将输入pool到很低的分辨率，然后再使用upsampling结合之前多个分辨率尺度的features提高分辨率，如此进行多次，最终输出结果。

文章尝试了多种hourglass网络的设定：重复使用上述提到的pooling+upsampling操作，重复的次数进行了多次尝试。作者还在流行的两个benchmarks，FLIC和MPII上，进行了实验，都获得了比sota的准确率提升好几个百分点的效果。


**2. Related works**

自从DeepPose（[Deeppose: Human pose esitmation via deep neural networks]()）这篇文章的提出以来，human pose estimation的研究重心就从传统的方法向使用deep networks的方法进行转移。DeepPose直接使用CNN来regress keypoints，也就是joints，的$$x,y$$坐标值。而[Joint training of a convolutional network and a graphical model for human pose estimation]()则是输出heatmaps而不是直接输出坐标值来表示joints的位置，而且还同步使用了同一张输入的多个分辨率来增强效果。这篇文章很大程度上基于第二篇文章的工作，研究如何使用多个分辨率的输入来改进效果。

[Joint training of a convolutional network and a graphical model for human pose estimation]()的一个关键特征是它联合使用了CNN和graphical model。他们所用的graphical model学习了joints之间的空间位置关系。而本文将不会使用任何graphical models或者其它的方式来对human body进行显式的建模。

也有一些工作对于pose estimation任务使用successive predictions的方式进行学习。这篇文章也使用了intermediate supervision的方式，所以也使用了successive predictions的思想，但是本文使用的架构和学习方式却是不一样的。

本文所提出的hourglass网络在stacking之前和那些使用CNN进行pose estimation的方法是差不多的。hourglass network和它们的区别在于多个top-down processing（从高分辨率到低分辨率的pooling）和bottom-up processing（从低分辨率到高分辨率的umsampling）模块的堆叠。不进行堆叠的hourglass network（也就是说只有一个模块的hourglass network）也叫做conv-deconv或者encoder-decoder结构。有很多工作使用这组conv-deconv结构的网络来进行其他的任务，比如说semantic segmentation、image generation等。但在本文提出的方法里，并没有使用uppooling或者deconv layers，而是使用简单的最近邻upsampling操作以及skip connections来进行bottom-up操作。


**3. Network Architecture**

**3.1. Hourglass Design**

hourglass网络的设计受启发于获取各个尺度的信息的需求。尽管局部信息对于识别比如faces或者hands等局部物体来说很重要，但最终的pose estimation需要对整个human body有一个全面的理解。人的orientations，四肢的arrangements，以及相邻关节的关系等等，这些信息都需要同一张图片不同尺度的信息才能良好的获得。hourglass网络是一个简单而又有效的设计，其能够将各个尺度的信息综合起来从而输出pixel-wise的预测。

网络必须要有某些机制来有效的处理和合并不同尺度的特征。有些研究者尝试使用独立的不同的网络来处理不同分辨率的图片输入，再将这些输入综合起来处理。而本文并不采用这种方法，本文使用的是一个单一的pipeline，使用skip connections来在各个分辨率下保存空间信息。

hourglass网络的设置如下：卷积层和max pooling层将输入图片的分辨率降低到一个很低的值。在每个max pooling层，在做max pooling之前，输入分叉为两部分，不进入max pooling的那部分会做更多的卷积操作，留着之后使用。最后在达到了最低分辨率之后，网络开始进行upsampling以及结合之前不同分辨率下的features的操作。为了将两个相邻分辨率的特征结合起来，作者对较低分辨率的feature map使用了最近邻upsampling方法，再加上较高分辨率的那个feature map，从而得到输出。hourglass网络的结构是对称的，之前每用max pooling降低分辨率一次，之后就使用upsampling提升分辨率一次，所以最终的分辨率会和输入图片分辨率一样。在之后，再加上两个卷积大小为$$1 \times 1$$的卷积层，从而得到网络最终的输出。网络的最终输出是一系列的heatmaps，每个heatmap表示的是每个keypoint在图片中每个像素点出现的概率值的大小。

上述一个hourglass模块如fig1所示。

![hourglass1]({{ '/assets/images/HOURGLASS-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. 一个hourglass模块的介绍。图中每个box都是fig2里的一个residual模块。*


![hourglass2]({{ '/assets/images/HOURGLASS-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. 左侧：一个residual模块。右侧：intermediate supervision process的一个解释。*


**3.2. Layer Implementation**

上述的hourglass结构里，每个小部分仍然具有改动的空间。不同的改动会造成最后的不同效果。作者使用了几种不同的layer设计，无非是在每个residual module里使用不同大小的卷积核，或者增加$$1 \times 1$$的卷积层。最后作者选定的是，在hourglass模块里只使用$$3 \times 3$$和$$1 \times 1$$的卷积核，而且每个模块都是residual模块。

从而最后网络的设计为，初始输入的图片分辨率为$$256 \times 256$$，为了节约计算成本，作者在将图片输入hourglass模块之前，先通过了一个带有残差链接的$$stride=2$$，$$padding=3$$，卷积核为7的卷积层，和一个max pooling层，从而分辨率变为$$64 \times 64$$，之后在进入上述所说的hourglass模块里。对于hourglass里所有层输出的feature maps，其通道数都是256。


**3.3. Stacked Hourglass with Intermediate Supervision**

本文提出的最终的网络结果是多个hourglass模块堆叠而成，前一个hourglass模块的输出是后一个hourglass模块的输入。这样的结构使得网络可以重新衡量初始的估计值，改进效果。而且使用这种结构，网络中间某个hourglass模块的输入也可以拿来计算loss，具体做法是，对于中间某个hourglass模块的输出，其经过$$1 \times 1$$卷积得到heatmap输出，从而可以拿来被计算loss，而这个heatmaps再经过$$1 \times 1$$的卷积层回到操作之前的通道数，再和这个hourglass的输出加起来，作为下一个hourglass模块的输入。具体过程如fig2右侧所示。

每个hourglass模块的权重并不共享，而且每个hourglass拿来计算的loss使用的是同一种loss以及同一个ground truth，具体由3.4来说明。


**3.4. Training Details**

作者在MPII和FLIC数据集上测试了所提出的方法的效果。注意到，这些数据集里很多图片里不止一个人，但本文的方法不具备检测多人pose estimation的能力，本文的方法是对于多人的情况，只检测最靠近图片中心区域的那个人。所有的输入图片都被resize到$$256 \times 256$$，作者还使用了旋转和scaling这两种图片增强技术，但并没有使用translation，因为这会改变图片中人的位置，而到底哪个人位于最靠近图片中心的位置是很重要的，因为本文在多人情况下只考虑这样的人，translation可能会导致标注错误。

而loss使用的是mse loss，ground truth是对于每个keypoints，设计一个以该keypoint为中心，方差为1的2维Gaussian分布作为这个keypoint的heatmap ground truth。


**4. Results**

**4.1. Evaluation**

evaluation是根据标准的Percentage of Correct Keypoints（PCK）metric来计算的，给定一个normalized的distance，落在ground truth这个distance以内的都算判断正确，而PCK metric计算的是判断正确的keypoints占的比例。对于FLIC数据集，这个distance是利用躯干的大小进行了normalize，而对于MPII数据集，这个distance是根据头的大小进行了normalize。

在FLIC和MPII数据集上的测试结果分别如fig3和fig4所示：

![hourglass3]({{ '/assets/images/HOURGLASS-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 3. 在FLIC上的PCK结果。*

![hourglass4]({{ '/assets/images/HOURGLASS-4.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 4. 在MPII上的PCK结果。*

fig5可视化了在MPII测试集上的效果。

![hourglass5]({{ '/assets/images/HOURGLASS-5.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 5. 在MPII测试集上的可视化结果。*


**4.2. Abalation Experiences**

作者研究了两个主要的结构设计对效果的影响：是否需要多个堆叠的hourglass模块，以及是否需要intermediate supervision。作者还测试了单个hourglass如果设计的更深会不会效果更好。实验结果如fig6所示：

![hourglass6]({{ '/assets/images/HOURGLASS-6.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 6. 各个不同的网络结构设计的准确率随着training iteration的进行的变化图。这个准确率是个平均准确率（将wrist，elbow，knee和ankle检测的准确率求了平均值）。*


而且使用较少的hourglass模块会使得每个hourglass模块承担更重的责任，也就是说，使用较少的hourglass模块，相对于使用较多的hourglass模块来说，对应位置的intermediate prediction会更加准确，如fig7所示：

![hourglass7]({{ '/assets/images/HOURGLASS-7.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 7. 左侧：第二个hourglass的intermediate prediction和第八个hourglass的intermediate prediction之间的对比。右侧：使用不同数量的hourglass网络的intermediate prediction的准确度的对比。*


**5. Further Analysis**

**5.1. Multiple People**

这篇文章并不能解决多人的问题，对于多人的情况，只会考虑最靠近图片中心的那个。如果要看多人的算法，可以去看OpenPose那篇论文。

**5.2. Occlusion**

本文的方法能够检测到被遮挡住的keypoints，但是检测的效果不是那么好，所以这仍然是一个需要被解决的问题。


**6. Conclusion**

本文详细阐述了使用hourglass模块构造成的网络对于human pose estimation任务的效果。intermediate supervision对于好的效果来说是很必要的。虽然说还有一些问题没有解决，但是本文提出的方法对于绝大多数情况下的human pose estimation的效果还是很好的。



### 7. [HRNet: Deep High-Resolution Representation Learning for Human Pose Estimation]()

*Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang*

*CVPR 2019*

[CODE](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch)

**Abstract**

这篇文章关注的是human pose estimation问题，焦点在于学习可靠的高分辨率representations。绝大多数现有的方法从low-resolution的representations里recover high-resolution的representations，而这个low-resolution representations是通过某种high-to-low resolution网络所得到的。但本文使用的网络在整个网络从前到后process的过程中都保留了high-resolution representations。

本文提出的方法从一个high-resolution的网络作为第一个stage开始，逐渐增加high-to-low resolution网络作为后续的stages，并且将这些不同分辨率的网络平行连接起来。


**1. Introduction**

2D human pose estimation一直是CV领域一个基础而又具有挑战性的任务。这个问题的目标是定位图片里human的解剖学keypoints（比如说，elbow，wrist等）或者身体部分。其有着很广泛的应用，包括human action recognition，human-computer interaction，animation等等。这篇文章感兴趣的是单人pose estimation问题，这是很多其他相关任务的基础，包括多人pose estimation问题、video pose estimation and tracking问题等等。

最近的发展证明deep CNN对于这个问题获得了sota的效果。绝大多数现存的方法将输入喂给一个网络，一般都是逐渐降低feature map的分辨率，之后再逐步提高feature map的分辨率，从而输出一个pixel-wise prediction。比如说，Hourglass网络就是利用了一个对称的high-to-low和low-to-high结构实现了这个过程。也有一些工作使用的是deconv来实现low-to-high的操作。

而本文提出了一个新颖的架构，叫做high resolution net（HRNet），其能够在网络推进的过程中一直保存着high resolution representations。这个架构从一个high-resolution的subnetwork出发作为第一个stage，逐渐增加high-to-low subnetworks从而构造出更多的stages，并且将这些多分辨率的subnetworks平行连接起来。并且通过交换并行的不同的分辨率的subnetworks之间的信息，实现了多尺度信息的融合。作者通过架构最后的high resolution representations的输出来估计keypoints。fig1解释了整体架构的样子。

![hrnet1]({{ '/assets/images/HRNET-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. HRNet架构的描述。其由多个并行的high-to-low resolution subnetworks组成，这些subnetworks之间有信息交互从而实现多尺度的信息融合。图片最上面和最左边的横箭头和竖箭头表示的是网络的深度和feature maps的尺度。*

本文提出的网络结构对于pose estimation这个任务来说，比现有的网络结构的优势有两处。（1）首先，本文的方法将high-to-low resolution subnetworks平行连接起来，而不是和其他网络一样顺序连接。因此，本文的方法可以一直保留着high resolution的feature map，而不是像其它方法一样需要从low resolution的feature map利用某种low-to-high的方式（比如upsampling等）来从low resolution representations里恢复high resolution representations，从而可以使得keypoints的定位更加准确。（2）其次，多大多数现存的feature融合方法是直接将high level和low level的features融合起来，而本文提出的方法是利用low-resolution representations来帮助增强high-resolution representations的表现能力，而对于每个high-resolution representation，所使用的low-resolution representation是对应深度的，从而数据的抽象程度应该是差不多的，最后使得high-resolution representataions具有表示human pose的能力。

作者在COCO keypoint detection dataset和MPII Human pose dataset上验证了所提出的方法的效果。并且还在PoseTrack dataset上验证了所提出的方法对于video pose tracking任务的效果。


**2. Related Work**

绝大多数解决单人pose estimation的传统方法使用的是probabilistic graphical model，而这些方法被最近使用deep CNN的方法远远超越了。而现在的基于CNN的方法主要分为两类：直接回归keypoints的locations，或者输出keypoints的heatmaps。

绝大多数输出keypoint heatmap的方法使用的网络包括三部分：一个类似于classification任务里所使用的那种网络降低输入的分辨率，一个主体输出和输入同等分辨率的representations，和一个regressor来输出keypoint的heatmaps。上述网络的第一部分和第二部分使用的是high-to-low和low-to-high的结构，可能也会加上一些多尺度的feature融合或者intermediate supervision的结构（如同Hourglass网络里那样）。

*High-to-low & low-to-high*

high-to-low过程目的是为了生成low-resolution、high-level的representations，low-to-high过程则是要生成high-resolution的representations。这两个过程可能会被重复多次来提升网络的性能。

具有代表性的网络设计包括：（1）对称的high-to-low和low-to-high过程。Hourglass网络是其中的代表作。（2）着重强调high-to-low，以及轻量化的low-to-high过程。high-to-low的网络基于的是ImageNet classification网络（比如说ResNet等），而low-to-high的网络只是简单的几层bilinear-upsampling或者tranpose convolution层。（3）和dilated convolution相结合。一些方法将ResNet或者VGGNet的最后两个stages里的卷积换成了dilated卷积，从而这两个stage能够不减少resolution，然后再加上轻量化的low-to-high网络来进一步提升resolution，而避免了只是用dilated卷积造成的计算量非常大的问题。fig2描述了具有代表性的4种pose estimation网络。

![hrnet2]({{ '/assets/images/HRNET-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. 基于high-to-low和low-to-high过程的几种具有代表性的网络结构。（a）Hourglass网络。（b）Cascaded pyramid网络.（c）SimpleBaseline：使用tranposed convolution来作为low-to-high网络的计算。（d）将upsampling和dilated卷积结合起来使用。在（a）里，high-to-low和low-to-high的过程是对称的。在（b），（c）和（d）里，high-to-low的过程是占很大部分的，使用的是ResNet或者VGGNet的一部分，而low-to-high的过程是轻量化的。在（a）（b）里，虚线表示的是skip connections，用于融合high-to-low和low-to-high过程中同分辨率的feature maps。而在（b）的最后，将low-level和high-level的features都融合起来进行了处理。*

*Multi-scale fusion*

一个最直接的方法就是将不同分辨率的图片喂给不同的网络，再将它们的输出综合起来。Hourglass将high-to-low过程和low-to-high过程中同分辨率的feature maps用skip connections连接了起来，从而将high-to-low的feature maps喂给了对应的low-to-high的某一层。

*Intermediate supervision*

Hourglass使用了这个方法改进了网络的效果。

*Our Approach*

本文提出的方法平行的连接high-to-low的subnetworks，其在网络的进行过程中一直保留了high resolution representations用作最后的heatmap estimation。其通过不断的融合由high-to-low subnetworks产生的representations来生成可靠的high-resolution的representations。本文提出的方法和那些需要单独的low-to-high upsampling以及综合low-level和high-level的representations的方法是不一样的。本文的方法，也并没有使用intermediate heatmap supervision，既能获得准确的keypoints位置，又是计算量很小的。

对于classification和segmentation任务，有着和本文类似的multi-scale networks。本文的方法部分收到它们的启发，但是它们这些方法也显然无法直接用到human pose estimation这个任务上。


**3. Approach**

human pose estimation，或者叫做keypoint detection，目的是从一张大小为$$H \times W \times 3$$的图片$$I$$里检测$$K$$个keypoints或者parts（比如，elbow，wrist等）的位置。sota的方法将这个问题转换为估计$$K$$个大小为$$H^' \times W^'$$的heatmaps，$$\lbrace H_1, H_2, \cdots, H_K \rbrace$$，其中$$H_k$$表示的是第$$k$$个keypoint的位置的probability map。

我们继续沿着主流的pipeline来设计我们的网络，这个主流的pipeline使用的是一个CNN网络，其由三个部分组成：几层stride不等于1的卷积层来减小输入图片的resolution，基于减小的resolution的主干部分（输入的resolution要等于输入的），以及一个regressor用来输出heatmaps。我们的焦点在于设计第二部分，而我们提出的方法叫做HRNet。

**3.1. Sequential Multi-resolution subnetworks**

现存的解决pose estimation的网络是通过将high-to-low resolution的subnetworks顺序连接起来构造的，其中每个subnetwork就代表一个stage，其由几层卷积层和最后一个downsampleing层构成，从而每个stage的输出的分辨率就是输出分辨率的$$1/4$$。

我们用$$\mathcal N_{sr}$$来表示这是第$$s$$个stage，而$$r$$表示的是分辨率index，也就是，这个stage的输入分辨率是第一个stage的输入分辨率的$$\frac{1}{2^{r-1}}$$。从而有$$S$$个stage（比如说4个）的high-to-low的网络就可以如下表示：

$$\mathcal N_{11} \rightarrow \mathcal N_{22} \rightarrow \mathcal N_{33} \rightarrow \mathcal N_{44}$$

**3.2. Parallel multi-resolution subnetworks**

我们从一个high resolution subnetwork开始，逐渐增加high-to-low resolution subnetworks构成新的stage，然后将这些multi-resolution的subnetworks平行连接起来。结果就是，下一个stage有好几个subnetworks，有的保留着上一个stage的分辨率，而有的会降低一般分辨率。

包含4个平行的subnetworks的网络结构如下所示：

![hrnet0]({{ '/assets/images/HRNET-0.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
 
**3.3. Repeated multi-scale fusion**

作者提出在平行的subnetworks之间引入exchange units，从而每个subnetwork可以获得其它平行的对应深度的subnetwork的输出。具体操作就是，对于每个stage的并行的subnetwork，其最终的输出（也就是输给下一个stage对应的subnetwork的输入）是这个subnetwork原本的输出，直接加上经过处理后的其它的同一个stage的并行的subnetworks的输出。而具体的处理方式如fig3所示：

![hrnet3]({{ '/assets/images/HRNET-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 3. 展示了三种情况下如何实现的同一个stage里并行的subnetworks的输出进行交互。*

也就是说对于每个stage里的一个subnetwork的原始输出，对于分辨率比它小的那些subnetwork输出，通过一个upsampling模块来增大输出的feature maps的分辨率，再加上去。对于那些分辨率比它大的输出，则采用downsampling层将分辨率降下来，再加上去。原则就是处理后的分辨率要相同，这样才能够加和。本文所用到的upsampling方法是先通过最近邻进行upsampling，再经过一个卷积核为$$1 \times 1$$大小的卷积层。而downsampling的方法是使用stride=2$$的卷积核为$$3 \times 3$$，padding=1的卷积层。

*Heatmap estimation*

本文的方法直接在最后一个stage的high-resolution feature map上加上一个regressor来输出heatmaps。loss定义的是mean squared loss，而ground truth heatmaps是根据所标注的keypoints的位置，生成一个和图片分辨率一样大小的以该keypoint为中心，方差为1的Gaussian分布而得来的。

*Network instantiation*

正如之前所所说的，整个框架分为三个部分：两个stride=2，卷积核为7的卷积层，将输入分辨率从$$256 \times 256$$降到$$64 \times 64$$；主体部分，也就是HRNet；以及一个regressor来输出heatmaps。而主体部分的设计包括了四个stages，一共也有4个平行的subnetworks，也就是正如3.2里第二个图示的那样。每个stage，resolution会被减少到一半，而channels则会翻倍（保证channels和resolution的乘积不变）。


### 8. [Multi-Task Head Pose Estimation in-the-Wild](https://ieeexplore.ieee.org/abstract/document/9303369)

*TPAMI 2021*


### 9. [Joint Learning of Semantic Alignment and Object Landmark Detection](https://openaccess.thecvf.com/content_ICCV_2019/html/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.html)

*ICCV 2019*


### 10. [Self-Supervised Pillar Motion Learning for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Self-Supervised_Pillar_Motion_Learning_for_Autonomous_Driving_CVPR_2021_paper.html)

*CVPR 2021*


### 11. [DGPose: Deep Generative Models for Human Body Analysis](https://link.springer.com/article/10.1007/s11263-020-01306-1)

*IJCV 2020*


### 12. [Test-Time Personalization with a Transformer for Human Pose Estimation](https://proceedings.neurips.cc/paper/2021/hash/1517c8664be296f0d87d9e5fc54fdd60-Abstract.html)

*NeurIPS 2021*


### 13. [On Equivariant and Invariant Learning of Object Landmark Representations](https://openaccess.thecvf.com/content/ICCV2021/html/Cheng_On_Equivariant_and_Invariant_Learning_of_Object_Landmark_Representations_ICCV_2021_paper.html)

*ICCV 2021*


### 14. [Structure-Aware Motion Transfer With Deformable Anchor Model](https://openaccess.thecvf.com/content/CVPR2022/html/Tao_Structure-Aware_Motion_Transfer_With_Deformable_Anchor_Model_CVPR_2022_paper.html)

*CVPR 2022*


### 15. [On Equivariant and Invariant Learning of Object Landmark Representations](https://openaccess.thecvf.com/content/ICCV2021/html/Cheng_On_Equivariant_and_Invariant_Learning_of_Object_Landmark_Representations_ICCV_2021_paper.html)

*ICCV 2021*


### 16. [Structure-Aware Motion Transfer With Deformable Anchor Model](https://openaccess.thecvf.com/content/CVPR2022/html/Tao_Structure-Aware_Motion_Transfer_With_Deformable_Anchor_Model_CVPR_2022_paper.html)

*CVPR 2022*


### 17. [Neural Head Reenactment with Latent Pose Descriptors](https://openaccess.thecvf.com/content_CVPR_2020/html/Burkov_Neural_Head_Reenactment_with_Latent_Pose_Descriptors_CVPR_2020_paper.html)

*CVPR 2020*


### 18. [SK-Net: Deep Learning on Point Cloud via End-to-End Discovery of Spatial Keypoints](https://arxiv.org/pdf/2003.14014.pdf)

*AAAI 2020*


### 19. [Learning Local RGB-to-CAD Correspondences for Object Pose Estimation](https://openaccess.thecvf.com/content_ICCV_2019/html/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.html)

*ICCV 2019*


### 20. [ASK: Adaptively Selecting Key Local Features for RGB-D Scene Recognition](https://ieeexplore.ieee.org/abstract/document/9337174)

*TIP 2021*


### 21. [Efficient Online Multi-Person 2D Pose Tracking with Recurrent Spatio-Temporal Affinity Fields](https://openaccess.thecvf.com/content_CVPR_2019/papers/Raaj_Efficient_Online_Multi-Person_2D_Pose_Tracking_With_Recurrent_Spatio-Temporal_Affinity_CVPR_2019_paper.pdf)

[POST](https://cmu-perceptual-computing-lab.github.io/spatio-temporal-affinity-fields/)

*CVPR 2019 Oral*


### 22. [Learning Individual Styles of Conversational Gesture](https://openaccess.thecvf.com/content_CVPR_2019/papers/Ginosar_Learning_Individual_Styles_of_Conversational_Gesture_CVPR_2019_paper.pdf)

[POST](http://people.eecs.berkeley.edu/~shiry/speech2gesture/)

*CVPR 2019*


### 23. [Transferring Dense Pose to Proximal Animal Classes](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sanakoyeu_Transferring_Dense_Pose_to_Proximal_Animal_Classes_CVPR_2020_paper.pdf)

[POST](https://gdude.de/densepose-evolution/)
[CODE](https://github.com/asanakoy/densepose-evolution)

*CVPR 2020*


### 24. [Causal Discovery in Physical Systems from Videos](https://proceedings.neurips.cc/paper/2020/hash/6822951732be44edf818dc5a97d32ca6-Abstract.html)

*NeurIPS 2020*


### 25. [Learning of Low-Level Feature Keypoints for Accurate and Robust Detection](https://openaccess.thecvf.com/content/WACV2021/html/Suwanwimolkul_Learning_of_Low-Level_Feature_Keypoints_for_Accurate_and_Robust_Detection_WACV_2021_paper.html)

*WACV 2021*


## Unsupervised 2D Keypoint Learning

### 1. [Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)

*NeurIPS 2018*

### 2. [Unsupervised Learning of Object Keypoints for Perception and Control](https://proceedings.neurips.cc/paper/2019/hash/dae3312c4c6c7000a37ecfb7b0aeb0e4-Abstract.html)

*NeurIPS 2019*

### 3. [S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via Multi-View Consistency](https://arxiv.org/pdf/2009.14711.pdf)

*Arxiv 2020*


### 4. [Object landmark discovery through unsupervised adaptation](https://proceedings.neurips.cc/paper/2019/hash/97c99dd2a042908aabc0bafc64ddc028-Abstract.html)

*NeurIPS 2019*


### 5. [Self-supervised Learning of 3D Objects from Natural Images](https://arxiv.org/pdf/1911.08850.pdf)

*Arxiv 2019*


### 6. [Unsupervised Learning of Object Landmarks via Self-Training Correspondence](https://proceedings.neurips.cc/paper/2020/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html)

*NeurIPS 2020*

[CODE](https://github.com/malldimi1/UnsupervisedLandmarks)

Another version of this paper is: [From Keypoints to Object Landmarks via Self-Training Correspondence: A novel approach to Unsupervised Landmark Discovery](https://arxiv.org/pdf/2205.15895.pdf), which has been submitted to TPAMI. 这个版本的论文的代码略有修改，放在了[这里](https://github.com/malldimi1/KeypointsToLandmarks)

**Abstract**

这篇文章对于无监督学习物体关键点提出了一种新的范式。和已有的利用image generation或别的代理方法来进行无监督学习不同，本文提出的是一种自学习的方式，从general的keypoints candidates出发，来逐渐学习改进一个keypoint的detector和descriptor，逐渐将keypoints调整到更精确的位置上。为了达到这个目标，作者提出了一个iterative的算法，其在（1）利用feature clustering来生成假标签；和（2）对于每个假标签，利用对比学习来学习它的features，这两个任务之间反复横跳着更新（alternatively）。本文中所用的detector和descriptor共享一个backbone，这样做的好处是可以使得keypoint的位置能够逐步收敛到稳定的点，将那些不稳定的点filter掉。和先前的工作相比，作者提出的方法可以在物体有很大角度变化的情况下仍然学习到对应的keypoints。作者在很多复杂的数据集上验证了所提出的方法，包括LS3D，BBCPose，Human3.6M，PennAction等，均获得了sota的效果。


**1. Introduction**

Object parts，也被称作landmarks，携带有一个物体在三维空间里的形状和位置信息，特别是对于那些deformable的物体，比如说人脸，身体和手。landmarks因为可以表示具有语义信息的部分的位置，所以经常需要通过手动标记来定位。

Landmark detection的目的是能够训练一个模型，其对于物体能够估计到它的parts或者说landmarks的位置。对这个问题的研究主要集中于使用监督的方法，其需要大量有人为标注的landmarks的图片。part-based detection问题里常用的物体类别有人脸，人的身体等。然而，就像很多其他的CV的问题一样，依赖人为标签来进行模型开发是很费时费力，成本很高的，因此基于无监督学习的方法来解决这个问题就变得比较重要且有意义了。

利用无监督方法来进行object landmarks学习一眼看上去似乎是不可能的。一个人类标注员对于物体以及它的各个部分都有自己的概念理解，而且她/他也能明白由于角度变化、遮挡或者自己遮挡所导致的landmarks的变化。但是相反的是，现有的无监督学习方法经常依赖于某些代理任务，而学习landmarks这样的一个主要任务通常是通过某种latent过程来实现的（比如说[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning]）。还有一些方法使用某些代理任务，比如说equivariance：[Unsupervised learning of landmarks by descriptor vector exchange]，[Unsupervised learning of object frames by dense equivalent image labelling]，[Unsupervised learning of object landmarks by factorized spatial embeddings]，或者image generation：[Unsupervised learning of object landmarks through conditional image generation]，[Object landmark discovery through unsupervised adaption]，[Unsupervised discovery of object landmarks as structural representataions]。基于equivariance原则的方法表明，一个detector必须在经过已知的某种自定义的image deformation下保持连续（也就是说一张图片在经过image deformation前后，这个detector所检测到的keypoints要保证correspondence，其中这个image deformation是已知的），从而他们就利用这个原则来学习模型。而基于image generation的方法的流程是使用一个generator来reconstruct原输入图片，这个generator的输入是原图片经过了deformation之后的图片，而且这个generator是依赖于某个detector的输出的。detector和generator通过一个bottleneck来交换信息可以从输入里蒸馏出物体的几何信息，因为一个generator如果想从根据一张图片经过deformation之后的图片作为输入还能够reconstruct原输入照片，那它就需要对图片里物体的几何特征以及landmarks有足够的理解（也就是说，输入图片经过了某种deformation之后传给了generator，然后希望generator能够恢复原输入，这个时候generator所拿到的图片里的object的几何特征变了，但纹理特征没有，也就是说它需要找到deformation前后landmarks的对应关系来学习到这个deformation对于几何特征造成了什么样的影响， 才能够reconstruct原输入）。 

尽管这些方法在某些场景下取得了好的效果，但这些场景下物体一般都只有比较小的角度变化（比如说正面的人脸，人的身体，猫脸等），这些方法在以下两个角度上具有局限性。首先，代理任务并不能确保detector就能够显式的学习到物体的landmarks，因此会产生那些不太会被人类标注员所标注的landmarks。其次，这些方法都需要人为生成deformations，因为只有这样才能够产生一对图片，其landmarks之间的对应关系是知道的（deformation前后）。但是通过这样一对图片来学习（一张图片以及经过了deformation的这张图片）会导致模型对于更复杂的类内variation不具有鲁棒性，比如说背景变化、角度变化（经过了3D rotation）、或者说那些articulated的物体（比如说人的身体）。

在这篇文章里，作者观察到，尽管landmark detectors在无监督的方式下很难被训练，generic keypoint detectors却很容易获取。基于这个事实，作者提出了一个新的方法来从generic keypoint detector来获取landmark detector。generic keypoints，也可以叫做salient或者interest points，就是一张图片里那些”发生了某种事情的“地方（也就是说这个地方在appearance上发生了变化）以及边缘所在的地方。本文也并不使用坐标点来表示keypoints，而是使用pixel-based的descriptors来表示keypoints，这样做的好处是可以更方便的找到两张图片里相对应的keypoints。generic keypoints可以直接用Sobel filters（比如SIFT）或者预训练好的模型直接得到（比如SuperPoint）。

基于keypoints和landmarks之间的相似性和不同，这篇文章的目的是将一系列预先给定的generic keypoints变换为语意连贯的能够表示object parts的landmarks，在训练的过程中对keypoints进行filter以及refine。为了达到这个目标，作者提出了一个新的方法来（1）不再使用代理任务而是使用self-training的方法来学习landmarks；（2）以及可以从随机的图片对里获取类内物体图片的variations。

作者所提出的方法的出发点是一个具有generic keypoints标注的某类物体的数据集，而且这些keypoints里包括那些能够表示物体part的语义信息的landmarks，而方法主要要做的就是filter以及refine这些keypoints。从这出发，作者的目标是提出一个自训练的方法来使用一种完全无监督的方式学习一个landmark detector。这篇文章所提出的方法和SuperPoint（[]）类似，都是具有一个detector head、一个descriptor head和共用的backbone，通过自训练的方式来iteratively定位keypoints而且给这些keypoints学习具有correspondence的descriptors。作者的目标是将一个keypoint detector转变为一个landmark detector，这些landmark具有语义信息。为了达成这个目标，作者的方法在（1）使用descriptor clustering进行keypoints伪标签以及keypoints correspondence的学习（2）以及使用伪标签进行自学习来训练模型，这两个任务之间alternately反复横跳。

作者发现，和先前的方法相比，本文的方法能够学习到那些具有很大的3D角度变换的landmarks，这一点从fig1就能看出来。作者还在一系列数据集上验证了所提出方法的有效性。

![unsuper]({{ '/assets/images/UNSUPER-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. 和之前那些方法都不同，本文提出的方法能够解决物体在进行很大角度变换之后的landmarks的对应关系，而且能解决对称的问题（也就是对称的物体的两个对称的landmark也是不同的）。*

这篇文章是对之前这个团队的那片NeurIPS文章的扩充，方法上和实验上都有很多补充。而且还有个很大的不同是，在NeurIPS那篇文章里，最终所检测到的landmarks的数量也是算法所需要优化的一个变量，而在这里，将采用和其它论文一样的操作，将其作为一个固定的超参数，其是通过使用一个two-way K-means算法实现的（将在3.6里说）。而且，在NeurIPS那篇文章里，negative pair selection的过程是选取两张图片里不同cluster标签的keypoints，这可能会导致两张图片里因为物体角度不同所导致的具有correspondence的keypoints被认为是negative pair（因为此时这两个keypoints也具有不同的cluster标签，需要后续的cluster融合才能合在一起），所以在这篇文章里，为了避免这个情况，这篇文章里的negative pair的选取只选取同一张图片里的两个具有不同cluster标签的keypoints。最后，和NeurIPS那篇文章里直接使用一开始的detector的结果来初始化descriptors不同，这里采用了某种warm up的方法，效果会更好。作者还在NeurIPS那篇文章里没有用到的很多数据集上进行了测试以及ablation study来证明所提出的方法的有效性和必要性。

总的来说，这篇文章的贡献可以总结如下：
* 作者提出了一个使用无监督方法来学习物体landmarks的新的观点：不再使用代理任务，而且使用自学习的方式，给初始化输入的generic keypoints都学习descriptors从而来filter以及refine它们来得到最终的landmarks;
* 据作者所知，本篇文章的方法，也就是在（1）利用feature clustering进行伪标签以及correspondence的学习；（2）以及利用伪标签进行feature的学习，这两个任务之间alternatively学习的方式，是用无监督方法通过伪标签进行landmarks学习的首创；
* 和之前的方法相比，作者提出的方法可以解决物体有很大角度变化时的landmark的标注，这得归功于descriptor feature space的over-parameterisation，因为这样可以使得一个descriptors能够包含几种角度下的同一个landmarks的features；
* 最后，在各种数据集上的大量实验以及ablation study证明了所提出方法的有效性。


**2. Related Work**

*2.1 Landmark discovery*

*2.2 Keypoint detection*

*2.3 Self-training via clustering*


**3. Method**

这一节将介绍作者提出的方法的各个部分。

**3.1. Problem statement**

$$\mathcal{X} = \lbrace x \in \mathbb{R}^{W \times H \times 3} \rbrace$$是某个物体种类的$$N$$张图片的集合（比如说人脸、身体等）。在$$\mathcal{X}$$上运行一个generic keypoint detector（可以是SIFT，或者别的预训练好的model比如说SuperPoint，等），现在训练集合$$\mathcal{X}$$就变成了$$\lbrace x_j, \lbrace p_i^j \rbrace_{i=1}^{N_k} \rbrace$$，其中$$p_i^j \in \mathbb{R}^2$$是一个keypoint二维坐标，$$N_j$$是图片$$x_j$$里所检测到的keypoints的数量。这些$$p_i$$是没有任何顺序的，而且不同图片之间的$$p_i$$也是没有correspondence的。而且，有一些keypoints可能是outliers，这是由于背景导致的。我们的最终目标是要学习一个heatmap 









### 7. [JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting](https://arxiv.org/pdf/2106.09679.pdf)

*Arxiv 2021*


### 8. [LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity](https://openaccess.thecvf.com/content/WACV2022/html/Karmali_LEAD_Self-Supervised_Landmark_Estimation_by_Aligning_Distributions_of_Feature_Similarity_WACV_2022_paper.html)

*WACV 2022*


### 9. [Skeleton Merger: An Unsupervised Aligned Keypoint Detector](https://openaccess.thecvf.com/content/CVPR2021/html/Shi_Skeleton_Merger_An_Unsupervised_Aligned_Keypoint_Detector_CVPR_2021_paper.html)

*CVPR 2021*


### 10. [Unsupervised discovery of object landmarks as structural representation](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)

*CVPR 2018*

[POST](https://www.ytzhang.net/projects/lmdis-rep/)


### 11. [Unsupervised Discovery of Object Landmarks as Structural Representataions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)

*Yuting Zhang, Yijie Guo, Yinxin Jin, Yijun Luo, Zhiyuan He, Honglak Lee*

*CVPR 2018*

[POST](https://www.ytzhang.net/projects/lmdis-rep/)


**Abstract**

深度神经网络可以使用很丰富的latent representations来表示图片，但它们一般做不到直接表示图片里物体的结构信息。这篇文章通过无监督的方式来学习图片里物体的结构信息。作者提出了一个autoencoding的框架来将检测到的landmarks显式的表示为structural representations。encoding模块输出的是landmarks坐标，其加了限制从而能够输出有效的landmarks信息；而decoding模块以这些landmarks作为输入重构输入图片，从而形成一种end-to-end的结构来学习这些landmarks。这种方式所检测到的landmarks是semantically meaningful的，和人工标注的landmarks更为接近。文章所提出的方法所检测到的landmarks同时也是那些预训练模型所得到的图片features的补充。而且，文章所提出的方法自然的构建了一个无监督的界面，可以让用户通过控制这些landmarks从而改变物体的形状。


**1. Introduction**

CV致力于理解那些能够反映物体物理特性并且和物体appearance无关的物体结构信息。这样的固有结构信息可以作为high-level的visual understanding的intermediate representations。然而，对于物体结构的人工标注或者设计（比如，skeleton，semantic parts）非常的耗时耗力，而且对于绝大多数的物体种类来说是不现实的，从而如何利用无监督的方式自动解决探索object structure的问题就成了十分吸引人的命题。

现在的神经网络可以通过学习latent representations来高效的解决很多不同的视觉任务，包括image classification，segmentation，object detection，human pose estimation，3D reconstruction，image generation等等。一些已有的研究工作表名这些representations自然编码了很多视觉特征信息。然而，很少有证据表明深度神经网络可以自然表示出一类物体的固有的结构信息。

我们的目标在于使用非监督的方式学习物体的结构信息。作为一种表示物体结构的固有的一种表示形式，landmarks能够表示同一种类但是不同个体的物体稳定的局部语义信息。[Unsupervised learning of object landmarks by factorized spatial embeddings]()提出了一个无监督学习的方式在突破可能有transformations的情况下利用CNN来检测稳定的landmarks表示物体的局部语义信息。然而，这个方法并没有显式的鼓励这些landmarks出现在那些可以用作image modeling的位置上。

这篇文章以一种generic的image modeling过程来解决了landmark检测的问题。具体来说，这篇文章提出的方法利用landmark检测作为image autoencoding的一个中间步骤。为了充分利用基于landmark的image decoder里的信息，梯度需要经过landmark coordinates，这就使得[]()文章里的non-differentiable的设置变得不可行了。这篇文章采用了另一种方法来计算landmark coordinates，从而使得整个encoder-decoder过程是端到端而且是完全differentiable的，从而decoder模块的信息就可以通过image reconstruction使得encoder模块所学习到的landmark更具有结构化信息。作者同时还引入了额外的正则项来迫使检测到的landmarks具有想要的性质，并且避免了landmark coordinates会编码无关的或者冗余的latent信息。

这篇文章的贡献总结如下：
* 作者提出了一个differentiable的autoencoder架构来做object landmark discovery的任务，这个框架可以使得decoder里关于image reconstruction的信息可以反向传播到进行landmark coordinates discovery的encoder里去。作者介绍了几种限制条件来使得landmarks具有想要的特性，迫使encoder所检测到的representations是合理的landmarks。
* 文章所提出的方法可以在无监督的情况下对于很多的object种类自动检测视觉信息丰富的landmarks，而且检测效果和利用大规模人工标注的landmarks信息进行训练的监督网络的效果差不多。
* 文章所提出的方法检测到的landmarks在识别visual attributes方面展现出了很强的区分效果。
* 所提出的基于landmark的decoder对于可控的image decoding是有用的，比如说object shape manipulation以及structure-conditioned image generation等。


**2. Related Work**


**3. Autoencoding-based landmark discovery**

作者的目标在于将landmarks作为一个显式的视觉内容的表示来自动进行检测。作者提出了一个autoencoder，其将landmark coordiates编码为encoder的输出（3.1里介绍）。因为没有人工标注的landmarks信息作为监督，作者提出了几种限制来鼓励encoder所检测到的landmarks确实表示了图片里的内容以及和人类的认知相符（3.2里介绍）。文章所提出的限制阻止了这些landmark representations会堕落为那些人类无法识别的representations。encoder不仅会输出landmark coordiates，其还会输出每个所检测到的landmark的local latent descriptor（3.3里介绍）。之后再使用这些landmarks和这些local latent descriptors来重建输入的图片（3.4里说）。这一节还会展示整个网络的结构（fig1）以及训练的目标函数（3.5里说）。整个框架就是为了landmark discovery和无监督的image modeling而设计的。

![usr1]({{ '/assets/images/USR-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. 用作无监督landmark discovery任务的autoencoding框架。*


**3.1. Architecture of landmark detector**

作者将landmark localization问题描述为在图片里检测keypoints的任务。具体来说，每个landmark都有一个对应的detector，其通过卷积的方式输出一个detection score map（也就是一个heatmap），其中最大值的位置所对应的就是landmark的位置。在这个框架里，作者使用一个深度神经网络来将输入图片$$\pmb I$$转换为输出是$$K+1$$个通道的detection confidence map $$\pmb D \in \left[0,1\right]^{W \times H \times (K+1)}$$。这个confidence map会检测$$K$$个landmarks，而第$$K+1$$个通道表示的是背景。$$\pmb D$$的分辨率$$W \times H$$可以和输入图片的分辨率一样，或者更小，但是它们需要是同比例的。

受到hourglass网络在human pose estimation任务里的成功的启发，作者提出一个轻量化的hourglass网络，来从输入图片中获取原始的detection score map $$\pmb R$$：

$$\pmb R = hourglass_l (\pmb I; \theta_l) \in \mathbb R^{W \times H \times (K+1)} \tag{1}$$

其中$$\theta_l$$表示的是网络参数。hourglass网络可以让detectors兼顾检测局部特征和利用全局信息。在获得了$$\pmb R$$之后，再在$$\pmb R$$的每个位置都做一次softmax，也就是每个位置沿着所有的通道数做一次softmax（包括表示背景的score map），从而所得到的$$\pmb D$$的每个位置，沿着所有通道的值加起来就是1，$$\pmb D$$里的每个值都在0到1之间：

$$\pmb D_k(u,v) = \frac{exp(\pmb R_k(u,v))}{\Sigma_{k^{'}=1}^{K+1} exp(\pmb R_{k^{'}}(u,v))} \tag{2}$$

其中$$\pmb D_k$$是$$\pmb D$$的第$$k$$个通道。

将$$\pmb D_k$$看作一个加权的map，作者使用加权的均值坐标作为第$$k$$个landmark的坐标，也就是：

$$(x_k, y_k) = \frac{1}{\zeta_k} \sum_{v=1}^H \sum_{u=1}^W (u,v) \cdot \pmb D_k(u,v) \tag{3}$$

其中$$\zeta_k = \sum_{v=1}^H \sum_{u=1}^W \cdot \pmb D_k(u,v)$$是空间归一化因子。这样的设置可以使得从decoder反向传播来的梯度经过landmark的坐标，除非$$\pmb D_k$$所有的权重都在一个点上或者均匀分布，这在概率上是不可能出现的。作者将landmark和landmark detector简记为：

$$\pmb l = \left[x_1, y_1, \cdots, x_K, y_K \right]^T = landmark(\pmb I; \theta_l) \tag{4}$$

fig1上面蓝色部分的左半部分表示的就是landmark detector部分。


**3.2. Visual concept of landmarks**

$$\pmb l$$里的元素应该是所检测到的landmark coordinates，但是到目前为止，并没有限制条件使得它们是landmarks，到现在为止它们只是任意的latent representations。因此，作者提出了下列soft constraints作为正则项来迫使这些所检测到的representations具有landmarks的特性。

*Concentration constraint*

作为一个单一location的detection confidence map，$$\pmb D_k$$的值需要集中于一个局部的区域内。$$\pmb D_k / \zeta_k$$可以被认为是一个2维的概率分布，从而可以计算出其沿着$$x$$轴和$$y$$轴的方差，$$\sigma_{det, u}^2, \sigma_{det, v}^2$$。作者定义了如下的constraint loss来鼓励这两个方差都很小：

$$L_{conc} = 2\pi e (\sigma_{det, u}^2 + \sigma_{det, v}^2)^2 \tag{5}$$

上述的公式5表示的是以公式3计算出来的$$(x_k,y_k)$$为中心，以$$(\sigma_{det,u}^2 + \sigma_{det, v}^2)/2 \cdot \mathbb I$$为协方差矩阵的二维高斯分布的熵的exponential。这个高斯分布是$$\pmb D_k / \zeta_k$$的一个近似，而公式5越小，分布就越集中于中心点的位置，从而符合我们的要求。

这个近似的分布用公式表示则是：

$$\overline{ \pmb{ D_k}}(u,v) = (1 / WH) \mathcal N((u,v); (x_k, y_k), \sigma_{det} \mathcal I) \tag{6}$$

其之后还会被用到。

*Separation constraint*

理想情况下，autoencoder的目标函数可以自动使得$$K$$个landmarks分布在不同的区域内从而很好地完成decoder里的reconstruction任务。然而，由于随机的初始化，由公式3定义的landmark coordinates的计算方法很可能导致landmarks互相靠得很近。这会导致优化过程陷入梯度下降算法无法逃脱的局部最优点。为了解决这个问题，作者提出了一个显式的loss来空间分隔这些landmarks：

$$L_{sep} = \sum_{k \neq k^{'}}^{1, \cdots, K} exp(\frac{-\lVert (x_{k^{'}}, y_{k^{'}}) - (x_k, y_k) \rVert_2^2}{2 \sigma_{sep}^2}) \tag{7}$$

*Equivariance constraint*

一个特定的landmark应该位于一个微信的局部特征的地方（这个局部特征也应该由明确的语义信息）。这需要landmarks对于image transformations来说具有equivariance的特性。更具体来说，如果相对应的视觉信息仍然存在于transformed之后的图片中的话，一个landmark应该要跟着transformation变化而变化（比如说camera或者object的移动）。将coordiante transformation表示为$$g(\cdot, \cdot)$$，从而原image $$\pmb I$$和transformed之后的image $$\pmb I^{'}$$就有着如下的对应关系：$$\pmb I^{'}(u,v) = \pmb I(g(u,v))$$，并且$$\pmb I^{'}$$的landmarks $$\pmb l^{'}$$就可以用针对$$\pmb I$$所学习的landmark detector来获得：$$\pmb l^{'} = \left[ x_1^{'}, y_1^{'}, \cdots, x_K^{'}, y_K^{'} \right] = landmark(\pmb I^{'})$$，也就是说，$$g(x_k^{'}, y_k^{'}) = (x_k ,y_k)$$，从而引入如下的限制：

$$L_{eqv} = \sum_{k=1}^K \lVert g(x_k^{'}, y_k^{'}) - (x_k, y_k) \rVert_2^2 \tag{8}$$

如果$$g$$是已知的话，那么这个loss就是定义好了的。受到[Unsupervised learning of object landmarks by factorized spatial embeddings]()的启发，作者使用一个thin plate spline（TPS）来使用随机参数模拟$$g$$。作者使用随机的translation，rotation以及scaling来为TPS确定global affine component；之后再perturb一系列control points来确定TPS的local component。除了使用[Unsupervised learning of object landmarks by factorized spatial embeddings]()里所提到的传统的TPS control points，作者还使用3.1里所提到的detector检测到的keypoints作为control points和这些control points交替使用。而且，如果训练集合是以视频的形式出现的，可以将dense motion flow用作$$g$$，而下一帧就是$$\pmb I^{'}$$。

*Cross-object correspondence*

这篇文章提出的方法并没有显式的保证不同object instances上所检测到的landmarks的对应关系。landmarks的不同object instances之间的的semantic稳定性主要是依靠这样一个事实确保的：对于CNN来说，由同一个卷积核所获取的视觉特征一般来说都会有semantic相似性。


**3.3. Local latent descriptors**

对于简单的图片，比如说MINIST，landmarks就足够描述物体的形状了。但对于大多数实际的图片来说，landmarks并不足以表示所有的视觉内容，所以就需要额外的latent representations来encode补充信息。一方面，我们不能引入过多的全局信息，因为这样会导致模型不容易学习到landmark的局部信息（因为这样的全局信息就足够decoder完成reconstruction任务了）；而另一方面，我们也需要一定的全局信息来帮助landmarks的定位。为了解决这个trade-off，作者给每个landmark都计算了一个low-dimensional的local descriptor。

一个hourglass网络被用来获取一个feature map $$\pmb F$$，其和detection confidence map $$\pmb D$$的尺寸是一样的：

$$\pmb F = hourglass_f(\pmb I; \theta_f) \in \mathbb R^{W \times H \times S} \tag{9}$$

对于每个landmark来说，作者使用一个average pooling来获取这个landmark的local feature descriptor，其中这个average pooling的权重是由一个中心在这个landmark点的soft mask构成的。具体来说，我们将公式6定义的$$\overline{ \pmb{ D_k}}$$作为这个soft mask（公式6定义的这个分布实际上是detection confidence map的一个高斯分布的近似）。之后，一个可学习的linear operator（实际上就是几层MLP）被加在每个landmark用上述方式所获取的features上从而为每个landmark学习到一个独立的feature descriptor（注意到每个landmark都有一个自己的linear operator，不是共享的）。

>作者认为之前所学习到的feature $$\pmb F$$使得所有的点的features都公用一个空间，而此处对于每个landmark都用一个linear projection将其映射到自己独立的空间里。

具体来说，第$$k$$个landmark的latent descriptor就是：

$$\pmb f_k = \pmb W_k \sum_{v=1}^H \sum_{u=1}^W (\overline{ \pmb{ D_k}}(u,v) \cdot \pmb F(u,v)) \in \mathbb R^{C} \tag{10}$$

其中$$C < S$$。每个landmark独有的这个linear operator使得每个landmark descriptor可以编码独有的信息。公式10也可以被用来获取一个背景的descriptor。使用之前所说的方法利用一个高斯分布来近似背景的confidence map是不合理的，所以直接令$$\overline{ \pmb{ D_{K+1}}} = \pmb D_{K+1} / \zeta_{K+1}$$。注意到$$\pmb f_k$$对于feature map $$\pmb F$$和detection confidence map $$\pmb D_k$$来说都是可微的。

将所有的landmarks的descriptors和背景的descriptor放在一起，我们就有了$$\pmb f = \left[ \pmb f_1, \pmb f_2, \cdots, \pmb f_{K+1} \right] \in \mathbb R^{C \times (K+1)}$$。fig1的下半部分的左边部分表示了获取这个landmark descriptors $$\pmb f$$的过程。


**3.4. Landmark-based decoder**

作者先从所检测到的landmark coordinates恢复detection confidence map $$\tilde{ \pmb{ D}} \in \mathbb R^{W \times H \times (K+1)}$$。具体来说，作者使用一个二维高斯分布来表示每个landmark的confidence map，其中心点是每个landmark的coordinates，其协方差矩阵是$$\sigma_{det}^2 \mathbb I$$，其中$$\sigma_{dec}$$不是之前计算出来的，是一个超参数。

$$\tilde{ \pmb{R_k}}(u,v) = \mathcal N((u,v); (x_k, y_k), \sigma_{dec}^2 \mathbb I), k=1,2,\cdots,K, \tilde{ \pmb{ R_{K+1}}} = \pmb 1 \tag{11}$$

然后再从$$\tilde{ \pmb{ R}}$$中，对于每个位置，沿着所有的通道做一次归一化，从而恢复detection score map $$\tilde{ \pmb{ D}}$$：

$$\tilde{ \pmb{ D_k}}(u,v) = \tilde{ \pmb{ R_k}}(u,v) / \sum_{k=1}^{K+1} \tilde{ \pmb{ R_k}}(u,v) \tag{12}$$

fig1的上半部分的右半部分描述了这个过程。

对于每个landmark的descriptor $$f_k$$（也包括背景的），作者对于每个landmark来说都利用一个landmark-specific linear operator $$\tilde W_k$$将这个descriptor再次进行线性变化，之后加上一个activation function（文中使用的是LeakyReLU），试图将其再次转换到所有feature公用的空间里（也就是之前$$\pmb F$$所代表的空间）。使用$$\tilde D$$作为global unpooling的权重，feature map的恢复过程如下：

$$\tilde {\pmb {F}}(u,v) = \sum_{k=1}^{K+1} \tilde{ \pmb{ D_k}}(u,v) \cdot \tau(\tilde{ \pmb{ W_k}} \pmb f_k) \in \mathbb R^{W \times H \times S} \tag{13}$$

其中$$\tau$$就是activation function。这个过程由fig1下半部分的右半部分所描述。

本文里的方法使得反向传播算法可以通过landmark coordinates进行传播。每个landmark对应的高斯分布的方差$$\sigma_{dec}^2$$决定其周围的邻居可以给这个landmark贡献多少信息。在训练一开始的时候，需要比较大的$$\sigma_{dec}^2$$来使得训练可行，也就是说每个landmark要依靠的邻居点还很多，而随着训练的进行，需要更准确的定位，也就是需要比较小的方差。对于这样两个相互矛盾的需求，作者同时使用了拥有不同$$\sigma_{dec}$$值来获取多个版本的$$\tilde{ \pmb{ D}}$$和$$\tilde{ \pmb{ F}}$$：$$(\tilde{ \pmb{ D_1}}, \tilde{ \pmb{ F_1}}), (\tilde{ \pmb{ D_2}}, \tilde{ \pmb{ F_2}}), \cdots, (\tilde{ \pmb{ D_M}}, \tilde{ \pmb{ D_M}})$$。

最后，将上述这些$$(\tilde{ \pmb{ D_1}}, \tilde{ \pmb{ F_1}}), (\tilde{ \pmb{ D_2}}, \tilde{ \pmb{ F_2}}), \cdots, (\tilde{ \pmb{ D_M}}, \tilde{ \pmb{ D_M}})$$沿着通道这个维度连起来，再输入一个hourglass网络里，最终获取重建的图片：

$$\tilde{ \pmb{ I}} = hourglass_d (\left[\tilde{ \pmb{ D_1}}, \tilde{ \pmb{ F_1}}), (\tilde{ \pmb{ D_2}}, \tilde{ \pmb{ F_2}}), \cdots, (\tilde{ \pmb{ D_M}}, \tilde{ \pmb{ D_M}} \right]; \theta_d) \tag{14}$$

fig1里的最右边的灰色线表示了这个过程。


**3.5. Overall training objective**

图片reconstruction loss驱动了整个training的进行，定义这个loss为$$L_{recon} = \lVert \pmb I - \tilde{ \pmb{ I}} \rVert_F^2$$，而且输入的图片$$\pmb I$$是被归一化到0到1之间的。从而，整个网络的loss，$$L_{AE}$$定义为：

$$L_{AE} = \lambda_{recon} L_{recon} + \lambda_{conc} L_{conc} + \lambda_{sep} L_{sep} + \lambda_{eqv} L_{eqv} \tag{15}$$


**4. Experiments**

作者在很多数据集上验证了所提出的方法，对于human faces来说有CelebA，AFLW；cat head数据集；从PASCAL 3D获得的一个car数据集；UT Zappos50k获得的一个shoe数据集；Human3.6M获得的human pose数据集；MINIST数据集；AwA获得的animal数据集。


### 12. [Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)


*NeurIPS 2018*


### 13. [Unsupervised Part-Based Disentangling of Object Shape and Appearance](https://openaccess.thecvf.com/content_CVPR_2019/html/Lorenz_Unsupervised_Part-Based_Disentangling_of_Object_Shape_and_Appearance_CVPR_2019_paper.html)


*CVPR 2019*


### 14. [Unsupervised Learning of Object Keypoints for Perception and Control](https://proceedings.neurips.cc/paper/2019/hash/dae3312c4c6c7000a37ecfb7b0aeb0e4-Abstract.html)

*NeurIPS 2019*


### 15. [Unsupervised learning of object structure and dynamics from videos](https://proceedings.neurips.cc/paper/2019/hash/d82c8d1619ad8176d665453cfb2e55f0-Abstract.html)

*NeurIPS 2019*


### 16. [TransGaGa: Geometry-Aware Unsupervised Image-To-Image Translation](https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_TransGaGa_Geometry-Aware_Unsupervised_Image-To-Image_Translation_CVPR_2019_paper.html)

*CVPR 2019*


### 17. [Unsupervised Learning of Landmarks by Descriptor Vector Exchange](https://openaccess.thecvf.com/content_ICCV_2019/html/Thewlis_Unsupervised_Learning_of_Landmarks_by_Descriptor_Vector_Exchange_ICCV_2019_paper.html)

*ICCV 2019*


### 18. [Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos](https://openaccess.thecvf.com/content_CVPR_2020/html/Jakab_Self-Supervised_Learning_of_Interpretable_Keypoints_From_Unlabelled_Videos_CVPR_2020_paper.html)

*CVPR 2020*


### 19. [Neural Head Reenactment with Latent Pose Descriptors](https://openaccess.thecvf.com/content_CVPR_2020/html/Burkov_Neural_Head_Reenactment_with_Latent_Pose_Descriptors_CVPR_2020_paper.html)

*CVPR 2020*

### 20. [Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction](https://proceedings.neurips.cc/paper/2019/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html)

*NeurIPS 2019*


### 21. [Slim DensePose: Thrifty Learning From Sparse Annotations and Motion Cues](https://openaccess.thecvf.com/content_CVPR_2019/html/Neverova_Slim_DensePose_Thrifty_Learning_From_Sparse_Annotations_and_Motion_Cues_CVPR_2019_paper.html)

*CVPR 2019*


### 22. [Self-Supervised Viewpoint Learning From Image Collections](https://openaccess.thecvf.com/content_CVPR_2020/html/Mustikovela_Self-Supervised_Viewpoint_Learning_From_Image_Collections_CVPR_2020_paper.html)

*CVPR 2020*


### 23. [Unsupervised Learning of Intrinsic Structural Representation Points](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Unsupervised_Learning_of_Intrinsic_Structural_Representation_Points_CVPR_2020_paper.html)

*CVPR 2020*


### 24. [Unsupervised Human Pose Estimation Through Transforming Shape Templates](https://openaccess.thecvf.com/content/CVPR2021/html/Schmidtke_Unsupervised_Human_Pose_Estimation_Through_Transforming_Shape_Templates_CVPR_2021_paper.html)

*CVPR 2021*


[POST](https://infantmotion.github.io)


**Abstract**

人的姿态估计一直都是CV领域的一个重要问题，其应用横跨虚拟现实、安全监控、动态追踪等等。在医疗领域，动态追踪对于婴儿监控是非常重要的。尽管已经有了很多算法被提出来解决这个问题，但其都需要大量有标注的数据集来训练。在这篇文章里，作者提出了一个新的方法来为成人和婴儿学习一个姿态估计的网络。作者将这个问题转换为一个利用深度网络所得到的features的template匹配问题。通过transforming一个由2D高斯分布而组成的人体模板，文章的方法可以来估计2D keypoints。通过加入connectivity prior的限制条件，文章的模型可以生成有意义的keypoints。作者在两个数据集上，成人和婴儿数据集，验证了方法的效果。


**1. Introduction**

在今天这个数字化的世界里，图片和视频是无穷无尽的无标签但是却有着内部结构的数据。为了更好的利用这样的数据，我们需要能够用无监督学习的方式来学到这些数据里的信息，这是CV和ML领域最重要也是最吸引人的问题之一。

自监督方法已经被证明对于视频来说是一种很好的无监督学习的方法。在视频数据里，一个物体一般会保持其固有的特征分布不变化，而只是变化这些特征的线性组合。

在这篇文章里，作者研究的是人的姿态估计问题。收到一大批应用（比如动作捕捉，视觉监控，机器人控制）的激励，有很多研究者都投入到了这个方向里，而且还手工标注了很多数据集用来进行监督学习。基于标注数据集的监督学习方法目前还是工业界最常用的方法，因为其具有很高的准确性。

然而，制作这样的数据集是非常费时费力的，而且一般也只能针对某个很窄的领域。而某些领域甚至获取不了有标注的图片，比如说医疗领域。

为了解决这个问题，无监督和弱监督姿态估计的方法将图片分解为appearance（个体的独特特征，比如衣服、身高等）和pose（描述个体的空间姿态信息，各个关节之间的位置关系）。在这篇文章里，自监督的任务，比如说image reconstruction或者translation，被用来进行姿态估计。

作者因此集中于学习一个类别的物体的2D几何信息。


### 25. [Object landmark discovery through unsupervised adaptation](https://proceedings.neurips.cc/paper/2019/hash/97c99dd2a042908aabc0bafc64ddc028-Abstract.html)

*NeurIPS 2019*


### 26. [Unsupervised Object Keypoint Learning using Local Spatial Predictability](https://openreview.net/forum?id=GJwMHetHc73)

*ICLR 2021*


### 27. [Semi-supervised Keypoint Localization](https://openreview.net/forum?id=yFJ67zTeI2)

*ICLR 2021*


### 28. [Self-Supervised Keypoint Discovery in Behavioral Videos](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Self-Supervised_Keypoint_Discovery_in_Behavioral_Videos_CVPR_2022_paper.pdf)

*CVPR 2022*


### 29. [Self-Supervised Learning of Class Embeddings from Video](https://openaccess.thecvf.com/content_ICCVW_2019/html/CEFRL/Wiles_Self-Supervised_Learning_of_Class_Embeddings_from_Video_ICCVW_2019_paper.html)

*ICCV 2019*


### 30. [Weakly Supervised Keypoint Discovery](https://arxiv.org/pdf/2109.13423.pdf)

*Arxiv 2021*


### 31. [Pretrained equivariant features improve unsupervised landmark discovery](https://arxiv.org/pdf/2104.02925.pdf)

*Arxiv 2021*


### 32. [Unsupervised Landmark Learning from Unpaired Data](https://arxiv.org/pdf/2007.01053.pdf)

*Arxiv 2020*

[CODE](https://github.com/justimyhxu/ULTRA)


### 33. [Video Interpolation and Prediction with Unsupervised Landmarks](https://arxiv.org/pdf/1909.02749.pdf)

*Arxiv 2019*


### 34. [An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/html/Xie_An_Empirical_Study_of_the_Collapsing_Problem_in_Semi-Supervised_2D_ICCV_2021_paper.html)

*ICCV 2021*


### 35. [Learning Landmarks from Unaligned Data using Image Translation](https://openreview.net/pdf?id=xz3XULBWFE)

*Arxiv 2019*


### 36. [Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/data/unsupervised_pose.pdf)

*CVPR Oral*

[POST](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/)


### 37. [LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity](https://openaccess.thecvf.com/content/WACV2022/html/Karmali_LEAD_Self-Supervised_Landmark_Estimation_by_Aligning_Distributions_of_Feature_Similarity_WACV_2022_paper.html)

*WACV 2022*


### 38. [JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting](https://arxiv.org/pdf/2106.09679.pdf)

*Arxiv 2021*


### 39. [Unsupervised Learning of Facial Landmarks based on Inter-Intra Subject Consistencies](https://arxiv.org/pdf/2004.07936.pdf)

*Arxiv 2020*


### 40. [Unsupervised Landmark Learning from Unpaired Data](https://arxiv.org/pdf/2007.01053.pdf)

*Arxiv 2020*


### 41. [Self-Supervised Learning of Class Embeddings from Video](https://openaccess.thecvf.com/content_ICCVW_2019/html/CEFRL/Wiles_Self-Supervised_Learning_of_Class_Embeddings_from_Video_ICCVW_2019_paper.html)

*ICCV 2019*


### 42. [DeepSFM: Structure From Motion Via Deep Bundle Adjustment](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460222.pdf)

*ECCV 2020*


### 43. [SfM-Net: Learning of Structure and Motion from Video](https://arxiv.org/pdf/1704.07804.pdf)

*Arxiv 2017*


### 44. [LatentKeypointGAN: Controlling GANs via Latent Keypoints](https://xingzhehe.github.io/LatentKeypointGAN/)

*Arxiv 2021*


### 45. [Unsupervised Part Discovery from Contrastive Reconstruction](https://proceedings.neurips.cc/paper/2021/hash/ec8ce6abb3e952a85b8551ba726a1227-Abstract.html)

*NeurIPS 2021*


### 46. [Video Interpolation and Prediction with Unsupervised Landmarks](https://arxiv.org/pdf/1909.02749.pdf)

*Arxiv 2019*


### 47. [An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/html/Xie_An_Empirical_Study_of_the_Collapsing_Problem_in_Semi-Supervised_2D_ICCV_2021_paper.html)

*ICCV 2021*


### 48. [Unsupervised Part Segmentation Through Disentangling Appearance and Shape](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Unsupervised_Part_Segmentation_Through_Disentangling_Appearance_and_Shape_CVPR_2021_paper.html)

*CVPR 2021*


### 49. [Unsupervised learning of object structure and dynamics from videos](https://proceedings.neurips.cc/paper/2019/hash/d82c8d1619ad8176d665453cfb2e55f0-Abstract.html)

*NeurIPS 2019*


### 50. [Unsupervised Part-Based Disentangling of Object Shape and Appearance](https://openaccess.thecvf.com/content_CVPR_2019/html/Lorenz_Unsupervised_Part-Based_Disentangling_of_Object_Shape_and_Appearance_CVPR_2019_paper.html)

*CVPR 2019*


### 51. [Unsupervised Learning of Object Structure and Dynamics from Videos](https://openreview.net/pdf/345dd2a33716f95cef4028f94fbd0a322234f1e0.pdf)

*NeurIPS 2019*


### 52. [Few-shot Keypoint Detection with Uncertainty Learning for Unseen Species](https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Few-Shot_Keypoint_Detection_With_Uncertainty_Learning_for_Unseen_Species_CVPR_2022_paper.pdf)

*CVPR 2022*


### 53. [TACK: Few-Shot Keypoint Detection as Task Adaptation via Latent Embeddings](https://sites.google.com/view/2021-tack)

*Arxiv 2021*


### 54. [ViewNet: Unsupervised Viewpoint Estimation From Conditional Generation](https://openaccess.thecvf.com/content/ICCV2021/html/Mariotti_ViewNet_Unsupervised_Viewpoint_Estimation_From_Conditional_Generation_ICCV_2021_paper.html)

*ICCV 2021*


### 55. [Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)

*ICCV 2017*

**Abstract**

自动学习一个object category的结构信息仍然是CV领域未解决的问题。在这篇文章里，作者提出一个新的无监督的方式来学习一个类别的物体的keypoints，从而表示这个类别物体的结构。作者的方法基于factorizing image deformations，这种deformation可能是由viewpoint变化引起的，或者是因为物体本身的形变引起的。作者提出的方法是认为deformation前后的图片的keypoints具有consistency的性质。作者还表明，用这种方法所学习到的keypoints，即使不需要加上限制条件使得同一个类别的不同图片（也就是不同物体）之间的keypoints具有对应关系，它们就已经自动对应上了。作者对于一系列物体类别都做了实验来验证所提出方法的有效性，这些图片既有natural的，也有实验室生成的。作者还表明了这种方法所学习到的keypoints在人脸数据集上和人工手动标注的keypoints高度一致。


**1. Introduction**

图片里物体的appearance不仅依赖于物体本身固有的特性比如说shapes，material，其也依赖于一些外在因素，比如说视角，光照等。因此，从图片里学习到物体的固有特性是个非常困难的任务，尤其是使用无监督的方法。

尽管有这些困难，object detection算法的效果发展的越来越好，现在的深度神经网络已经在很多的数据集（包括PASCAL VOC，Microsoft COCO）等上面有了很好的效果。但是，我们仍然不知道这些效果好的object detection算法是否是真的学习到的物体的固有特性。早期的object detectors，包括HOG，DPMs等，基于的是尺度和旋转不变的2D filters。最近的object detectors，比如说SSD，Fast-RCNN等，利用深度神经网络强大的学习能力，学到了物体各个尺度、各个角度的2D filters（也就是网络里的卷积核）。实际上的原理是不变的，只不过手动设计的时代被替换成了利用深度神经网络自动学习的时代，而且filters的种类和特性要更多更强。因此，这些模型更倾向于以一种全局性的图片的特征来获取所探测到的objects的特性，将object表示为一些联系很弱的2D patterns的集合（这些2D patterns就是由2D filters所表示的）。

为了获取一个对物体更深的理解，我们需要对它们的和视角无关的本质的结构信息进行建模。通常来说，这种结构信息会显式的用landmarks，parts或者skeletons来表示。给定足够的人工标注，我们是可以训练出来一个深度神经网络对于输入的图片，给出图片中物体的上述的结构信息的。但是，不使用人工标注信息来使得模型具有从图片中学习到这种结构信息的这个任务仍然是没有被解决的。

在这篇文章里，作者提出了一个新的方法使用无监督学习的方式来从输入图片中学习和视角无关的物体的结构信息，如fig1所示。

![fse1]({{ '/assets/images/FSE-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig1. 作者提出了一个新的方法，可以在没有任何标注的情况下学习视角无关的keypoints。这个方法使用了一个viewpoint factorization的过程，其学习了一个可以进行image deformation的keypoint detector。这个模型可以用来学习rigid或者deformable objects。*

在3.1节描述总体的设计思想。在3.2节表示这个模型使用keypoints作为物体的结构信息表示。而且这个keypoint detector不适用任何的标注信息。在3.3节里描述了具体训练的过程，也就是网络的输入是一对图片，其中一张图片是由另一张图片经过某种已知的transformation得来的，而且我们要求keypoint detector对于这两张图片所学习到的keypoints是等价的（原图片所得到的keypoints也要经过一次同样的transformation）。这样的transformations在真实的情况下是由于视角变化或者物体形变而产生的，但是作者在文中使用的是随机的perturbation，并且说明这样就足够学习到consistent且meaningful的keypoints了（如何设计这种perturbation是这篇文章的重点）。

作者表明所提出的这个方法对于单个的rigid或者deformable object instance都是可行的（3.1.1里说），而且这个方法还可以对于某个类别的objects图片集，学习一个模型来输出这个类别的物体的keypoints（3.1.2里说）。作者说他们的方法不需要任何的限制条件，就可以保证对于同一个类别的包含不同物体instance的不同图片，能学习到具有对应关系的keypoints。

作者在很多物体类别和数据集上验证了所提出的方法的有效性，包括shoe，animals，human faces等。作者也表明这种方法所学习到的keypoints和人工标注的keypoints高度重合。


**2. Related Work**

**3. Method**

3.1介绍了学习一个object category本质的结构信息的viewpoint factorization方法。之后，3.2将其应用到学习object keypoints，以及3.3讨论了训练的具体细节。

**3.1. Structure from viewpoint factorization**

$$S \subset \mathbb R^3$$表示一个物体的3维surface，比如说一只鸟，$$\pmb x: \Lambda \rightarrow \mathbb R$$表示的是这个物体的一张图片，其中$$\Lambda \subset \mathbb R^2$$是image domain，如fig2所示。$$S$$是物体的一个固有的特性，和$$\pmb x$$是无关的。作者考虑的是学习一个函数$$q = \Phi_S(p; \pmb x)$$将$$S$$上的点$$p \in S$$映射到对应的图片$$\pmb x$$上的某个点$$q \in \Lambda$$上去。

![fse2]({{ '/assets/images/FSE-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig2. 物体结构的建模过程。。*

作者提出了一个利用viewpoint factorization来自动学习$$\Phi_S$$的方法。为了实现这个目标，我们考虑从另一个角度来看这个物体从而获得的另一张图片$$\pmb{x^{'}}$$。利用$$g$$来表示由于视角变化导致的图片歪曲，从而$$\pmb{x^{'}} \approx \pmb x \circ g$$。利用$$\Phi_S$$，我们可以将$$g$$$分解为：

$$g = \Phi_S(\dot; \pmb{x^{'}}) \circ \Phi_S(\dot; \pmb x)^{-1} \tag{1}$$

也就是说，我们将这个歪曲$$g: q \rightarrow q^{'}$$分解为先找到图片$$\pmb x$$里的点$$q$$对应在$$S$$上的点$$p=\Phi_S^{-1}(q ;\pmb x)$$，然后再找到这个点$$p$$在图片$$\pmb{x^{'}}$$上对应的点$$q^{'} = \Phi_S(p;\pmb{x^{'}})$$。

公式1表示的factorization也可以用下述等价的约束条件来表示：

$$\forall q \in S: \Phi_S(p; \pmb x \circ g) = g(\Phi_S(p; \pmb x)) \tag{2}$$

这个约束条件表明$$S$$上所检测到的点$$p$$需要随着视角的变化仍然保持一致。

为了学习这个函数$$\Phi_S$$，作者将这个函数利用一个深度神经网络来表示，并采用一种Siamese的结构，输入是$$(\pmb x, \pmb{x^{'}}, g)$$。注意到，如果我们任意给定一个物体的两个视角的图片，那么$$g$$一般是不知道的，所以说与其再利用某种方法来估计$$g$$，不如直接随机的构造出$$g$$，再利用$$g$$从$$\pmb x$$上构造出$$\pmb{x^{'}}$$。

>尽管训练的过程使用的是同一张图片和这张图片经过transformation之后得到的歪曲的图片作为训练图片对，但是训练好的模型仍然能够在同一种类物体的不同图片之间学习到keypoints的对应关系。对于那些角度变化特别大的情况，则不能保证效果，这就是future works了。


**3.1.1 Deformable objects**

上述描述的是rigid物体的建模，也就是说$$S$$是不变的，而其也可以直接拓展到deformable的物体上去。假设$$S$$经过了某些deformation $$w$: \mathbb R^3 \rightarrow \mathbb R^3$$。从而形变后的surface就是$$S_{'} = \lbrace w(p):p \in S \rbrace$$。作者同时还提出了一个共用的surface $$S_0$$，其表示的是这个类别的object的一个标准的surface，也就是说其它的surface都可以从这个surface变形而来。将从$$S_0$$到$$S$$的deformation记为$$\pi_S$$，那么对于$$S_0$$上任意一个点$$r$$，就有$$\pi_S(r) \in S$$，而且$$\forall w: w(\pi_S(r)) = \pi_{S^{'}}(r)$$。然后，对于$$S_0$$上的点$$r$$到图片$$\pmb x$$上的点$$q$$的映射$$\Phi(r, \pmb x)$$，可以用$$\Phi_S$$来表示：$$\Phi(r; \pmb x) = \Phi_S(\pi_S(r); \pmb x)$$。从而公式2就写成：

$$\forall r \in S_0: \Phi(r; \pmb x \circ g) = g(\Phi(r; \pmb x)) \tag{3}$$

这也就是说即使是由形变造成的物体变化，变化前后所检测到的物体的keypoints也是要对应的。


**3.1.2 Object Categories**

除了物体的形变，上述的构造对于同一个类别的不同物体之间的差异也是可以解释的。为了实现这种解释，只需要做出一个假设：同一个种类的所有物体的surfaces对于一个category-specific surface $$S_0$$来说，都是isomorphic的，如fig2所示。

和derformable object的情况不一样，几何性质（也就是之前说的$$g$$）是不足以使得从$$S_0$$映射到不同object $$S$$的映射$$\pi_S$$能够相关联的（因为这个时候$$g$$已经无法简单的定义出来了），从而不同图片的keypoints之间的对应关系就不好建立了。然而，我们还希望这些这些$$\pi_S$$具有semantically consistent的性质，也就是还存在对应关系，比如说$$\pi_S(r)$$表示的是图片$$S$$里右眼的位置，而$$\pi_{S^{'}}(r)$$表示的也是图片$$S^{'}$$里右眼的位置。本文的一个重要的贡献就在于，如果将模型按照3.1里所说的那样进行训练，那么训练好的模型对于一个种类的不同图片，能够自动检测到semantically consistent的keypoints。

**3.2 Landmarks detection networks**

在这一节里，作者具体的举例说明3.1里所描述的方法。首先，我们需要确定如何将从$$S_0$$映射到图片$$\Lambda$$的函数$$\Phi(\dot; \pmb x)$$表示为一个神经网络的输出。作者的方法是在$$K$$个离散的点采样来表示：$$\Phi(\pmb x) = (\Phi(r_1; \pmb x), \cdots, \Phi(r_K; \pmb x))$$。在这样的设定下，函数$$\Phi(\pmb x)$$就可以被理解为检测的是图片$$\pmb x$$的$$K$$个keypoints，$$p_k = \Phi(r_k; \pmb x)$$。作者并没有对这些keypoints加以别的约束。

如果$$\Phi$$是由一个神经网络所表示的，我们可以使用现有的任何一种用于keypoint detection的框架来实现它（比如HRnet，Hourglass等）。绝大多数这种框架都是预测的score maps $$\Psi(\pmb x) \in \mathbb R^{H \times W \times K}$$，对于每个keypoint $$r_k$$和图片的像素点位置$$u \in \lbrace 1, \cdots, H \rbrace \times \lbrace 1, \cdots, W \rbrace \subset \mathbb R^2$$，$$\Psi_{u r_k}(\pmb x)$$表示了一个得分。之后，可以对于每个通道，将每个score map利用softmax进行处理。从而得到了概率分布的score map。再之后，就可以利用求均值的方式求出每个keypoints的位置了。


**3.3. Learning formulation**

在这一节里，作者展示了equivalence constraint，也就是公式3，是如何使得整个模型训练起来的。想法是将整个学习过程表示为一个Siamese结构，也就是说网络同时处理图片$$\pmb x$$，经过已知的$$g$$处理过从而得到的$$\pmb{x^{'}}$$。从而我们得到了一个loss：

$$\mathcal L_{align} = \frac{1}{K} \sum_{r=1}^K \lVert \Phi(\pmb x \circ g) - g(\Phi(\pmb x)) \rVert^2 \tag{4}$$

这一节剩下的部分讨论如何使用keypoint probabilites来表示公式4，以及加上一个新的diversity项会使得训练效果更好。

*Probability maps loss*

公式4用了softmax argmax operator来定位和计算keypoints，而实际上可以不经过这两个运算，而直接在probability maps上操作，简化梯度计算的过程。想法就是利用下面的loss来替代公式4里的$$\mathcal L_{align}$$：

$$\mathcal L_{align}^{'} = \frac{1}{K} \sum_{r=1}^K \sum_{uv} \lVert u - g(v) \rVert^2 p(u \vert \pmb x, r) p(v \vert \pmb{x^{'}}, r) \tag{5}$$

其中$$p(u \vert \pmb x, r)$$表示的是对于图片$$\pmb x$$，网络算出来的第$$r$$个score map在$$u$$位置的值，而$$u$$表示的是二维坐标，而不是一维的。$$p(v \vert \pmb{x^{'}}, r)$$也是同理。

最小化公式5会有两个想要得到的效果。首先，这个loss鼓励这两个probability maps尽可能的重合。其次，其鼓励这两个probability maps尽可能地集中在一个点。

将公式5再进行分解，得到：

$$\sum_u \lVert u \rVert^2 p(u \vert \pmb x, r) + \sum_v \lVert g(v) \rVert^2 p(v \vert \pmb{x^{'}}, r) - 2 (\sum_u u p(u \vert \pmb x, r))^T(\sum_v v p(v \vert \pmb{x^{'}}, r))$$

上述变形可以使得quadratic的计算量变为linear的。


*Diversity loss*

上述的loss会使得网络学习到的keypoints都是consistent的，但并没有阻止网络所学习到的都是同一个点，也就是这些点全都聚集到了一起。

为了避免这个现象，作者加了一个diversity loss来使得这些probability maps对应到图片的不同区域。最直接的方式就是直接对两个probability相互重合的区域增加惩罚：

$$\mathcal L_{div}(\pmb x) = \frac{1}{K^2} \sum_{r=1} \sum_{r^{'}=1} \sum_u p(u \vert \pmb x, r) p(u \vert \pmb x, r^{'})$$

上述公式的一个缺点就是计算量是quadratic的。一个简便的方法是：

$$\mathcal L_{div}^{'} (\pmb x) = \sum_u (\sum_{r=1}^K p(u \vert \pmb x, r) - max_{r=1, \cdots, K} p(u \vert \pmb x, r))$$


**4. Experiments**

**4.1 Implementation details**

对于一张图片$$\pmb x$$和用thin-plate splines（薄板样条插值）TPS实现的transformation $$g_1, g_2$$，网络的输入图片对为：$$g_1 \circ \pmb x$$和$$g_2 \circ (g_1 \circ \pmb x)$$。

TPS参考下面的文献：




## Keypoint matching

### 1. [Multi-Image Semantic Matching by Mining Consistent Features](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.pdf)

*Qianqian Wang, Xiaowei Zhou, Kostas Daniilidis*

*CVPR 2018*

**Abstract**

这篇论文提出了一种多张图片匹配的方法来估计多张图片之间的语义对应关系。和之前需要优化所有的成对的对应关系的方法不一样，我们提出的方法只需要识别并匹配图片集合里一个稀疏的features集合就可以。利用这种方法，我们提出的方法可以去除那些不可重复的features，而且对于上千张图片也是可以操作的。我们还提出了一个low-rank的约束来确保在整个图片集合里的features对应关系具有geometric consistency。我们的方法除了在multi-graph matching和semantic flow任务上表现很好之外，我们还可以用这个方法来重构object-class模型，并且从没有任何标注的图片中找到object-class landmarks。


**1. Introduction**

计算图片之间的feature对应关系在CV领域是个很基础的问题。low-level的geometric features（比如说SIFT）对于相同场景下的图片匹配做得很好。最近，对于semantic matching的兴趣越来越高涨，比如说在不同的object instances或者scenes之间建立semantic对应关系。semantic matching这个领域大多数的研究工作集中在成对的情况下，也就是说每次只考虑一个图片对（两张）。而在多张图片中找到consistent correspondences在很多情况下很重要，比如说object-class model reconstruction，automatic landmark annotation等。这篇文章的关注点就是多张图片的semantic matching任务。

尽管semantic matching和multi-image matching问题已经有了很多的进展（related work），但是下述的问题仍然存在。首先，semantic matching的可重复的feature point的寻找仍然是个未解决的问题。之前的工作通过使用所有的pixels（dense flow）或者随机挑选points来避开这个问题，但是导致的结果就是大量的不可重复的features，它们导致和其它的图片之间建立不了真正的对应关系。其次，之前的multi-image matching方法主要优化的是cycle consistency，而并没有同时考虑geometric consistency。现在已经有了很多方法在pairwise的设定下加入了geometric约束（比如说RANSAC和graph matching），但对于multi-image的设定并没有解决方案。最后，现存的multi-image matching方法计算量很大，对于几百张图片就算不了了。所以分析更大的数据集需要更scalable的算法。

在大多数情况下，我们只需要一些具有cycle consistency和geometric consistency的可以高度重复的features能够对应就可以了，而它们只会占据features集合的一小部分。dense的对应关系可以通过插值的方法来实现。因此，和之前那些优化所有的pairwise对应关系的multi-image matching方法不同的是，我们将问题描述为一个feature选择和labeling的问题：从繁复的pairwise对应关系出发，我们在每张图片的初始candidates集合里选取一些feature points，然后通过给它们labels来构建它们在多张图片之间的对应关系。上述挑选和加上标签的过程是通过共同优化cycle consistency和geometric consistency来实现的。将问题描述成这样的形式可以让我们：1）显式的在初始的feature points集合里解决掉那些不可重复的feature points；2）大量的减少变量的数量，从而设计出一个scable的模型，可以解决上千张图片。受到factorization-based structure from motion的一些经典结果的启发，我们提出一个low-rank的约束来使得multi-image matching具有geometric consistency，而且其有很高效的优化算法。fig 1给了个例子来解释问题描述以及我们所提出的方法。

![eg]({{ '/assets/images/MATCHING-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. 给定多张图片的初始feature candidates和每两张图片之间的带有噪音的features之间的对应关系，我们提出的方法从这些feature points中识别出一部分可靠的，并且为它们在所有的图片间构建cycle consistent和geometric consistent的对应关系。上述figure给了一个通过我们提出的方法从1000张图片中挑选出的可靠的feature points的例子（彩色的十字表示feature points）。十字的颜色代表着其之间的对应关系。上述figure的最后一列显示了一张图片的初始的feature points集合（上面）以及手动标注的landmarks（下面）。有意思的是，用我们的方法（unsupervised的方法）来找到的可靠的feature points和手动标注的landmarks高度相似。*

我们这篇文章的主要贡献如下：

* 我们提出了一个新的方法来解决multi-image semantic matching的问题，我们将其转换为一个feature selection和labeling的问题。我们所提出的算法可以在图片集合中找到consistent的feature points，并且是scable的，可以处理上千张图片。
* 我们为multi-image matching介绍了一个新的low-rank的约束，从而我们的算法可以同时优化cycle-consistency和geometric consistency。
* 我们在标准的benchmarks上阐述了我们的方法的很好的效果。我们同时也展示了两个应用：1）从一系列同类但不同instance的图片出发重建3D object-class models，而且并不需要任何手动标注；2）匹配了1000张猫头的图片并且发现算法自动找到的feature points十分具有代表性，它们和表示eyes，ears，mouth等图片的关键点很重合，所以说我们这个方法在automatic landmark detection这个任务上也是很有潜力的。

**2. Related work**

**Image Matching**

在经典的image matching任务里，图片之间的稀疏的feature对应关系是通过low-level的geometric feature detectors（比如说corners和covariant区域）和descriptors（比如说SIFT，SURF，HOG等）来估计的。geometric consistency是通过使用RANSAC作为一个后续的处理步骤或者通过解决一个最小化图片之间的geometric distortion的graph matching问题来实现的。很多最近的工作尝试在不同的场景下找到semantic对应关系。hierarchical matching和region-based方法已经被用来找到图片里的high-level的semantics。

**Learning detectors and descriptors**

最近的结果表明利用CNN获取的deep features在matching领域很有效果，并且远远超过了手动设计的features，即便CNN不是针对matching任务来训练的也是一样。监督学习被用来显式的学习descriptors。监督学习的标签来自于手动标注的对应关系、图片transformations和一些额外的信息，比如说CAD models等。同时，也有一些研究尝试学习可重复并且对于transformations鲁棒的feature detectors。这些方法可以被看成一种非监督的学习方式，它们从图片集合中学到了可靠并且consistent的对应关系。


**Multi-image matching**

multi-image matching在技术上和joint maching方法相关。大多数现存的方法都是利用cycle-consistency来改进pairwise对应关系。很多方法被提了出来，比如说unclosed cycle elimination，constrained local optimization，spectral relaxation以及convex relaxation。我们所提出的方法和上述都不同，因为我们希望能找到在所有图片上最consistent的features而不是在所有的pairwise对应关系之间进行优化，从而使得我们的方法更加的scable。而且，我们介绍了一个low-rank的约束来对于所选择的feature points做geometric consistency的优化。在[这篇文章](https://openaccess.thecvf.com/content_iccv_2015/papers/Yan_A_Matrix_Decomposition_ICCV_2015_paper.pdf)里为了multi-graph matching提出的matrix decomposition的方法在graph edges上采用了同样的low-rank约束。在这篇文章里，我们的low-rank约束直接加在feature points locations上，算法更加高效。最近也有的方法提出一种高效的方法来为matching任务寻找具有辨别型的feature points clusters，但是他们并没有考虑geometric consistency的约束。



**3. Preliminaries and notation**

**3. 1 Pairwise matching**

给定n张需要去匹配的images，每张image有$$p_i$$个feature points，对于每个image pair $$(i,j)$$的pairwise feature对应关系可以用一个partial permutation矩阵$$P_{i,j} \in \{0, 1\}^{p_i, p_j}$$来表示，其还需要满足一个限制条件：doubly stochastic constraints，

$$0 \leq P_{i,j} 1 \leq 1$$

$$0 \leq P_{i,j}^T 1 \leq 1$$

$$P_{i,j}$$可以通过在满足上述条件下，最大化其自身和feature similarities之间的inner product来实现。这是一个linear assignment问题，这个问题已经被研究透彻了，并且可以用HUngarian algorithm来解决。寻找$$P_{i,j}$$还可以被构造成一个graph matching问题，之后转换为一个quadratic assignment问题（QAP）。而这个问题通过最大化一个既含有local compatibilities（feature similarity）信息又含有structural compatibilities（spatial rigidity）信息的目标函数来找到assignment。尽管QAP是NP-hard的，但很多高效的算法已经被提出来近似地解决这个问题。我们使用通过linear matching或者graph matching得到的结果，$$W_{i,j} \in R^{p_i \times p_j}$$，作为我们的输入。

**3.2 Cycle consistency**

近期的工作[1](https://arxiv.org/pdf/1402.1473.pdf), [2](https://pages.cs.wisc.edu/~pachauri/perm-sync/assignmentsync.pdf)和[3](https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_27.pdf)提出使用cycle consistency作为一个constraint来进行多张images的匹配。如果下面的条件对于任意的图片$$i,j,k$$都满足，那么所有的图片对之间的对应关系就是cyclically consistent的：

$$P_{i,j} = P_{i,z}P_{z,j}$$

cycle consistency可以通过介绍一个虚拟的universe来更加清晰的描述，这个universe定义为这些图片构成的集合的unique features [2](https://pages.cs.wisc.edu/~pachauri/perm-sync/assignmentsync.pdf)。universe里的每个feature point都必须至少被一张图片所记录到，并且还要建立与这张图片上某个feature point的对应关系。假设图片i和universe之间的对应关系可以用一个partial permutation矩阵$$X_i \in \{0,1\}^{p_i \times u}$$来表示，其中$$u$$是universe里feature points的个数，而且$$u \geq p_i$$对于所有的$$i$$都成立。因为对于universe来说，我们假设cycle-consistent是成立的，所以说pairwise对应关系就可以被表示为

$$P_{i,j} = X_i X_j^T$$

我们将所有的partial matrix按照下面的方式连起来：

![lql]({{ '/assets/images/MATCHING-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}

有结果表明，集合$$\{P_{ij}| \forall i,j\}$$是cyclically consistent的当且仅当$$P$$可以被分解为$$XX^T$$。


**4. Proposed methods**

**4.1 Matching by labeling**

回忆一下，$$X \in \{0,1\}^{m \times u}$$是从image feature points到universe feature points的一个map，其中$$m$$和$$u$$分别表示image的local features的个数和universe的features的个数。另一种对$$X$$的解释是$$X$$的每一行是image里的feature的一个label。对于不同图片的features，如果它们的label相同，那么就是相互对应的。为了能够处理所有的图片里的features，之前的工作经常会用一个很大的$$u$$。

但是，并不是图片里的每一个feature都适合拿来做匹配。尤其是在semantic matching的任务里，很多的随机或者平均采样的features在多张图片之间是nonrepeatable的，它们应该在matching的时候被剔除。通过这个事实的启发，我们从大量的feature points里只挑选出$$k$$个，而$$k$$是我们预先设定好的超参数，也就是说每张图片只有$$k$$个feature points被选出来做matching。

>这个$$k$$需要针对图片里所包含的object的不同的种类做不同的调整，而不能通过某种方式学习到，这可以是将来的一个拓展的方向。

假设图片$$i$$的feature points和universe之间的对应关系的partially permutation矩阵是$$X_i \in \{0.1\}^{p_i \times k}$$。每个$$X_i$$是一个partial permutation矩阵，而且是一个瘦长型的矩阵，因为列数为$$k$$，是个较小的值。这个矩阵需要满足：

$$ 0 \leq X_i 1 \leq 1$$

$$ X_i^ T 1 = 1$$

也就是说每个图片里的feature points可能在universe里没有对应的点，有也最多只有一个。但universe里的每个点在每张图片里都有对应的feature points。

>这和我们考虑的情况就不一样了，因为我们考虑的是3D的object，每张图片可能不含有全部的关键点的信息，所以说每列的和也不是1。

>而且这种方法也不能有occlution，也就是说所有的图片里的object的所有的关键点都要能看得到。

集合$$\lbrace X_i, 1 \leq i \leq n \rbrace$$就是我们这篇工作里需要去估计的东西。$$X_iX_j^T$$就会给出图片$$i$$和图片$$j$$之间的feature points对应关系。根据3.2里说的，只要$$P$$能分解为$$X_iX_j^T$$，那么其就是cycle-consistent的，而此处就直接通过$$X_iX_j^T$$来表示$$P_{i,j}$$了，所以当然是cycle-consistent的。因为我们想要在initial pairwise matching的基础上找到具有cycle-consistent的并且具有辨识度的feature points，我们通过最小化initial pairwise matching的结果和利用$$X_iX_j^T$$算出来的图片$$i$$和图片$$j$$之间的cycle-consistent的匹配结果，来估计$$X$$的值：

$$ \min_{X} \frac{1}{4}||W - XX^T||^F_2$$

$$s.t. X_i \in P^{p_i \times k}, 1 \leq i \leq n$$

其中$$P$$代表所有的partial permutation矩阵的集合，$$W \in R^{m \times m}$$是$$W_{i,j}$$的集合，其中$$m = \Sigma^t_{i=1} p_i$$是所有图片的feature points的数量的总和。通过解决上述的优化问题，我们就能够找到这个图片集合里最具有代表性的$$k$$个互相匹配的feature points，而且它们满足cycle-consistent的性质。


**4.2 Geometric constraint**

假设我们对同一个场景的$$n$$张图片找到了$$k$$个匹配的feature points。我们使用$$M_i \in R^{2 \times k}$$来表示在图片$$i$$里的这$$k$$个feature points的二维坐标，而且是按照顺序排列的。将所有的这样$$n$$个矩阵$$M_i$$按照行连接起来，就得到了一个$$M \in R^{2n \times k}$$的矩阵，其中每列就是每个feature point。$$M$$在structure from motion里叫做measurement matrix [4](https://link.springer.com/article/10.1007/BF00129684)。有结果表明，在orthographic projection处理之后，$$M$$的秩是4。

在我们的问题里，我们将图片$$i$$里所有的feature points的二维坐标表示为$$C_i \in R^{2 \times p_i}$$。之后，通过$$X_i$$我们可以选出$$k$$个feature points，而这$$k$$个feature points在图片$$i$$里的二维坐标就是$$M_i^{'} = C_iX_i$$，其中$$M_i^{'}$$储存了所选中的$$k$$个feature points的在图片$$i$$里的二维坐标信息，而且是按顺序排列的。

同样的，我们也可以将$$M_i^{'}$$按行来排列，从而得到一个矩阵$$M^{'} \in R^{2n \times k}$$：

![ma]({{ '/assets/images/MATCHING-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}

如果feature points都是正确的被挑选出来并且正确的被贴上label，那么矩阵$$M^{'}$$在经过orthographic projection之后就应该是秩为4的。即使object是non-rigid的，$$M^{'}$$仍然可以被一个low-rank矩阵近似 [5](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.230.5079&rep=rep1&type=pdf)。上述的结论可以很好的被利用起来，从而更好的来估计$$X$$的值。假设$$M^{'}$$的真实的秩不比$$r$$大。最小化下面的项可以让我们对于所选择的$$k$$个feature points加上geometric consistent的约束：

$$f_{geo} = \frac{1}{2}||M^{'} - Z||^F_2 = \frac{1}{2}\Sigma^n_{i=1} ||C_iX_i - Z_i||^F_2$$

其中$$Z \in R^{2n \times k}$$是一个辅助变量，其秩不大于$$r$$，并且$$Z_i \in R^{2 \times k}$$代表着矩阵$$Z$$的第$$(2i-1)$$行和第$$2i$$行。


>这里的操作很奇怪，为什么要引入一个新的$$Z$$，为什么不直接SVD分解矩阵$$M^{'}$$，之后挑选出最大的r个eigenvalue重新组合成一个矩阵，再来最小化$$M^{'}$$和这个新矩阵之间的Frobius norm。


**4.3 Formulation**

将cycle-consistency和geometric consistency联合起来，我们就获得了最后我们需要优化的目标函数：

$$ \min_{X,Z} \frac{1}{4}||W - XX^T||^F_2 + \frac{\lambda}{2}\Sigma^n_{i=1} ||C_iX_i - Z_i||^F_2$$

$$s.t. X_i \in P^{p_i \times k}, 1 \leq i \leq n, rank(Z) \leq r$$

其中$$\lambda$$控制geometric constraint所占的比重。


**4.4 Optimization**

对于上述的optimization问题，我们通过block coordinate descent方法来优化，也就是说交替的固定其它的变量优化其中一部分变量。

我们曾经尝试将上述optimization问题里对$$X$$矩阵要求点是0或者1的离散条件，改成$$X$$里的点是0到1之间的连续的值，这样的改动在quadratic assignment problems里很常见。然而，我们观察到如果我们做了这样的改动，那么$$C_iX_i = Z_i$$这个约束对于任意的$$Z_i$$来说就是个ill-posed问题，从而geometric constraint就不管用了。因此，我们要保留$$X$$的integer constraint。

>这里值得再深究

因此，为了让上述的optimization问题能够操作，我们将optimization式子里的两项解耦，也就是让它们没有关系，我们引入一个辅助变量$$Y \in R^{m \times k}$$，在第一项中用$$Y$$来替代$$X$$，并且再加入一项来使得$$X$$和$$Y$$尽量靠近，注意到$$Y$$里每个元素的值不是离散的0或者1，而是实数域。从而我们的optimization问题就变成了下面这样：

$$ \min_{X,Z} \frac{1}{4}||W - YY^T||^F_2 + \frac{\lambda}{2}\Sigma^n_{i=1} ||C_iX_i - Z_i||^F_2 + \frac{\rho}{2}||X - Y||^F_2$$

$$s.t. X_i \in P^{p_i \times k}, 1 \leq i \leq n, Y \in C, rank(Z) \leq r$$

其中$$C$$表示满足下列约束关系的矩阵集合：

$$0 \leq Y \leq 1, 0 \leq Y_i1 \leq 1, Y_i^T1 = 1, 1 \leq i \leq n$$

其中$$\lambda$$和$$\rho$$分别用来控制X和Y之间的差距以及geometric constraint所占的比重。当$$\rho$$趋近于无穷大的时候，上述optimization问题则等价于原始的那个。

将之前的optimization问题改造成上述的形式，是因为改造完之后，每个subproblem都变得很简单了。我们用以下的方式交替更新Y，X和Z。

Y是通过projected gradient descent方法来更新的：

$$ Y \leftarrow \Pi_C(Y-\eta(YY^TY - WY + \rho(Y-X))) $$

其中$$\Pi_C$$表示到$$C$$上的projection，$$\eta > 0$$是step size。我们先一直优化$$Y$$直到它收敛，再优化X和Z。

每个$$X_i$$是通过Hungarian algorithm来更新的，它的cost matrix构造如下：

$$H_i = \lambda D(C_i, Z_i) - 2 \rho Y_i$$$

其中$$D(C_i, Z_i) \in R^{p_i \times k}$$表示$$C_i$$和$$Z_i$$里每个二维点之间的Eucldean距离。

$$Z$$是通过SVD分解来更新的：

$$Z = U \Sigma V^T$$

其中U和V的columns都是来自于$$M^{'}$$的singular vectors，而$$\Sigma$$则是选取了$$M^{'}$$矩阵最大的$$r$$个singular values构造的对角矩阵。（也就是说构造一个rank为r的矩阵Z，其是用SVD分解$$M^{'}$$来实现的）

为了更好的收敛，我们利用一种逐步增大$$\rho$$的方式。对于每个$$\rho$$，我们按照上述方式先更新Y，再交替更新X和Z，直到目标函数不再减小。因为每次更新不会增加目标函数的值，所以说我们最终可以达到局部收敛。在我们的实验里，我们设置$$\rho$$为1，10，199，设置$$\lambda=1$$以及$$r=4$$。

因为这个优化问题是nonconvex的，而且还涉及连续和离散两种变量，所以说需要很好的初始化的技巧。我们首先按照解下述优化问题来初始化Y的值（忽略geometrical consistency）：

$$ \min_{Y} \frac{1}{4}||W - YY^T||^F_2 $$

$$s.t. Y \in C$$

这个优化问题的解是：

$$ Y \leftarrow \Pi_C(Y-\eta(YY^TY - WY)) $$

而X的初始化可以通过将Y离散化而得来，也就是将Y的值变成0或1。


**5. Experiments**

**5.1 Multi-graph matching**

我们先再multi-graph matching任务上验证我们方法的有效性，在这个任务里，feature point的位置是有标注的，但是它们的对应关系需要被正确预计。匹配的准确率是用recall来衡量的，定义为算法所找到的的真实的对应关系除以所有的真实的对应关系的总和。

我们使用CMU dataset和WILLOW Object Class dataset来做实验。CMU dataset包含hotel sequence和house sequence。每一帧图片都标注了30个feature points的位置并给了它们的SIFT特征。WILLOW Object Class dataset提供了五个object类型的图片（car，duck，motorbike，face和winebottle）以及为每张图片提供10个标注位置的特征点。因为物体的外观在每个类别的众多图片中差别很大，所以几何描述子SIFT很难作用。所以我们采用了利用预训练的CNN网络来提取deep features的方法来获取特征。详细的说，就是每张图片都喂给了一个AlexNet（在ImageNet上预训练），然后将Conv4和Conv5的关键点对应位置的输出连起来作为特征。对于这两个数据集，初始的pairwise对应关系都是通过linear matching solver Hungarian algorithm所得到的，之后再用我们本文中提出的方法来操作。三个有公开代码的方法被用作baselines：[spectral method](https://pages.cs.wisc.edu/~pachauri/perm-sync/assignmentsync.pdf)，[MatchLift](https://arxiv.org/pdf/1402.1473.pdf)和[MatchALS](https://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.pdf)。对于所有的方法，universe的feature points的个数都被设置为每张图片里标注的points的数量。

recall的结果由Table 1显示。Table 1表明我们的方法要比其它的方法要好很多。Table 1还显示了两个结果：1）如果利用[graph matching solver RRWM](https://www.researchgate.net/profile/Minsu-Cho-5/publication/221304918_Reweighted_Random_Walks_for_Graph_Matching/links/54c50ae80cf256ed5a98633c/Reweighted-Random-Walks-for-Graph-Matching.pdf)来取代之前的初始的pairwise对应关系，匹配的准确率在除了duck的情况下都能到100%。2）如果忽略geometrical consistency，匹配的准确率会急剧降低。

![Ta]({{ '/assets/images/MATCHING-4.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Table 1. 在CMU dataset和WILLO Object Class dataset上的recall的结果。我们提出的方法与spectral method，MatchLift以及MatchALS方法进行了对比。$$Ours^-$$指的是我们的方法没有geometric consistency的情况。$$Input^+$$和$$Ours^+$$分别指的是使用RRWM graph matching方法来进行初始的pairwise对应关系的计算，在没用我们的方法和用了我们的方法之后的结果。

fig 2显示了利用geometrical consistency我们可以纠正geometrically distorted matches。

![exam]({{ '/assets/images/MATCHING-5.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 2. 上面一行是没有使用geometrical consistency的情况，下面一行是使用了的情况。蓝色的十字表示正确的对应关系，而红色的十字表示错误的对应关系。*

我们所提出的方法还能够自动选择可靠的feature points用来做matching。fig 3给了个例子，我们给CMU dataset加了随机的feature points。而我们的方法可以去掉这些随机加入的feature points。

![add]({{ '/assets/images/MATCHING-6.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 3. 正确的匹配是绿色的，错误的匹配是红色的。上面一行是只用了pairwise方法来找到的对应关系，而下面一行是再经过我们提出的方法找到的对应关系。我们还在原数据集的每一帧中加入了30个随机挑选位置的feature points作为outliers。我们设置$k=30$。*


**5.2 Dense semantic matching**

在这一节里，我们通过将我们提出的方法和region based semantic flow methods（比如说，[proposal flows](https://arxiv.org/pdf/1703.07144.pdf)）结合起来的方式来在dense matching这个任务上测试我们提出的方法的效果。在proposal flow method里，图片之间的region proposals的对应关系先被估计出来，然后再转换为一个dense flow field。对于一系列图片，我们将所提出的方法应用在proposal flow上，从而改进proposals的pairwise对应关系，因此改进dense flow。

我们通过在PF-WILLOW dataset上来衡量效果，这个数据集将WILLOW Object Class dataset分成了10个子类。它们分别是car(S), (G), (M), duck(S), motorbike(S), (G), (M), winebottle(w/oC), (w/C)和(M)，其中(S)和(G)表示side和general角度，而(C)表示背景很杂乱，(M)表示混合角度。每个子类都包含10张不同object的图片。

percentage of correct keypoints (PCK)被用来作为衡量标准。它衡量了当利用estimated flow将一张图片的标注的keypoints转移到另一张图片上的时候位置正确的keypoints的比例。一个预测的feature point被认为是在正确的位置上如果它到groundtruth point的距离在$$\alpha max(h,w)$$以内，其中$$\alpha$$在0到1之间，$$h$$和$$w$$分别是object bounding box的高和宽。对于proposal flow，selective search（SS）被用来作为proposal generator，HOG是feature descriptor，local offset matching（LOM）用作geometric matching strategy。每张图片都会被提取500个proposals，并用来做matching以及生成dense flow。在我们的算法里，每个proposal都被当做一个feature point，每个proposal的中心点就作为我们geometric consistency里所用的coordinate。对于每个类别我们都设定选取10个feature points，也就是$$k=10$$。

Table 2显示我们的方法改进了原论文里的效果。fig 4给了可视化的效果。

![add]({{ '/assets/images/MATCHING-7.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Table 2*

![add]({{ '/assets/images/MATCHING-8.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 4. 我们将source images扭曲成target image，使用了proposal flow估计出来的dense correspondence，也使用了我们提出的方法进行了优化。*


**5.3 Object-class model reconstruction**

将不同的object instances匹配起来在从一系列图片来重建object-class model这个任务里一直都是一个主要的挑战。一些前期的工作依赖于图片里标注的keypoints [6](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Carreira_Virtual_View_Networks_2015_CVPR_paper.pdf), [7](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kar_Category-Specific_Object_Reconstruction_2015_CVPR_paper.pdf), [8](https://openaccess.thecvf.com/content_cvpr_2014/papers/Vicente_Reconstructing_PASCAL_VOC_2014_CVPR_paper.pdf)。
最近也有工作不需要keypoints标注，但需要object mask来去除背景，[9](https://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.pdf)。我们的方法可以为reconstruction提供consistent的对应关系，而且不需要额外的人工标注。我们利用FG3DCar dataset来验证，匹配一共37张左眼的sedan图片，之后再来重建一个3D模型。我们还额外收集了含有30张不同的motorbikes的dataset。

类似于[9](https://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.pdf)的做法，我们在利用structured forests检测出的图片边缘上均匀采样。而不同于[9]的是，因为我们的方法能够去掉那些nonrepeatable的feature points，所以background里的feature points就自动没了，我们不需要object mask。大约每张图片获取了550个feature points candidates。在5.1里所说的deep features被用作每个feature points的feature，并且之前提到的graph matching solver RRWM被用来做初始pairwise matching。

我们使用precision作为衡量标准，定义为算法所找到的对应关系里真实的对应关系，除以算法找到的所有的对应关系。真实的对应关系的定义和前面在PCK里所用的方法差不多。我们使用不同的$$k$$来测试我们的方法的效果，fig 5显示了结果。

![add]({{ '/assets/images/MATCHING-9.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 5. 早FG3DCar dataset上，对于不同的$$k$$值，precision的结果。*

fig 5的结果表明，使用我们的方法改进的效果要比使用pairwise matching要好得多，pairwise matching在背景很杂乱的时候效果很差。

>但是问题是，用precision来衡量，而且从fig 5也能看出来，在$$k$$很小的时候效果比较好，也就是说在feature points对应关系
越少，效果越好。但这并不能说明什么，因为对于对应关系多的情况，即使它的precision比较低，但也可能是因为多出来的那部分都检测的不对，但这并不能说明它的效果就弱于对应关系少但是效果好的情况。但文章里说的是，所用的对应关系越少，效果越好，说明它们的方法去除掉了那些nonrepeatable的feature points。但这是得不出这个结果的，除非可视化来看到底去除掉了哪些feature points。

fig 6说明了文中提出的方法确实能去除掉很多nonrepeatable的背景中的对应关系（看看就好，没有什么说服力）

![back]({{ '/assets/images/MATCHING-9.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 6. 两个sedan图片之间的匹配结果。正确的和错误的匹配分别用蓝色和红色来表示。上面一行表示的是使用pairwise对应关系得到的结果，下面一行是使用文中提出的方法得到的结果。可以看到很多初始的背景里的feature points都被文中的方法给去掉了。*

对于reconstruction来说，我们简单的利用factorization method来运行了affine reconstruction。最后被选择的feature points，对应关系，以及reconstruction结果在fig 7中被可视化。可以看到，绝大多数的被选择的feature points落在object上并且都能正确匹配，尽管object的外观和视角都很不一样。尽管有一些噪音和没有追踪到的points，我们可以在reconstruction的结果中看到sedan和motorbikes的形状。如果用更成熟的reconstruction方法一定会得到更好的结果。

之前并没有工作在不同的instances之间利用unsupervised的方法进行relative pose estimation。

![rec]({{ '/assets/images/MATCHING-10.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 7. *


**5.4 Automatic landmark annotation**

我们将所提出的方法应用在cat head dataset的前1000张图片上。和之前的实验一样，feature points candidates从图片边缘中采样得到，每张图片得到大约43个candidates。我们将$$k$$设置为10，也就是selected feature points的数量是10。结果在fig 1中显示。如同fig 1所示的那样，初始的candidates在整张图片上均匀分布（包括背景），而selected feature points都在object上，而且对应关系也能在具有不同外观和姿态的object上建立。更加有意思的是，这种自动检测的feature points和人工标注的landmarks重合度很高，都表示了cat的具有特征性的feature points，比如说ears，eyes，mouth等。这个结果说明了我们的方法对于automatic landmark detection这个方向也存在潜力，整个过程模拟了人标注的过程：我们比较了一系列的图片，并且找到了不随着外观、几何形状以及姿态而改变的那部分feature points。


**5.5 Computational complexity**

**6. Conclusion**

我们展示了一个新的方法，其将在多张图片上寻找semantic matching的问题转换为了feature points selection和labeling的问题来解决。我们所提出的方法可以在一系列图片之间建立可靠的feature points对应关系，而且这种对应关系还是cycle consistent和geometry consistent的。实验表明我们的方法要比之前的multi-image matching方法要好，并且对于上千张图片也是scable的。我们同时还阐述了几个可能的应用：改进dense flow estimation，不使用人工标注进行reconstruction object-class models，以及自动进行image landmark标注。


### 2. [A convex relaxation for multi-graph matching](https://openaccess.thecvf.com/content_CVPR_2019/html/Swoboda_A_Convex_Relaxation_for_Multi-Graph_Matching_CVPR_2019_paper.html)

*CVPR 2019*


### 3. [Probabilistic Permutation Synchronization Using the Riemannian Structure of the Birkhoff Polytope](https://openaccess.thecvf.com/content_CVPR_2019/html/Birdal_Probabilistic_Permutation_Synchronization_Using_the_Riemannian_Structure_of_the_Birkhoff_CVPR_2019_paper.html)

*CVPR 2019*


### 4. [HiPPI: Higher-Order Projected Power Iterations for Scalable Multi-Matching](https://openaccess.thecvf.com/content_ICCV_2019/html/Bernard_HiPPI_Higher-Order_Projected_Power_Iterations_for_Scalable_Multi-Matching_ICCV_2019_paper.html)

*ICCV 2019*


### 5. [MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization](https://openaccess.thecvf.com/content/CVPR2021/html/Huang_MultiBodySync_Multi-Body_Segmentation_and_Motion_Estimation_via_3D_Scan_Synchronization_CVPR_2021_paper.html)

*CVPR 2021*


### 6. [Quantum Permutation Synchronization](https://openaccess.thecvf.com/content/CVPR2021/html/Birdal_Quantum_Permutation_Synchronization_CVPR_2021_paper.html)

*CVPR 2021*


### 7. [Graduated Assignment for Joint Multi-Graph Matching and Clustering with Application to Unsupervised Graph Matching Network Learning](https://proceedings.neurips.cc/paper/2020/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html)

*NeurIPS 2020*


### 8. [Associative3D: Volumetric Reconstruction from Sparse Views](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600137.pdf)


*ECCV 2020*


### 9. [All Graphs Lead to Rome: Learning Geometric and Cycle-ConsistentRepresentations with Graph Convolutional Networks](https://arxiv.org/pdf/1901.02078.pdf)

*Arxiv 2019*


### 10. [Unsupervised 3D Reconstruction and Grouping of Rigid and Non-Rigid Categories](http://www.iri.upc.edu/files/scidoc/2344-Unsupervised-3D-Reconstruction-and-Grouping-of-Rigid-and-Non-Rigid-Categories.pdf)

*TPAMI 2022*


### 11. [Unifying Offline and Online Multi-Graph Matching via Finding Shortest Paths on Supergraph](https://ieeexplore.ieee.org/abstract/document/9076840)

*TPAMI 2021*


### 12. [Neural Graph Matching Network: Learning Lawler’s Quadratic Assignment Problem with Extension to Hypergraph and Multiple-graph Matching](https://ieeexplore.ieee.org/abstract/document/9426408)

*TPAMI 2021*


### 13. [Distributed and consistent multi-image feature matching via QuickMatch](https://journals.sagepub.com/doi/pdf/10.1177/0278364920917465)

*IJRR 2020*


### 14. [Robust Multi-Object Matching via Iterative Reweighting of the Graph Connection Laplacian](https://proceedings.neurips.cc/paper/2020/hash/ae06fbdc519bddaa88aa1b24bace4500-Abstract.html)

*NeurIPS 2020*


### 15. [Isometric Multi-Shape Matching](https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Isometric_Multi-Shape_Matching_CVPR_2021_paper.html)

*CVPR 2021*


### 16. [Multiple Graph Matching and Clustering via Decayed Pairwise Matching Composition](https://ojs.aaai.org/index.php/AAAI/article/view/5528)

*AAAI 2020*


### 17. [Multi-View Multi-Human Association With Deep Assignment Network](https://ieeexplore.ieee.org/abstract/document/9693506)

*TIP 2022*


### 18. [Scalable Cluster-Consistency Statistics for Robust Multi-Object Matching](https://ieeexplore.ieee.org/abstract/document/9665954)

*3DV*


### 19. [Fast, Accurate and Memory-Efficient Partial Permutation Synchronization](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Fast_Accurate_and_Memory-Efficient_Partial_Permutation_Synchronization_CVPR_2022_paper.html)

*CVPR 2022*


### 20. [Graph Neural Networks For Multi-Image Matching](https://openreview.net/forum?id=Hkgpnn4YvH)

*ICLR 2020*


### 21. [Higher-order Projected Power Iterations for Scalable Multi-Matching](https://openaccess.thecvf.com/content_ICCV_2019/papers/Bernard_HiPPI_Higher-Order_Projected_Power_Iterations_for_Scalable_Multi-Matching_ICCV_2019_paper.pdf)

*ICCV 2019*


### 22. [Joint Deep Multi-Graph Matching and 3D Geometry Learning from Inhomogeneous 2D Image Collections](https://ojs.aaai.org/index.php/AAAI/article/view/20220)

*AAAI 2022*


### 23. [A-ACT: Action Anticipation through Cycle Transformations](https://arxiv.org/pdf/2204.00942.pdf)

*Arxiv 2022*


### 24. [Quantum Motion Segmentation](https://arxiv.org/pdf/2203.13185.pdf)

*Arxiv 2022*


### 25. [Image Matching from Handcrafted to Deep Features: A Survey](https://link.springer.com/content/pdf/10.1007/s11263-020-01359-2.pdf)

*IJCV 2021*


### 26.[PRNet: Self-Supervised Learning for Partial-to-Partial Registration](https://proceedings.neurips.cc/paper/2019/hash/ebad33b3c9fa1d10327bb55f9e79e2f3-Abstract.html)

*NeurIPS 2019*


### 27. [DeepBBS: Deep Best Buddies for Point Cloud Registration](https://arxiv.org/pdf/2110.03016.pdf)

*3DV 2021*


### 28. [Joint-task Self-supervised Learning for Temporal Correspondence]()

*NeurIPS 2019*


### 29. [Continuous Surface Embeddings](https://proceedings.neurips.cc/paper/2020/hash/c81e728d9d4c2f636f067f89cc14862c-Abstract.html)

*NeurIPS 2020*


### 30. [Learning Facial Representations From the Cycle-Consistency of Face](https://openaccess.thecvf.com/content/ICCV2021/html/Chang_Learning_Facial_Representations_From_the_Cycle-Consistency_of_Face_ICCV_2021_paper.html)

*ICCV 2021*


### 31. [Demystifying Unsupervised Semantic Correspondence Estimation](https://arxiv.org/pdf/2207.05054.pdf)

[POST](https://mehmetaygun.github.io/demistfy.html)

*ECCV 2022*



### 32. [Unsupervised Part Discovery via Feature Alignment](https://arxiv.org/pdf/2012.00313.pdf)

*Arxiv 2020*


### 33. [LAKe-Net: Topology-Aware Point Cloud Completion by Localizing Aligned Keypoints](https://openaccess.thecvf.com/content/CVPR2022/html/Tang_LAKe-Net_Topology-Aware_Point_Cloud_Completion_by_Localizing_Aligned_Keypoints_CVPR_2022_paper.html)

*CVPR 2022*

---

*If you notice mistakes and errors in this post, don't hesitate to contact me at* **wkwang0916 at outlook dot com** *and I would be super happy to correct them right away!*

